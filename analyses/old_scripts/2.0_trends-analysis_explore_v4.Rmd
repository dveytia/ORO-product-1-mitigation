---
title: "2.0_trends-analysis_explore"
author: "Devi Veytia"
date: "2025-07-08"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
```




```{r load libraries, results = 'hide'}

# general data handing
library(dplyr)
library(dbplyr)
library(RSQLite)
library(R.utils)
library(ggplot2)
library(ggalluvial)
library(tidyr)
library(stringr)
library(viridis)
library(countrycode)
library(broom)
library(conflicted)
library(tidyverse)
library(cowplot)
library(ggpubr)
library(patchwork)
library(igraph)
library(ggraph)
library(tidygraph)
library(ggplot2)
require(VLTimeCausality) # remotes::install_github("DarkEyes/VLTimeSeriesCausality")
library(QPress) # remotes::install_github("SWotherspoon/QPress")
library(reshape2)


conflict_prefer("select", "dplyr")
conflicts_prefer(dplyr::filter)



## AESTHETICS
factor_aes <- readxl::read_excel(here::here("R/mitigation_factor_aesthetics2.xlsx"))
typeAES <- factor_aes[which(factor_aes$variable == "oro_type"),]
typeAES <- typeAES[order(typeAES$order),]

componentAES <- factor_aes[which(factor_aes$variable == "component"),]
componentAES <- componentAES[order(componentAES$order),]

## Set seed
addTaskCallback(function(...) {set.seed(123);TRUE})

```


# Introduction

Entry point: The distribution of and extent of scientific evidence, and its temporal variation, varies according to the type of ocean-related option (ORO) (Veytia et al, In review). 

Aim: This work is a hypothesis generating exercise, which asks: What are the possible drivers of this variability? What are the socio-political antecedents of breakpoints/inflections? Over time, is variability in scientific evidence linked to policy, public interest and action? What does this tell us about the research/policy/action cycle?



This aim will be addressed in two parts:

Part 1: Narrative synthesis. 
Here we qualitatively explore break points in publication #s, as well as action/implementation, and discuss possible drivers of these changes.

Part 2: Quantitative synthesis.

2.1 - Match/mis-match. Do the most evidenced OROs appear most frequently in policy and public interest metrics?
2.2 - Network analysis. Do we find evidence for causal links between the different nodes of the research/policy/action cycle? If so, what does this tell us about the pathways towards implementing OROs?


Summary of main conclusions:

Over the entire time series, our quantitative analysis of causality shows publications and public interest to be direct drivers of action, but that policy and legislation play an influential intermediary role.

That said, public interest does not match well with publications, which may cause conflicting pressures that act on action. Legislation and policy show better alignment.

However, it is clear from our narrative synthesis that the right legislation can have enormous impact producing step changes in action. Therefore not all policy/legislation documents act equally on the system in terms of impact, and the potential for one document to have a large impact is arguably higher than all the other dimensions we investigated. 



# Load data

Data structure:
list where each level is 
- data frame for publications: 
- data frame for n policy documents
- data frame for deployment

each data frame has the following id variables: oro_type, component (publication, policy, deployment, etc), variable_name,
each data frame has the following response variable: y

```{r load data}

# Load data for specific OROs
load(here::here("data", "derived-data", "mitigationORO_pubs.RData")) #pubs
load(here::here("data", "derived-data", "n_nonBindPolicy_docs.RData")) # legDat
load(here::here("data", "derived-data", "n_legislation_docs.RData")) # polDat
load(here::here("data/derived-data/mitigationDeployDat.RData")) # allDeployDat
load(here::here("data/derived-data/mitigationPostsDat.RData")) # postsDat

# Load data for all mitigation OROs
load(here::here("data", "derived-data", "mitigationORO_pubs_allMitigationOROs.RData")) # all_Pub
load(here::here("data", "derived-data", "n_legislation_docs_allMitigationOROs.RData")) # all_legDat
load(here::here("data", "derived-data", "n_nonBindPolicy_docs_allMitigationOROs.RData")) # all_polDat
load(file=here::here("data/derived-data/mitigationPostsDat_allMitigationOROs.RData")) # all_postsDat



year_lim <- c(2000, 2024) # years to analyse

# components to analyse
# Remove public support because it was derived from interest, and picked the most robust metric?
selectedComponents <- c("publications", "deployment","policy","legislation","public interest","public support") #  "public opposition"

# bind all data together
allComponentDat_model <- pubs %>%
  bind_rows(polDat) %>%
  bind_rows(legDat) %>%
  bind_rows(allDeployDat) %>%
  bind_rows(postsDat) %>%
  mutate(
    component = replace(component, component == "non-binding policy", "policy"),
    # y = scale(y, center=TRUE, scale=TRUE)
  ) %>%
  filter(year_lim[1] <= year & year <= year_lim[2],
         oro_type %in% typeAES$level) %>%
  filter(
    component %in% selectedComponents
  )


# Bind together for all mitigation OROs
allComponentDat_model_allMitOROs <- all_Pub %>%
  bind_rows(all_polDat) %>%
  bind_rows(all_legDat) %>%
  bind_rows(all_postsDat) %>%
  mutate(
    component = replace(component, component == "non-binding policy", "policy"),
    # y = scale(y, center=TRUE, scale=TRUE)
  ) %>%
  filter(year_lim[1] <= year & year <= year_lim[2]) %>%
  filter(
    component %in% selectedComponents
  )

```


# Entry point: The distribution of and extent of scientific evidence, and its temporal variation, varies according to the type of ocean-related option (ORO) 

*Key message*
Mitigation ORO publication effort is weighted towards marine renewable energy (ocean, located). Over time, we observe varying rates of increase, with inflection points – notably in MRE-Ocean occurring in 2001. This inspired this analysis: What are the drivers of this inflection point? To go further, we complement our data set of # publications with # policy documents, # legislative documents, # social media posts, and # social media posts (positive sentiment)


Skip the code and just run the chunk to visualize at the end

What was driving the inflection in the mitigation ORO branch? A particular ORO or a particular affiliation?
```{r plot publication timeseries -- MRE-Ocean is driving inflection, eval = FALSE}

p1_db <- RSQLite::dbConnect(RSQLite::SQLite(),
                      here::here("data","sqlite-databases","product1.sqlite"),
                                 create=FALSE)

uniquerefs <- tbl(p1_db, "uniquerefs_update2025") %>%
  select(analysis_id, year) 

predOroType <- tbl(p1_db, "pred_oro_type_mit_long") %>%
  left_join(uniquerefs, by = "analysis_id") %>%
  collect()

dbDisconnect(p1_db)

oroPub <- predOroType %>%
  mutate(
    std_prediction = ifelse(is.na(std_prediction), 0, std_prediction),
    level = gsub("[.]","-", level),
    year= as.numeric(year)
    ) %>%
  filter(!is.na(year)) %>%
  mutate(
    lower_prediction = mean_prediction - std_prediction,
    upper_prediction = mean_prediction + std_prediction
  ) %>%
  group_by(level, year) %>%
  summarise(
    y_mean = n_distinct(analysis_id[0.5 <= mean_prediction]),
    y_lower = n_distinct(analysis_id[0.5 <= lower_prediction]),
    y_upper = n_distinct(analysis_id[0.5 <= upper_prediction])
  )
  # mutate(
  #   oro_type = factor(
  #     level, 
  #     levels = typeAES$level,
  #     labels = typeAES$label
  #   )
  #   
  # ) 
allPub <- predOroType %>%
  mutate(
    std_prediction = ifelse(is.na(std_prediction), 0, std_prediction),
    level = gsub("[.]","-", level),
    year= as.numeric(year)
    ) %>%
  filter(!is.na(year)) %>%
  mutate(
    lower_prediction = mean_prediction - std_prediction,
    upper_prediction = mean_prediction + std_prediction
  ) %>%
  group_by(year) %>%
  summarise(
    y_mean = n_distinct(analysis_id[0.5 <= mean_prediction]),
    y_lower = n_distinct(analysis_id[0.5 <= lower_prediction]),
    y_upper = n_distinct(analysis_id[0.5 <= upper_prediction])
  )%>%
  mutate(level = paste("All mitigation OROs"))


allMit <- oroPub %>%
  bind_rows(allPub)

oroTimeseries_ggp<- ggplot(allMit%>% filter(1980 <= year, year <= 2024), aes(x=year))+
  geom_rect(xmin = 2001, xmax = 2005, ymin=-Inf, ymax=Inf, alpha = 0.1, fill="grey")+
  geom_line(aes(y=log(y_mean), col=level))+
  geom_ribbon(aes(ymin = log(y_lower), ymax = log(y_upper), fill = level), alpha = 0.5)+
  facet_wrap(vars(level), scales = "free_y")+
  scale_color_manual(
    breaks = c("All mitigation OROs", typeAES$level),
    values = c("#35a7d9", typeAES$colour),
    labels = c("All mitigation OROs", typeAES$label)
  )+
  scale_fill_manual(
    breaks = c("All mitigation OROs", typeAES$level),
    values = c("#35a7d9", typeAES$colour),
    labels = c("All mitigation OROs", typeAES$label)
  )+
  scale_y_continuous()+
  scale_x_continuous(limits = c(1980,2024))+
  # guides(col=guide_legend(nrow=2))+
  labs(x="Year", y = "log(N publications)", col = "ORO type")+
  theme_minimal()+
  theme(
    legend.position = "none",
    axis.text.x = element_text(angle=45, hjust=1)
  )

oroTimeseries_ggp
# ggsave(
#   here::here("figures/supplemental/nPublicationsTimeseriesPlots.pdf"),
#   plot = oroTimeseries_ggp,
#   width = 7, height=5
# )

```



```{r introduce complementary data - plot proportions by oro type, eval = FALSE}

## Publications
p1_db <- RSQLite::dbConnect(RSQLite::SQLite(),
                      here::here("data","sqlite-databases","product1.sqlite"),
                                 create=FALSE)

uniquerefs <- tbl(p1_db, "uniquerefs_update2025") %>%
  select(analysis_id, year) 

predOroTypeYear <- tbl(p1_db, "pred_oro_type_mit_long") %>%
  left_join(uniquerefs, by = "analysis_id") %>%
  filter(!is.na(year)) %>%
  group_by(level, year) %>%
  summarise(
    N = n_distinct(analysis_id[0.5 <= (mean_prediction)])
  ) %>%
  collect()%>%
  mutate(
    year = as.numeric(year),
    oro_type = gsub("[.]","-",level),
    component = "publications",
    variable_name = "Publications (N)"
  ) %>%
  select(oro_type, component, variable_name, year, N)

nPubs_oro <- tbl(p1_db, "pred_oro_type_mit_long") %>%
  group_by(level)%>%
  summarise(
    N = n_distinct(analysis_id[0.5 <= (mean_prediction)])
  )%>%
  collect()%>%
  mutate(
    oro_type = gsub("[.]","-",level),
    component = "publications",
    variable_name = "Publications (N)"
  ) %>%
  select(oro_type, component, variable_name, N)

dbDisconnect(p1_db)



## Sumarise by component overall proportional shar of each ORO
prop_oro_summary_df <- allComponentDat_model %>%
  filter(!(component %in% c("deployment","publications"))) %>%
  group_by(component, variable_name, oro_type) %>%
  summarise(
    N = sum(y, na.rm=T)
  ) %>%
  bind_rows(
    nPubs_oro
  ) %>%
  group_by(component) %>%
  mutate(
    total_component = sum(N, na.rm=T)
  ) %>%
  ungroup() %>%
  mutate(
    prop_component = N/total_component,
    component = factor(component, 
                       levels = componentAES$level, 
                       labels = componentAES$label_varname)
  )%>%
  mutate(
    component = droplevels(component)
  )
  
prop_oro_summary_df


# Plot proportion of each dimension per ORO
dist_component_ggp <- ggplot(data = prop_oro_summary_df, aes(x=component))+
  geom_col(aes(y=prop_component, fill=oro_type), position="stack")+
  geom_text(aes(label = scales::number(total_component, accuracy = 1, big.mark = ",")), check_overlap = TRUE, vjust=0, y=1)+
  scale_fill_manual(
    breaks = typeAES$level,
    values = typeAES$colour
  )+
  scale_y_continuous(expand = expansion(mult=c(0,0.1)))+
  labs(
    x="Dimension (i.e. Node)",
    y="Proportion",
    fill="ORO type"
  )+
  guides(fill=guide_legend(ncol=2))+
  theme_minimal(base_size = 10)+
  theme(
    axis.text.x = element_text(angle=45, hjust=1)
  )

dist_component_ggp


## Plot the time series as well
year_oro_summary_df <- allComponentDat_model %>%
  filter(!(component %in% c("deployment","publications"))) %>%
  group_by(component, variable_name, oro_type, year) %>%
  summarise(
    N = sum(y, na.rm=T)
  ) %>%
  bind_rows(
    predOroTypeYear
  ) %>%
  filter(
    year_lim[1] <= year, year <= year_lim[2]
  ) %>%
  group_by(component, year) %>%
  mutate(
    total_component = sum(N, na.rm=T)
  ) %>%
  ungroup() %>%
  mutate(
    prop_component = N/total_component,
    component = factor(component, 
                       levels = componentAES$level, 
                       labels = componentAES$label_varname)
  )%>%
  mutate(
    component = droplevels(component)
    # component = fct_recode(component, "Publications (log(N))"="Publications (N)")
  )%>%
  select(oro_type, component, year, prop_component, N) 
  


# Annual plot of each dimension per ORO
year_component_ggp <- ggplot(data = year_oro_summary_df, aes(x=year))+
  facet_wrap(vars(component), ncol=1, scales = "free_y")+
  geom_area(aes(y=N, fill=oro_type), position="stack")+
  scale_fill_manual(
    breaks = typeAES$level,
    values = typeAES$colour
  )+
  scale_y_continuous(expand = expansion(mult=c(0,0.1)))+
  labs(
    x="Year",
    y="N",
    fill="ORO type"
  )+
  theme_minimal(base_size = 10)+
  theme(
    legend.position = "bottom"
  )

year_component_ggp

year_component_prop_ggp <- ggplot(data = year_oro_summary_df, aes(x=year))+
  facet_wrap(vars(component), ncol=1, scales = "free_y")+
  geom_col(aes(y=prop_component, fill=oro_type), position="stack", width=1)+
  scale_fill_manual(
    breaks = typeAES$level,
    values = typeAES$colour
  )+
  labs(
    x="Year",
    y="Proportion",
    fill="ORO type"
  )+
  theme_minimal(base_size = 10)+
  theme(
    legend.position = "bottom"
  )

year_component_prop_ggp

oro_leg <- ggpubr::get_legend(dist_component_ggp)



plotList <- list(
  dist_component_ggp+theme(plot.margin = unit(rep(0.1, 4),"cm")),
  year_component_ggp+
    theme(legend.position = "none",
          plot.margin = unit(rep(0.1, 4),"cm")
          ),
  year_component_prop_ggp+
    theme(legend.position = "none",
          plot.margin = unit(rep(0.1, 4),"cm")
          )
)

plotLay <- 
"#AA#
BBCC"

combined <- wrap_plots(plotList,
                       design = plotLay,
                       widths = c(1,5,0.2,0.2),
                       heights = c(1,2)
                       )+
  plot_annotation(tag_levels = "a")


ggsave(
  here::here("figures/main/distribution_by_component_plots.pdf"),
  width=6.5, height=7,
  plot = combined
)

```





```{r, out.width="0.9\\linewidth", include=TRUE, fig.align="center", fig.cap=c("Distribution of N publications (predicted relevant by LLM classifier)"), echo=FALSE}

knitr::include_graphics(here::here("figures/supplemental/oro_outcome_and_timeseries.pdf"))
```

## Complementary data

To contextualize our publication data, we have added the following metrics:


Number of policy and legislative documents - Calculated by web scraping ECOLEX & FAOLEX databases then keyword searching full document pdfs for keywords relevant for each type of ORO. Document type metadata used to determine legislation vs policy

Interest - N posts returned from keyword searches (reddit, youtube, bluesky, linked in), weighted by # likes

Support - N posts (from above) that are positive sentiment (predicted using a pre-trained sentiment classification LLM)

Action - various metrics (see below)






A closer look at the ORO-specific 'action' metrics:

MRE - installed capacity (MW) - IRENA renewable electricity statistics

Efficiency - domestic freight transport energy efficiency (gCO2/tkm) - IEA

CCS - Storage Capacity associated with a project- oil and gas climate initiative

CDR-BC - # restoration projects - Duarte et al 2020

CDR-OAE/BioPump - # Field trials/startup companies - Ocean Visions Field trials, GESAMP climate intervention projects, OceanNETs ocean-based CDR companies


```{r plot all deployment time series, eval = FALSE}

allDep_ggp <- ggplot(
  data = allComponentDat_model %>%
    filter(component == "deployment") %>%
    mutate(
    oro_type = factor(oro_type, levels = typeAES$level, labels = typeAES$label)
  )
)+
  geom_line(aes(year, y, col = oro_type))+
  geom_text(aes(label = stringr::str_wrap(variable_name, width = 25),
                fontface=3), x=-Inf, y=Inf,
            check_overlap = TRUE, vjust=1, hjust=0, size=3)+
  facet_wrap(vars(oro_type), scales="free")+
  scale_colour_manual(
    breaks = typeAES$label,
    values = typeAES$colour
  )+
  labs(
    x="Year",
    y="Action metric",
  )+
  theme_minimal()+
  theme(
    legend.position = "none"
  )

allDep_ggp

# ggsave(here::here("figures/supplemental/mitigationDeploymentIndicators.pdf"), plot = allDep_ggp, width = 7, height=6)

```

```{r, out.width="0.9\\linewidth", include=TRUE, fig.align="center", fig.cap=c("ORO-specific action metrics"), echo=FALSE}

knitr::include_graphics(here::here("figures/supplemental/mitigationDeploymentIndicators.pdf"))
```
  


## Network analysis

*Key message*
Across the full time series, publications and public interest emerge as direct drivers of action, while policy and legislation serve as influential intermediaries


Method steps:

Step 1. For each ORO, determine if an edge exists between two nodes using variable lag transfer entropy
Step 2. Generalize these findings across OROs by creating a meta network across all the OROs. If an edge exists for an ORO, it is included in the meta network. 
Step 3. Use the meta network to build a qualitative network model, and simulate press perturbations to the different nodes and record the impact on action.



### Step 1: Variable-lag transfer entropy

For each ORO, determine whether there are links between the time series of the different nodes in our conceptual model (publications, policy, legislation, public interest metrics, action). However, there are likely time lags between the different time series and I am uncertain of how long these lags are; and the relationships may be nonlinear. 

To investigate what might be the best analysis, I think transfer entropy is a good option (see the publications below):


* Behrendt et al. 2019. RTransferEntropy— Quantifying information flow between different time series using effective transfer entropy
* Amornbunchornvej et al 2021. Variable-lag Granger Causality and Transfer Entropy for Time Series Analysis

Uses: To detect significance of information flow from X -> Y with a time lag. unlike Granger Causaility, can detect non-linear relationships.

Data Assumptions:

* evenly spaced, no NA (fill NAs with 0 in our time series)
* stationary data: refers to the idea that the statistical properties of a time series do not change over time. More specifically, a stationary time series is one in which the mean, variance, and autocorrelation structure are constant over time (no noticible changing levels, increasing variance). (use Box-Cox transform to make data normal)
* time lag is fixed (except see variable lag-Transfer entropy with bootstrapping, as bootstrapping approach increases the performance of transfer entropy methods in this task - install.packages("VLTimeCausality"))


VL-TE Pipeline:

* correct for heteroskedasity using a Box-Cox transformation to make your data normal (caret::BoxCoxTrans)
* get an iterable list of the pairwise edges for the different nodes in the time series
* Use VL-TE (bootstrapped) from the VLTTimeCausality package to get TE ratio and p-value (higher the ratio, stronger evidence for causality)
* Plot the TE ratio for each of the edges to determine the evidence for the relationship 


*Kieran questions*

* Does this approach to use variable-lag transfer entropy seem sensible? 
* Do you notice any errors in the analysis/interpretation?

```{r VL-TE analysis, eval = FALSE}
## Load in results in next chunk

# Define levels to loop through

# Loop 1 -- different oros
oros <- unique(allComponentDat_model$oro_type)

# Loop 2 -- process all possible edges
# Define nodes and edges to process
nodes <- unique(allComponentDat_model$component)
edges <- expand.grid(nodes, nodes)
edges <- edges[edges[,1] != edges[,2],]

# remove edge pairs whose calculations are inter-related (all the public interest metrics)
publicEdges <- grepl("public", edges[,1]) & grepl("public", edges[,2])
edges <- edges[!publicEdges,]


# Function to make BoxCoxTransformation -- make data normal
BCTransform <- function(series){
  BCMod <- caret::BoxCoxTrans(series)
  series_trans <- predict(BCMod, series)
}

# Wrapper to try the box-cox transformation with error handling
try_BCTransform <- function(series){
  tryCatch(
    {BCTransform(series)},
    error = function(e){
      return(series)
    }
  )
}


# # Define function to only use BC transform is HeteroSkedasticity present?
# # Decided not to use and just use BC transformation
# correctHeteroskedasticity <- function(series, years){
#   lmMod <- lm(series~years)
#   isHet <- lmtest::bptest(lmMod)
#   if(isHet$p.value <= 0.05){
#     series_trans <- BCTransform(series)
#     return(series_trans)
#   }else{
#     return(series)
#   }
# }
# try_correctHeteroskedasticity <- function(series, years){
#   tryCatch(
#     {correctHeteroskedasticity(series, years)},
#     error = function(e){
#       return(series)
#     }
#   )
# }



## Loop 1 -- loop through OROs
TE_results <- data.frame()
TE_results_summary <- data.frame()
for(oro in oros){
  # oro <- "MRE-Ocean" # for testing
  
  # Data processing
  # Fillin missing values with a 0 -- so that same years are represented across time series
  # correct for heteroskedasiticy
  oroDat <- allComponentDat_model %>%
    filter(oro_type == oro) %>%
    complete(component, year=year_lim[1]:year_lim[2], 
             fill = list(y=0), explicit= FALSE) %>%
    group_by(component) %>%
    mutate(y_trans = try_BCTransform(y)) %>%
    ungroup()
  # ggplot(oroDat, aes(x=year, y=y_trans))+geom_line()+facet_wrap(vars(component), scales="free_y")
  
  
  edgesResults <- data.frame(
    oro_type = rep(oro, nrow(edges)),
    NodeX = edges[,1],
    NodeY = edges[,2],
    TE_XCauseY = rep(NA, nrow(edges)),
    TE_pval = rep(NA, nrow(edges)),
    TE_ratio = rep(NA, nrow(edges)),
    optDelay = rep(NA, nrow(edges)),
    optCor = rep(NA, nrow(edges))
  )
  
  # Loop through the edges and test for causality
  for(e in 1:nrow(edges)){

    TE_out <- tryCatch(
      {VLTransferEntropy(
      Y= oroDat$y_trans[oroDat$component == edges[e,1]],
      X= oroDat$y_trans[oroDat$component == edges[e,2]],
      maxLag = 5,
      VLflag=TRUE,nboot=100, alpha = 0.05)},
      error = function(e){
        return(NULL)
      }
    )
    
    if(is.null(TE_out)){
      edgesResults$TE_pval[e] <- NA
      edgesResults$TE_XCauseY[e] <- NA
      edgesResults$TE_ratio[e] <- NA
      edgesResults$optDelay[e] <- NA
      edgesResults$optCor[e] <- NA
    }else{
      
      ## Save outputs into data frame
      edgesResults$TE_pval[e] <- TE_out$pval
      # TS$X causes TS$Y TRUE/FALSE
      edgesResults$TE_XCauseY[e] <- TE_out$XgCsY_trns 
      # Transfer entropy ratio -- If it is greater than one , then X causes Y.
      edgesResults$TE_ratio[e] <- TE_out$TEratio
      # optimal time delay inferred by cross-correlation of X,Y. It is positive if Y is simply just a time-shift of X (e.g. Y[t]=X[t-optDelay]).
      edgesResults$optDelay[e] <- TE_out$follOut$optDelay 
      # optimal correlation of Y[t]=X[t-optDelay] for all t
      # Maybe if optCor is negative, that can give me the sign 
      edgesResults$optCor[e] <- TE_out$follOut$optCor
    }
    
  }
  
  # summarize results to most causal direction if a two-way effect found
  # ie. When TRUE for both directions, resolve by picking the direction with the highest TE ratio
  edgesResultsSummary <- data.frame()
  uniqueEdges <- edges[!apply(edges, 1, is.unsorted), ]
  for(ue in 1:nrow(uniqueEdges)){
    
    incl = vector(length = nrow(edgesResults))
    for(i in 1:nrow(edgesResults)){
      tmp <- c(edgesResults[i,c("NodeX","NodeY")]) %in% uniqueEdges[ue,]
      incl[i] <- sum(tmp)==2
    }
    tmpDat <- edgesResults[incl,] %>%
      filter(TE_XCauseY) %>%
      arrange(desc(TE_ratio)) %>%
      slice_head(n=1)
    
    edgesResultsSummary <- edgesResultsSummary %>%
      bind_rows(tmpDat)
  }
  
    
  # Bind results
  TE_results <- TE_results %>%
    bind_rows(edgesResults)
  TE_results_summary <- TE_results_summary %>%
    bind_rows(edgesResultsSummary)
  
}

# clean
rm(oroDat, nodes, edges, TE_out, edgesResults)



## Check data
# View(TE_results)
# View(TE_results_summary)
# TE_results %>% filter(!is.na(TE_XCauseY)) %>%filter(TE_XCauseY == TRUE)  %>% View


# save(TE_results, TE_results_summary, file = here::here("data/derived-data/TE_results.RData"))
```



```{r load TE results}

load(here::here("data/derived-data/TE_results.RData")) # TE_results, TE_results_summary

```



### Step 2.1: Create Network by ORO group from TE edges

Aggregate Causal edges (identified from transfer entropy) Across oro_types

Aggregate the TE results across oro_types to build a meta-causal graph:

For each unique edge, compute:

* N significant: how many oro_types had significant causality. This indicates agreement/consistency
* Mean TE_ratio: average causal strength across significant cases.


Visualize the aggregated meta-network where:

* Edge width = mean TE ratio.
* Edge transparency = consistency (e.g. normalized N significant).
* Edge color = sign of correlation

```{r group oro networks conceptually and plot combined networks}
require(ggraph)
require(tidygraph)



## Get diagraph objects for each defined cluster
# Create a data.frame for lookup
concept_cluster_df <- data.frame(
  oro_type = c(
    "CDR-BC","CDR-BioPump","CDR-OAE",
    "MRE-Bio","MRE-Located", "MRE-Ocean"
  ),
  cluster = c(rep(1,3),rep(2, 3))
)

# Or keep all?
concept_cluster_df <- data.frame(
  oro_type = c(
    "CCS",
    "CDR-BC","CDR-BioPump","CDR-OAE",
    "Efficiency",
    "MRE-Bio","MRE-Located", "MRE-Ocean"
  ),
  cluster = c(1,rep(2,3),3,rep(4,3))
)

n_clusters <- length(unique(concept_cluster_df$cluster))

# List to hold one meta-graph per cluster
concept_meta_graphs <- vector("list", n_clusters)
concept_cluster_names <- vector("list", n_clusters)

for (k in 1:n_clusters) {
  # Get oro_types in this cluster
  types_in_cluster <- concept_cluster_df %>%
    filter(cluster == k) %>%
    pull(oro_type)
  
  # Filter TE results for these oro_types and significant edges
  cluster_te <- TE_results %>%
    filter(oro_type %in% types_in_cluster, 
           NodeX %in% selectedComponents,
           NodeY %in% selectedComponents,
           TE_pval <= 0.05,
           !is.na(TE_ratio),
           0 < TE_ratio
           )
  
  # Aggregate TE_ratio per NodeX → NodeY pair
  edge_summary <- cluster_te %>%
    ungroup()%>%
    group_by(NodeX, NodeY) %>%
    summarise(
      n_sig = n(),
      avg_ratio = mean(TE_ratio, na.rm = TRUE),
      avg_correlation = mean(optCor, na.rm=T),
      med_correlation = quantile(optCor, na.rm=T, probs = 0.5),
      avg_delay = mean(optDelay, na.rm=T),
      sign = ifelse(avg_correlation >= 0, 1, -1),
      oro_types = paste(unique(oro_type), collapse = ", "), 
      .groups="drop"
    )%>%
    mutate(
      relative_nsig = n_sig / max(n_sig, na.rm=T)
    )
  
  # Build igraph object
  g <- graph_from_data_frame(edge_summary, directed = TRUE)
  
  # Set edge weights and labels
  E(g)$weight <- edge_summary$avg_ratio
  E(g)$width <- log1p(E(g)$weight) * 1.5
  # E(g)$color <- scales::alpha("black", edge_summary$relative_nsig)
  E(g)$color <- scales::alpha(ifelse(E(g)$avg_correlation >= 0, "#4dac26", "#d01c8b"),edge_summary$relative_nsig)
  E(g)$sign <- edge_summary$sign

  
  concept_meta_graphs[[k]] <- g
  concept_cluster_names[[k]] <- paste(types_in_cluster, collapse = ", ")
}






## Plot the networks of the oro types


nodeTextSize <- 3.5
titleSize <- 12



# Convert network to ggplot
plot_ggraph_network <- function(g, cluster_id, cluster_label, dot_offset=0.05) {
  
  # maybe plot separately by positive and negative so I can make arrows and dots?
  g_tidy <- as_tbl_graph(g) 
  
  # Code for plotting dots at end of negative effects
  # --- Create layout (needed to access node positions) ---
  graph_layout <- create_layout(g_tidy, layout = "circle")
  
  # --- Extract negative edge targets ---
  neg_edges <- g_tidy %>%
    activate(edges) %>%
    filter(sign == -1) %>%  # Filter for negative edges
    as_tibble() %>%
    mutate(
      from_name = V(g_tidy)$name[from],
      to_name = V(g_tidy)$name[to]
    )
  
  # Get positions of source and target nodes for negative edges
  edge_positions <- graph_layout %>%
    filter(name %in% c(neg_edges$from_name, neg_edges$to_name))
  
  # Now calculate the offset position for the dot at the target end of the edge:
  # We will add an offset to the target node's position in the direction of the edge
  dot_positions <- neg_edges %>%
    rowwise() %>%
    mutate(
      # Get source and target positions
      source_x = edge_positions$x[edge_positions$name == from_name],
      source_y = edge_positions$y[edge_positions$name == from_name],
      target_x = edge_positions$x[edge_positions$name == to_name],
      target_y = edge_positions$y[edge_positions$name == to_name],
      # Compute the unit vector from source to target and apply the offset
      dx = target_x - source_x,
      dy = target_y - source_y,
      # Offset the dot by a small distance (here 5% of the edge vector)
      dot_x = target_x + -dot_offset * dx,
      dot_y = target_y + -dot_offset * dy
    )
  
  
  ## plot
  ggraph(g_tidy, layout = "circle") +  # You can try "kk" or "circle" or "fr" for spacing
    
    # # --- Positive edges with arrows ---
    geom_edge_fan(
      aes(
        width = weight,
        alpha = relative_nsig,
        label = paste0("lag = ", scales::number(avg_delay, accuracy = 1), ", ", oro_types),
        color = color,
        filter = sign==1
      ),
      arrow = arrow(length = unit(7, 'mm')),
      end_cap = circle(5, 'mm'),
      label_dodge = unit(0, 'mm'),
      label_push = unit(2, 'mm'),
      angle_calc = 'along',
      label_size=3,
      show.legend = FALSE
    ) +
  
    # --- Negative edges with dot ends (no arrow) ---
    geom_edge_fan(
      aes(
        width = abs(weight),  # abs for width scaling
        alpha = relative_nsig,
        label = paste0("lag = ", scales::number(avg_delay, accuracy = 1), ", ", oro_types),
        color = color,
        filter = sign==-1
      ),
      end_cap = circle(5, 'mm'),  # no arrow
      lineend = "round",   
      label_dodge = unit(0, 'mm'),
      label_push = unit(2, 'mm'),
      angle_calc = 'along',
      label_size=3,
      show.legend = FALSE
    ) +
    
    # --- Draw a point (dot) at target node of negative edges ---
    geom_point(
      data = dot_positions,
      aes(x = dot_x, y = dot_y, size = 2*log1p(weight)),
      shape = 21,             
      fill = "#d01c8b"
    ) +
    scale_size_identity()+
    
    # Plot nodes
    geom_node_point(size = 6, aes(color = name)) +
    
    scale_color_manual(values = componentAES$colour,
                       breaks = componentAES$level, 
                       guide="none")+
    scale_edge_color_identity()+
    scale_x_continuous(expand = expansion(mult=0.1))+
    scale_y_continuous(expand = expansion(mult=0.1))+

    geom_node_label(aes(label = componentAES$label[match(name, componentAES$level)]), repel = TRUE,
                   size = nodeTextSize, alpha=0.8
                   ) +

    labs(title = paste(cluster_label)) +
    theme_void() +
    theme(
      plot.title = element_text(size = titleSize, hjust = 0.5),
      plot.margin = unit(c(5,5,5,5), units = "mm")
      )
}

cluster_order <- 1:n_clusters
concept_ggraph_plots <- lapply(cluster_order, function(k) {
  plot_ggraph_network(concept_meta_graphs[[k]], k, concept_cluster_names[[k]], dot_offset = 0.1)
})




require(patchwork)

# Arrange the ggraph plots horizontally according to cluster number
final_plot <- wrap_plots(concept_ggraph_plots, nrow = 2, widths = c(1,1.2))
final_plot

# # Show or save
# ggsave(filename=here::here("figures/supplemental/conceptual_cluster_networks.pdf"), final_plot, width = 10, height = 8)

```

Negatvive Leg -> support -> action could actually be a direct effect of Leg -> action?


```{r for mCDR and MRE calculate node importance and influence}
plot_network_metrics <- function(network_graph){
  # causal influencers - legislation, policy
  influencers <- degree(network_graph, mode = "out") %>% 
    sort(decreasing = T) %>% 
    as.data.frame() %>%
    tibble::rownames_to_column("component") %>%
    mutate(metric = "Out-degree (N effects given)")
    
  # causal responders - deployment - 8
  responders <- degree(network_graph, mode = "in") %>% 
    sort(decreasing = T) %>% 
    as.data.frame() %>%
    tibble::rownames_to_column("component") %>%
    mutate(metric = "In-degree (N effects received)")
  
  
  # general influence - legislation                    policy           public interest
  eigenvalues <- sort(eigen_centrality(network_graph)$vector, decreasing =TRUE)  %>%
    as.data.frame() %>%
    tibble::rownames_to_column("component") %>%
    mutate(metric = "Eigenvector centrality")
  
  # mediators - deployment - 14
  mediators <- sort(betweenness(network_graph), decreasing=TRUE)%>%
    as.data.frame() %>%
    tibble::rownames_to_column("component") %>%
    mutate(metric = "Betweenness (N shortest paths)")
  
  
  ## format results 
  metaNetCentralityMetrics_tmp <- rbind(
    influencers,
    responders,
    eigenvalues,
    mediators
  )%>%
    mutate(
      component = factor(
        component, 
        levels = componentAES$level[componentAES$level %in% selectedComponents],
        labels = componentAES$label[componentAES$level %in% selectedComponents]
      )
    )%>%
    complete(component, metric)
  
  summary(metaNetCentralityMetrics_tmp)
  
  colnames(metaNetCentralityMetrics_tmp)[!(colnames(metaNetCentralityMetrics_tmp) %in% c("component","metric"))] <- "Value"
  
  ## make plot
  ## Plot of a heatmap of each metric value
  networkMetricstmp_ggp <- ggplot(
    data = metaNetCentralityMetrics_tmp %>%
      group_by(metric) %>%
      mutate(
        Value_scaled = scales::rescale(Value, to = c(0,1))
      ),
    aes(x=component, y=metric)
  )+
    geom_tile(aes(fill = Value_scaled))+
    # geom_text(aes(label = scales::number(Value, accuracy=0.1)))+
    scale_y_discrete(
      labels = function(x) stringr::str_wrap(x, width=15)
    )+
    scale_x_discrete(
      breaks = levels(metaNetCentralityMetrics_tmp$component),
      drop=FALSE
    )+
    scale_fill_distiller(palette = "Blues", direction=1, na.value = "grey")+
    labs(
      x = "Node",
      y="Network metric",
      fill = "Metric\n(scaled)"
    )+
    theme_minimal(base_size = 12)+
    theme(
      axis.text.x = element_text(angle = 45, hjust=1)
      # legend.position = "bottom"
    )
  # networkMetricstmp_ggp
  return(networkMetricstmp_ggp)
}


concept_metrics_heatmaps <- lapply(c(2,4), function(k) {
  plot_network_metrics(concept_meta_graphs[[k]])
})


```

```{r for mCDR and MRE compute QNM but remove because no feedbacks so results not informative, eval=FALSE}

source(here::here("R/impact.barplot.myMod"))
source(here::here("R/extend.vector"))
source(here::here("R/my.QPress.R"))
addTaskCallback(function(...) {set.seed(123);TRUE})

# Function to run press perturbation with timeout if it freezes
safe_impact_barplot <- function(ModSim, press, timeout = 30) {
  tryCatch(
    withTimeout(
      {
        impact.barplot.myMod(ModSim,
            perturb = press,
            plot=FALSE, percentage = TRUE
        )
      },
      timeout = timeout,
      onTimeout = "silent"   # do not throw error on timeout
    ),
    error = function(e) NULL,
    TimeoutException = function(e) NULL
  )
}

plot_QNM <- function(k, nSims = 5000, weightSimulation = FALSE, uncertainThreshold=0.5){
  
  # Get oro_types in this cluster
  oroTypes <- concept_cluster_df %>%
    filter(cluster == k) %>%
    pull(oro_type)

  ## 1. Pool the transfer entropy results
  sig_te_edges <- TE_results %>%
    filter(oro_type %in% oroTypes,
           NodeX %in% selectedComponents,
             NodeY %in% selectedComponents,
             TE_pval <= 0.05,
             !is.na(TE_ratio),
             0 < TE_ratio
             ) %>%
    group_by(NodeX, NodeY) %>%
    summarise(
      n_sig = n(),
      avg_ratio = mean(TE_ratio, na.rm = TRUE),
      avg_correlation = mean(optCor, na.rm=T),
      med_correlation = quantile(optCor, na.rm=T, probs = 0.5),
      avg_delay = mean(optDelay, na.rm=T),
      sign = ifelse(avg_correlation >= 0, 1, -1),
      oro_types = paste(unique(oro_type), collapse = ", "), 
      .groups = 'drop'
    )%>%
    mutate(
      Probability = n_sig / max(n_sig),
      Group = ifelse(uncertainThreshold < Probability,0,1),
      Type = ifelse(sign==1,"P","N")
    ) %>%
    arrange(desc(Probability)) %>%
    select(From=NodeX, To=NodeY, Group, Probability, Type)
  

  
  
  
  # Format the data frame for Q Press
  sig_nodes <- as.character(unique(c(sig_te_edges$From, sig_te_edges$To)))
  
  sig_te_edges <- sig_te_edges %>%
    mutate(
      From = factor(From, levels = sig_nodes),
      To = factor(To, levels=sig_nodes),
      Type = factor(Type, levels = c("N","P","U","Z")),
      # Group = factor(Group)
    ) %>%
    ungroup() %>%
    ## add Pair
    mutate(
      # Convert From and To to character to avoid issues with factors
      From_char = as.character(From),
      To_char = as.character(To),
      # Sort From and To alphabetically
      PairKey = pmin(From_char, To_char),
      PairKey = paste(PairKey, pmax(From_char, To_char), sep = "-")
    ) %>%
    mutate(Pair = as.integer(factor(PairKey)))
  
  
  
  
  allEdgesLabels <- edge.labels(sig_te_edges) 
  
  Mod <- enforce.limitation(sig_te_edges%>% select(From, To, Group, Type, Pair)) 
  
  
  ## 2. Build Qualitative network model
  # Optional: Simulate with weights?
  if(weightSimulation){
    s <- community.sampler(Mod)
    s$select(mean(uncertainSampleProbsDf$Probability))
    ModSim <- tryCatch(
      {system.simulate(nSims, Mod, s)},
      error = function(e){NULL}
    )
   
  }else{
    ModSim <- tryCatch(
      {system.simulate(nSims, Mod)},
      error = function(e){NULL}
    )
  }
  
  ## If Mod sim Null, return nothing, else calculate presses
  if(is.null(ModSim)){
    return(NULL)
  }else{
    
    ## 3. For each node (except action), simulate a press perturbation and record the response of action.
  
    # Loop over interventions on each node → observe effect on "deployment"
    all_nodes <- sig_nodes[sig_nodes != "deployment"]
    qp_results_df <- data.frame()
    for (node in all_nodes) {
      press <- c(1)
      names(press) <- node
    
      # Set evidence and query deployment
      temp <- safe_impact_barplot(ModSim, press, timeout = 30)
      if(is.null(temp)){
        return(tibble(Negative=NA, `No Change`=NA, Positive=NA, Press_perturb=node))
      }else{
        temp <- as.data.frame(t(temp["deployment",]))
        temp$Press_perturb <- node
      }
      
      qp_results_df <- qp_results_df %>% bind_rows(temp)
      
    } # end of calculating presses
    
    

    # Plot results
    uniquePP <- unique(qp_results_df$Press_perturb)
    
    qnm_ggp <- ggplot(qp_results_df, 
           aes(x=Press_perturb))+
      geom_col(aes(y=Positive), fill="#4dac26")+ 
      geom_col(aes(y=-Negative), fill = "#d01c8b")+
      
      geom_text(aes(y=Positive, label = Positive), vjust=0)+
      geom_text(aes(y=-Negative, label = Negative), vjust=1)+
      
      geom_hline(yintercept=0, col="red")+
      # scale_fill_stepsn(name = stringr::str_wrap("Positive effect on action (% sims)", width=25), n.breaks=5, colours = viridis(5))+
      scale_x_discrete(
        breaks = componentAES$level[
          componentAES$level %in% uniquePP
        ],
        labels = componentAES$label[
          componentAES$level %in% uniquePP
        ]
      )+
      scale_y_continuous(expand = expansion(mult=0.15))+
      labs(y="Effect on action (% sims)",x="Positive press perturbation node")+
      theme_minimal(base_size = 10)+
      theme(
        legend.position = "bottom",
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.x=element_text(angle=45, hjust=1)
      )
    
    return(qnm_ggp)
    
  } # end of !is.null(ModSim)
  
    
}


concept_qnm_barplots <- lapply(c(2,4), function(k){
  plot_QNM(k=k)
})

```


```{r plot analysis for mCDR and MRE OROs}

plotList <- c(
  concept_ggraph_plots[[2]],
  concept_metrics_heatmaps[[1]],
  concept_ggraph_plots[[4]],
  concept_metrics_heatmaps[[2]]
)

combined <- wrap_plots(plotList, ncol=2, axes = "collect_x", 
                       guides="collect", widths = c(2, 1))+
  plot_annotation(tag_levels = "a")

ggsave(
  here::here("figures/supplemental/networks_mCDR_MRE.pdf"),
  width=9, height=8, 
  plot=combined
)

```

### Step 2.2: Create Network for ALL OROs from TE edges

```{r Aggregate Causal Strengths Across all oros - create meta network, eval = FALSE}
meta_links <- TE_results %>%
  filter(NodeX %in% selectedComponents,
           NodeY %in% selectedComponents,
           TE_pval <= 0.05,
           !is.na(TE_ratio),
           0 < TE_ratio
           ) %>%
  group_by(NodeX, NodeY) %>%
  summarise(
    n_sig = n(),
    avg_ratio = mean(TE_ratio, na.rm = TRUE),
    avg_correlation = mean(optCor, na.rm=T),
    med_correlation = quantile(optCor, na.rm=T, probs = 0.5),
    avg_delay = mean(optDelay, na.rm=T),
    sign = ifelse(avg_correlation >= 0, 1, -1),
    oro_types = paste(unique(oro_type), collapse = ", "), 
    .groups = 'drop'
  )%>%
  mutate(
    relative_nsig = n_sig / max(n_sig)
  ) %>%
  arrange(desc(relative_nsig), desc(avg_ratio)) 


# Make meta graph
meta_graph <- graph_from_data_frame(meta_links, directed = TRUE)

# Set plotting attributes
E(meta_graph)$weight <- meta_links$avg_ratio
E(meta_graph)$width <- log1p(E(meta_graph)$weight) * 1.5
E(meta_graph)$color <- scales::alpha(ifelse(E(meta_graph)$avg_correlation >= 0, "#4dac26", "#d01c8b"),meta_links$relative_nsig)
E(meta_graph)$sign <- meta_links$sign


# quick plot
plot(meta_graph)



```

Quantify Node Importance and Influence using network centrality metrics:

* Out-degree / in-degree: raw count of causal effects given/received, ie driver vs responder.
* Eigenvector : influence in the broader network.
* Betweenness: mediators .


```{r calculate node importance and influence, eval = FALSE}
# causal influencers - legislation, policy
influencers <- degree(meta_graph, mode = "out") %>% 
  sort(decreasing = T) %>% 
  as.data.frame() %>%
  tibble::rownames_to_column("component") %>%
  mutate(metric = "Out-degree (N effects given)")
  
# causal responders - deployment - 8
responders <- degree(meta_graph, mode = "in") %>% 
  sort(decreasing = T) %>% 
  as.data.frame() %>%
  tibble::rownames_to_column("component") %>%
  mutate(metric = "In-degree (N effects received)")


# general influence - legislation                    policy           public interest
eigenvalues <- sort(eigen_centrality(meta_graph)$vector, decreasing =TRUE)  %>%
  as.data.frame() %>%
  tibble::rownames_to_column("component") %>%
  mutate(metric = "Eigenvector centrality")

# mediators - deployment - 14
mediators <- sort(betweenness(meta_graph), decreasing=TRUE)%>%
  as.data.frame() %>%
  tibble::rownames_to_column("component") %>%
  mutate(metric = "Betweenness (N shortest paths)")


## format results and save

metaNetCentralityMetrics <- rbind(
  influencers,
  responders,
  eigenvalues,
  mediators
)%>%
  mutate(
    component = factor(
      component, 
      levels = componentAES$level,
      labels = componentAES$label
    )
  )
colnames(metaNetCentralityMetrics)[2] <- "Value"


## Save results
metaNetCentralityMetricsSummary <- metaNetCentralityMetrics %>%
  group_by(metric) %>%
  slice_max(n=3,order_by = Value)

write.csv(
  metaNetCentralityMetricsSummary,
  file = here::here("outputs/metaNetworkMetrics_summary.csv")
)

write.csv(
  metaNetCentralityMetrics,
  file = here::here("outputs/metaNetworkMetrics_all.csv")
)


```

```{r plot meta graph network and metric heatmap, eval=FALSE}

## Plot of the network
plot_ggraph_meta_network <- function(g, plotTitle=NULL, titleSize = 10, nodeTextSize = 3, dot_offset = 0.01) {
  g_tidy <- as_tbl_graph(g)
  
  # Code for plotting dots at end of negative effects
  # --- Create layout (needed to access node positions) ---
  graph_layout <- create_layout(g_tidy, layout = "circle")
  
  # --- Extract negative edge targets ---
  neg_edges <- g_tidy %>%
    activate(edges) %>%
    filter(sign == -1) %>%  # Filter for negative edges
    as_tibble() %>%
    mutate(
      from_name = V(g_tidy)$name[from],
      to_name = V(g_tidy)$name[to]
    )
  
  # Get positions of source and target nodes for negative edges
  edge_positions <- graph_layout %>%
    filter(name %in% c(neg_edges$from_name, neg_edges$to_name))
  
  # Now calculate the offset position for the dot at the target end of the edge:
  # We will add an offset to the target node's position in the direction of the edge
  dot_positions <- neg_edges %>%
    rowwise() %>%
    mutate(
      # Get source and target positions
      source_x = edge_positions$x[edge_positions$name == from_name],
      source_y = edge_positions$y[edge_positions$name == from_name],
      target_x = edge_positions$x[edge_positions$name == to_name],
      target_y = edge_positions$y[edge_positions$name == to_name],
      # Compute the unit vector from source to target and apply the offset
      dx = target_x - source_x,
      dy = target_y - source_y,
      # Offset the dot by a small distance (here 5% of the edge vector)
      dot_x = target_x + -dot_offset * dx,
      dot_y = target_y + -dot_offset * dy
    )
  
  
  ## plot
  gg <- ggraph(g_tidy, layout = "circle") +  # You can try "kk" or "circle" or "fr" for spacing
    
    # # --- Positive edges with arrows ---
    geom_edge_fan(
      aes(
        width = weight,
        alpha = relative_nsig,
        label = paste0("lag = ", scales::number(avg_delay, accuracy = 1)),
        color = color,
        filter = sign==1
      ),
      arrow = arrow(length = unit(7, 'mm')),
      end_cap = circle(5, 'mm'),
      label_dodge = unit(0, 'mm'),
      label_push = unit(2, 'mm'),
      angle_calc = 'along',
      label_size=3,
      show.legend = FALSE
    ) +
  
    # --- Negative edges with dot ends (no arrow) ---
    geom_edge_fan(
      aes(
        width = abs(weight),  # abs for width scaling
        alpha = relative_nsig,
        label = paste0("lag = ", scales::number(avg_delay, accuracy = 1)),
        color = color,
        filter = sign==-1
      ),
      end_cap = circle(5, 'mm'),  # no arrow
      lineend = "round",   
      label_dodge = unit(0, 'mm'),
      label_push = unit(2, 'mm'),
      angle_calc = 'along',
      label_size=3,
      show.legend = FALSE
    ) +
    
    # --- Draw a point (dot) at target node of negative edges ---
    geom_point(
      data = dot_positions,
      aes(x = dot_x, y = dot_y, size = 2*log1p(weight),alpha = relative_nsig),
      shape = 21,             
      fill = "#d01c8b"
    ) +
    scale_size_identity()+
    scale_alpha(guide="none")+
    
    # Plot nodes
    geom_node_point(size = 7, aes(color = name)) +
    scale_color_manual(values = componentAES$colour,
                       breaks = componentAES$level, 
                       guide="none")+
    scale_edge_color_identity()+
    scale_x_continuous(expand = expansion(mult=0.1))+
    scale_y_continuous(expand = expansion(mult=0.1))+

    geom_node_label(aes(label = componentAES$label[match(name, componentAES$level)]), repel = TRUE,
                   size = nodeTextSize, alpha=0.8
                   ) +
    theme_void() +
    theme(
      plot.title = element_text(size = titleSize, hjust = 0.5),
      plot.margin = unit(c(5,5,5,5), units = "mm")
      )
  
  
  if(!is.null(plotTitle)){
    gg <- gg + labs(title = paste(plotTitle)) +
      theme(
        plot.title = element_text(size = titleSize, hjust = 0.5)
      )
  }
  return(gg)
}

metaNetwork_ggp <- plot_ggraph_meta_network(
  meta_graph,nodeTextSize = 3.5, dot_offset = 0.05
)




metaNetwork_ggp

## Plot of a heatmap of each metric value
networkMetrics_ggp <- ggplot(
  data = metaNetCentralityMetrics %>%
    group_by(metric) %>%
    mutate(
      Value_scaled = scales::rescale(Value, to = c(0,1))
    ),
  aes(x=component, y=metric)
)+
  geom_tile(aes(fill = Value_scaled))+
  scale_y_discrete(
    labels = function(x) stringr::str_wrap(x, width=15)
  )+
  scale_fill_distiller(palette = "Blues", direction=1)+
  labs(
    x = "Node",
    y="Network metric",
    fill = "Metric\n(scaled)"
  )+
  theme_minimal(base_size = 12)+
  theme(
    axis.text.x = element_text(angle = 45, hjust=1)
    # legend.position = "bottom"
  )
networkMetrics_ggp


## combine plots
ggarrange(plotlist = list(metaNetwork_ggp, networkMetrics_ggp),
          labels = paste0(letters[1:2], ")"),
          label.x = 0, vjust =1.2, font.label = list(size = 13)
)  %>%
  ggpubr::ggexport(filename = here::here("figures/supplemental/metaNetwork_Graph_and_MetricHeatmap.pdf"),
                   nrow = 1,
           width = 10, height= 4)


```






### Step 3: Build a Qualitative Network Model


Aim: Drawing upon the causal links identified using transfer entropy (ie. the meta-network in step 2), what does it imply about the drivers of ORO action? What levers can be pulled to affect change?

Analysis:

1. Pool the transfer entropy results. If an edge is present in an ORO-level network, keep the edge in the meta network. Optional: if the level of agreement is below a certain threshold, the edge is uncertain and simulated with a probability weight (for now this is set to FALSE)

2. From the pooled network, create a qualitative network model

3. For each node (except action), simulate a press perturbation and record the response of action.


*Kieran questions*

* Does this approach seem sensible? 
* Do you notice any errors in the analysis/interpretation?


```{r build qualitative network model for the meta network -- all oros, eval = FALSE}

nSims <- 5000

source(here::here("R/impact.barplot.myMod"))
source(here::here("R/extend.vector"))
source(here::here("R/my.QPress.R"))

addTaskCallback(function(...) {set.seed(123);TRUE})

## Constant variables
# Do we want to simulate with probability weights? T/F
weightSimulation = FALSE
# the threshold of agreement below which the edge is sample with a probability weight of 0.5
uncertainThreshold = 0.5 
oroTypes <- unique(allComponentDat_model$oro_type)




## 1. Pool the transfer entropy results. 
sig_te_edges <- meta_links %>%
  mutate(
    Probability = relative_nsig,
    Group = ifelse(uncertainThreshold < Probability,0,1),
    Type = ifelse(sign==1,"P","N")
  ) %>%
  arrange(desc(Probability)) %>%
  select(From=NodeX, To=NodeY, Group, Probability, Type)



# Format the data frame for Q Press
sig_nodes <- as.character(unique(c(sig_te_edges$From, sig_te_edges$To)))

sig_te_edges <- sig_te_edges %>%
  mutate(
    From = factor(From, levels = sig_nodes),
    To = factor(To, levels=sig_nodes),
    Type = factor(Type, levels = c("N","P","U","Z")),
    # Group = factor(Group)
  ) %>%
  ungroup() %>%
  ## add Pair
  mutate(
    # Convert From and To to character to avoid issues with factors
    From_char = as.character(From),
    To_char = as.character(To),
    # Sort From and To alphabetically
    PairKey = pmin(From_char, To_char),
    PairKey = paste(PairKey, pmax(From_char, To_char), sep = "-")
  ) %>%
  mutate(Pair = as.integer(factor(PairKey)))




allEdgesLabels <- edge.labels(sig_te_edges) 
  
# Mod <- retain.groups(sig_te_edges, groups=0) # all certain interactions
Mod <- enforce.limitation(sig_te_edges%>% select(From, To, Group, Type, Pair)) # enforce self-limiting effect
# s<- community.sampler(Mod)
# s$select(0.5)
# ModSim <- system.simulate(100, Mod,sampler = s) # simulate
# print(impact.table(ModSim))


## 2. Build Qualitative network model
# Optional: Simulate with weights?
if(weightSimulation){
  # uncertainSampleProbsDf <- sig_te_edges %>%
  # filter(Group == 1) %>%
  # select(Pair, Probability) %>%
  # distinct(Pair,.keep_all = TRUE)

  # ModSim <- my.system.simulate(nSims, Mod,required.groups = c(0), uncertainSampleProbsDf=uncertainSampleProbsDf)

  # Or uncertain with probability
  s <- community.sampler(Mod)
  s$select(mean(uncertainSampleProbsDf$Probability))
  ModSim <- system.simulate(nSims, Mod, s)
 
}else{
  ModSim <- system.simulate(nSims, Mod)
}


## 3. For each node (except action), simulate a press perturbation and record the response of action.

# Loop over interventions on each node → observe effect on "deployment"
all_nodes <- sig_nodes[sig_nodes != "deployment"]
qp_results_df <- data.frame()
for (node in all_nodes) {
  press <- c(1)
  names(press) <- node

  # Set evidence and query deployment
  temp <- impact.barplot.myMod(ModSim,
      perturb = press,
      plot=FALSE, percentage = TRUE
  )
  temp <- as.data.frame(t(temp["deployment",]))
  temp$cluster <- "Meta-network"
  temp$Press_perturb <- node
  
  qp_results_df <- qp_results_df %>% bind_rows(temp)
  
}

 

## Results
# Plot results
uniquePP <- unique(qp_results_df$Press_perturb)

qnm_ggp <- ggplot(qp_results_df, 
       aes(x=Press_perturb))+
  geom_col(aes(y=Positive), fill="#4dac26")+ 
  geom_col(aes(y=-Negative), fill = "#d01c8b")+
  
  geom_text(aes(y=Positive, label = Positive), vjust=0)+
  geom_text(aes(y=-Negative, label = Negative), vjust=1)+
  
  geom_hline(yintercept=0, col="red")+
  # scale_fill_stepsn(name = stringr::str_wrap("Positive effect on action (% sims)", width=25), n.breaks=5, colours = viridis(5))+
  scale_x_discrete(
    breaks = componentAES$level[
      componentAES$level %in% uniquePP
    ],
    labels = componentAES$label[
      componentAES$level %in% uniquePP
    ]
  )+
  scale_y_continuous(expand = expansion(mult=0.15))+
  labs(y="Effect on action (% sims)",x="Positive press perturbation node")+
  theme_minimal(base_size = 10)+
  theme(
    legend.position = "bottom",
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    axis.text.x=element_text(angle=45, hjust=1)
  )

qnm_ggp


## Save plot
ggsave(
  here::here("figures/supplemental/qp_metaNetwork_barplot.pdf"),
  plot = qnm_ggp,
  width=5, height=4, units="in"
)

```

```{r try with only certain edges but doesnt work it freezes}
# ### Recalculate with only most certain edges - doesnt work freezes --------
# sig_te_edges_cert <- sig_te_edges %>%
#   filter(Group == 0) 
# 
# allEdgesLabels_cert <- edge.labels(sig_te_edges) 
#   
# Mod_cert <- retain.groups(sig_te_edges, groups=0) # all certain interactions
# ModSim_cert <- system.simulate(nSims, Mod_cert)
# 
# all_nodes_cert <- node.labels(Mod_cert)
# all_nodes_cert <- all_nodes_cert[all_nodes_cert != "deployment"]
# 
# qp_results_df_cert <- data.frame()
# for (node in all_nodes_cert) {
#   press <- c(1)
#   names(press) <- node
# 
#   # Set evidence and query deployment
#   temp <- impact.barplot.myMod(ModSim_cert,
#       perturb = press,
#       plot=FALSE, percentage = TRUE
#   )
#   temp <- as.data.frame(t(temp["deployment",]))
#   temp$cluster <- "Meta-network"
#   temp$Press_perturb <- node
#   
#   qp_results_df_cert <- qp_results_df_cert %>% bind_rows(temp)
#   
# }
# 
#  
# 
# ## Results
# # Plot results
# uniquePP_cert <- unique(qp_results_df_cert$Press_perturb)
# 
# qnm_cert_ggp <- ggplot(qp_results_df_cert, 
#        aes(x=Press_perturb))+
#   geom_col(aes(y=Positive))+
#   geom_col(aes(y=-Negative))+
#   
#   geom_text(aes(y=Positive, label = Positive), vjust=0)+
#   geom_text(aes(y=-Negative, label = Negative), vjust=1)+
#   
#   geom_hline(yintercept=0, col="red")+
#   # scale_fill_stepsn(name = stringr::str_wrap("Positive effect on action (% sims)", width=25), n.breaks=5, colours = viridis(5))+
#   scale_x_discrete(
#     breaks = componentAES$level[
#       componentAES$level %in% uniquePP
#     ],
#     labels = componentAES$label[
#       componentAES$level %in% uniquePP
#     ]
#   )+
#   scale_y_continuous(expand = expansion(mult=0.15))+
#   labs(y="Effect on action (% sims)",x="Press perturbation")+
#   theme_minimal(base_size = 10)+
#   theme(
#     legend.position = "bottom",
#     axis.text.y = element_blank(),
#     axis.ticks.y = element_blank(),
#     axis.text.x=element_text(angle=45, hjust=1)
#   )
# 
# qnm_ggp_cert



```


```{r plot all network figures together, eval = FALSE}

plotList = list(A=metaNetwork_ggp+theme(plot.margin = unit(rep(0.1, 4), "cm")), 
                B=networkMetrics_ggp+theme(plot.margin = unit(rep(0.2, 4), "cm")),
                C=qnm_ggp+theme(plot.margin = unit(rep(0.1, 4), "cm")))

pltLayout <- "#AA#
              BBCC"

combined_patchwork <- wrap_plots(plotList, ncol = 2, heights = c(1.25,1), 
                                 widths = c(0.05,0.5,0.5,0.4), #c(0.8,1), 
                                 design = pltLayout) + plot_annotation(tag_levels = 'a')

ggsave(here::here("figures/main/metaNetwork_networkMetricHeatmap_QNMbarplot.pdf"),
       combined_patchwork, width = 9, height = 8)

```



```{r, out.width="0.9\\linewidth", include=TRUE, fig.align="center", fig.cap=c("Network analysis figures a) meta netowrk, b) network metrics, c) quanitative network model response of action to press perturbation"), echo=FALSE}

knitr::include_graphics(here::here("figures/main/metaNetwork_networkMetricHeatmap_QNMHeatmap.pdf"))
```








# Narrative synthesis.

*Key message*
Key changes in policy/legislation, catalysed by the right socio-political environment, can have enormous impact on publication & action.


## All mitigation OROs and international policy

```{r all components all mitigation OROs}
# allComponentDat_model_allMitOROs


intPolMoments <- readxl::read_excel(
  here::here("data/raw-data/international_policy_moments.xlsx")
)%>%
  mutate(
    year_min = as.numeric(year_min),
    year_max = as.numeric(year_max),
    component = "International policy"
  )


tmpDat <- allComponentDat_model_allMitOROs %>%
  filter(!is.na(year), component %in% selectedComponents) %>%
  mutate(
    y = ifelse(component %in% 
                 c("deployment"), y, log(y))
  ) %>%
  bind_rows(intPolMoments)
  

facetLabels <- c("Publications" = "Publications\nlog(N articles)",
                 "Policy" = "Policy\nlog(N documents)",
                 "Legislation" = "Legislation\nlog(N documents)",
                 "Interest" = "Interest\nlog(N posts)",
                 "Support" = "Support\nlog(N positive posts + likes)",
                 # "Opposition" = "Opposition\nlog(N negative posts + likes)",
                 "International policy"="International policy moments"
                 )

tmpDat <- tmpDat %>%
  mutate(
    component = factor(component, levels = c(componentAES$level, "International policy"), 
                       labels = c(componentAES$label, "International policy"))
  ) %>%
  mutate(
    component = droplevels(component)
  )%>%
  mutate(
    component = plyr::revalue(component, facetLabels)
  )



ggp_tmp <- ggplot()+
  geom_line(data = tmpDat, aes(x=year, y=y))+
  geom_rect(data = tmpDat %>% filter(grepl("international", component, ignore.case=T)),
           aes(xmin=.data$year_min, xmax=.data$year_max),
           ymin = 0, ymax=Inf,
           fill="grey")+
  geom_vline(data = tmpDat %>% filter(grepl("international", component, ignore.case=T)),
           aes(xintercept=.data$year_min),
           linewidth = 0.5, linetype = "solid", col="black")+
  geom_text(data = tmpDat %>% 
              filter(grepl("international", component, ignore.case=T)) %>%
              mutate(
                y= rep(seq(0.1,0.7, length.out=3),ceiling(nrow(intPolMoments)/3))[1:nrow(intPolMoments)]
              ),
           aes(x=.data$year_min, y=.data$y, label= stringr::str_wrap(.data$label, width=10)),
           angle=30, hjust=0, col="black", size=2, nudge_x = 0.1)+
  scale_x_continuous(limits = year_lim)+
  facet_wrap(vars(component),ncol=1, scales = "free_y")+
  labs(
    y="",
    x="Year"
  )+
  theme_minimal(base_size = 10)+
  coord_cartesian(clip="off")+
  theme(
    plot.margin = unit(c(0.1, 0.1,0.1,0.1), units = "cm")
  )
ggp_tmp

```


```{r remove - broad-scale timeseries of all mitigation ORO publications and international policy moments}

## Plot of international policy moments
intPolMoments <- readxl::read_excel(
  here::here("data/raw-data/international_policy_moments.xlsx")
)%>%
  mutate(
    year_min = as.numeric(year_min),
    year_max = as.numeric(year_max),
    component = "International policy"
  ) %>%
  arrange(year_min)

tmpYlims <- c(min(intPolMoments$year_min), max(intPolMoments$year_max)+2)

ggp_intPolMom <- ggplot()+
  geom_rect(data = intPolMoments,
           aes(xmin=.data$year_min, xmax=.data$year_max),
           ymin = -Inf, ymax=Inf,
           fill="grey")+
  geom_vline(data = intPolMoments,
           aes(xintercept=.data$year_min),
           linewidth = 0.5, linetype = "solid", col="black")+
  geom_text(data = intPolMoments %>%
              mutate(
                y= rep(seq(0.1,0.7, length.out=3),ceiling(nrow(intPolMoments)/3))[1:nrow(intPolMoments)]
              ),
           aes(x=.data$year_min, y=.data$y, label= stringr::str_wrap(.data$label, width=10)),
           angle=30, hjust=0, col="black", size=2, nudge_x = 0.2)+
  scale_x_continuous(limits = tmpYlims)+
  scale_y_continuous(limits = c(0,1))+
  labs(
    y="",
    x="Year"
  )+
  theme_minimal(base_size = 10)+
  coord_cartesian(clip="off")+
  theme(
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    plot.margin = unit(c(0.1, 0.1,0.1,0.1), units = "cm")
  )

ggp_intPolMom


## Plot of ORO publications
p1_db <- RSQLite::dbConnect(RSQLite::SQLite(),
                      here::here("data","sqlite-databases","product1.sqlite"),
                                 create=FALSE)

uniquerefs <- tbl(p1_db, "uniquerefs_update2025") %>%
  select(analysis_id, year) 

predOroType <- tbl(p1_db, "pred_oro_type_mit_long") %>%
  left_join(uniquerefs, by = "analysis_id") %>%
  collect()

dbDisconnect(p1_db)


allPub <- predOroType %>%
  mutate(
    std_prediction = ifelse(is.na(std_prediction), 0, std_prediction),
    level = gsub("[.]","-", level),
    year= as.numeric(year)
    ) %>%
  filter(!is.na(year)) %>%
  mutate(
    lower_prediction = mean_prediction - std_prediction,
    upper_prediction = mean_prediction + std_prediction
  ) %>%
  group_by(year) %>%
  summarise(
    y_mean = n_distinct(analysis_id[0.5 <= mean_prediction]),
    y_lower = n_distinct(analysis_id[0.5 <= lower_prediction]),
    y_upper = n_distinct(analysis_id[0.5 <= upper_prediction])
  )%>%
  mutate(level = paste("All mitigation OROs"))



allPub_ggp<- ggplot(allPub %>% filter(tmpYlims[1]-5 <= year, 
                                      year <= tmpYlims[2]+5), aes(x=year))+ #
  geom_line(aes(y=log(y_mean), col=level))+
  geom_ribbon(aes(ymin = log(y_lower), ymax = log(y_upper), fill = level), alpha = 0.5)+
  scale_color_manual(
    breaks = c("All mitigation OROs", typeAES$level),
    values = c("#35a7d9", typeAES$colour),
    labels = c("All mitigation OROs", typeAES$label),
    guide="none"
  )+
  scale_fill_manual(
    breaks = c("All mitigation OROs", typeAES$level),
    values = c("#35a7d9", typeAES$colour),
    labels = c("All mitigation OROs", typeAES$label),
    guide="none"
  )+
  scale_y_continuous()+
  scale_x_continuous(limits = tmpYlims)+
  # guides(col=guide_legend(nrow=2))+
  labs(x="Year", y = "log(N publications)")+
  theme_minimal()+
  theme(
    legend.position = "none"
    # axis.text.x = element_text(angle=45, hjust=1)
  )

allPub_ggp


## Combine and save plots

combined_plots <- wrap_plots(ggp_intPolMom, allPub_ggp, ncol=1, axes = "collect_x")+
  plot_annotation(tag_levels = "a")


ggsave(
  here::here("figures/supplemental/intPolicyMoments_and_allMitPublications.pdf"),
  plot = combined_plots,
  width=6, height=4
)

```


## Break point analyses

```{r changepoint analysis all oros, eval=FALSE}

# The oro types to analyse 
oroTypes = unique(allComponentDat_model$oro_type)


# Jags initialization parameters
jagsInits <- list(
  # Maximum number of changepoints to look for
  "K_max"=3,
  # Time buffer for looking for change points. 
  # i.e. don't find a change point at the first or last year
  "cpmin"="jagsData$MINX+1", 
  "cpmax"="jagsData$MAXX-1")

# Compile all data into a list to iterate through
dataList <- list()
for(c in selectedComponents){
  dataList[[c]] <- allComponentDat_model %>% filter(oro_type %in% oroTypes, component == c)
 
}

# Compile all model inputs into a list to iterate through
modelInputs <- list(
  "data" = dataList,
  "bugsModFiles" = list(
    "deployment" = "R/bugs-models/GammaModMultCP", # continuous non-negative
    "publications" = "R/bugs-models/PoissonModMultCP", 
    "policy" ="R/bugs-models/PoissonModMultCP", #"R/bugs-models/ZIPModMultCP", # zero-inflated poisson
    "legislation" = "R/bugs-models/PoissonModMultCP", #"R/bugs-models/ZIPModMultCP"
    "public interest" = "R/bugs-models/PoissonModMultCP",
    "public support" = "R/bugs-models/PoissonModMultCP",
    "public opposition"= "R/bugs-models/PoissonModMultCP"
  ),
  "jagsData" = list(jagsInits)[rep(1,length(components))]
)

# The different time series components to analyse (e.g. deployment, legislation, policy, posts, etc)
components <- names(modelInputs$bugsModFiles)

## Loop through all oro types and time series (components) to run change point analysis
cutpoint_results <- data.frame()
cutpoint_densities <- data.frame()

for(oro in oroTypes){
  # oro = "CCS"
  
  for(i in 1:length(components)){
    # i = 4
      
    print(paste(oro,":", components[i]))
    
    # get model to fit
    modFile <- modelInputs$bugsModFiles[[components[i]]]
    
    # If OAE deployment, y is poisson not Gamma so change
    if(components[i] == "deployment" & oro %in% c("CDR-OAE")){
      modFile <- "R/bugs-models/PoissonModMultCP"
    }
    
    # Format data
    jagsData <- modelInputs$jagsData[[i]]
    myinits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 123)
    dat <- modelInputs$data[[components[i]]][modelInputs$data[[components[i]]]$oro_type == oro,]
    if(nrow(dat) < 5){
      print("Insufficient data")
      next
    }
    yearLims <- range(modelInputs$data[[components[i]]][
      modelInputs$data[[components[i]]]$oro_type == oro,"year"
    ], na.rm=T)
    
    # Format specific inputs depending on the model
    if(grepl("pubBinom", modFile)){ 
      trialVar <- ifelse(grepl("ORO", modelInputs$y_variable[[i]]),
                         "n_OC","total_posts")
      successVar <- ifelse(grepl("ORO", modelInputs$y_variable[[i]]),
                         "n_ORO","n_posts")
      # if a proportion, only keep proportions of total values above 100 # modelInputs$y_variable[[i]] == "prop_ORO"
      keepYears <- dat$year[dat$variable_name == trialVar & 100 < dat$y]
      dat <- dat[dat$year %in% keepYears,]
      dat <- dat %>% arrange(variable_name, year)
      nTrial <- dat$y[dat$variable_name == trialVar]
      jagsData$nTrial <- nTrial
      jagsData$nSuccess <- dat$y[dat$variable_name == successVar]
      # datCast <- reshape2::dcast(dat, ... ~ variable_name, value.var = "y")
    }
    
    if(grepl("ZIP", modFile)){ 
      # if the model is zero inflated poisson, 
      # Fill missing years with 0 values
      dat <- dat %>% tidyr::complete(year = yearLims[1]:yearLims[2])
      dat$y[is.na(dat$y)] <- 0
      dat <- dat %>% tidyr::fill(component, oro_type, variable_name)
      # add the latent binomial indicator, as to whether there are no documents at all
      myinits$w <- dat$y
      myinits$w[myinits$w > 0] <- 1
      
    }
    
    if(grepl("Poisson", modFile)){ 
      # Fill missing years with 0 values
      dat <- dat %>% tidyr::complete(year = yearLims[1]:yearLims[2])
      dat$y[is.na(dat$y)] <- 0
      dat$y <- round(dat$y) # force discrete y values
      dat <- dat %>% tidyr::fill(component, oro_type, variable_name)
      
    }
    
    if(grepl("Gamma", modFile)){
      # Fill missing years with 0 values
      dat <- dat %>% tidyr::complete(year = yearLims[1]:yearLims[2])
      dat$y[is.na(dat$y)] <- 0
      dat$y[dat$y==0] <- 0.1
      dat <- dat %>% tidyr::fill(component, oro_type, variable_name)
    }
    
    # make sure no NAs
    dat <- na.omit(dat)
    
    # Skip if after subsetting data there is insufficient data points
    if(nrow(dat) < 5){
      print("Insufficient data after subsetting")
      next
    }
    #with(dat, plot(year, y))
    
    jagsData$y <- dat$y
    jagsData$x <- dat$year 
    jagsData$MINX <- min(jagsData$x)
    jagsData$MAXX <- max(jagsData$x)
    jagsData$cpmin <- eval(parse(text = jagsData$cpmin))
    jagsData$cpmax <- eval(parse(text = jagsData$cpmax))
    
    if((jagsData$cpmax-jagsData$cpmin)<5){
      jagsData$cpmin <- jagsData$MINX
      jagsData$cpmax <- jagsData$MAXX
    }
    
    # jagsData$log_y <- log(dat$y)
    # jagsData$N <- length(dat$y)
    
    ## Fit mdel
    bugs.model <- readChar(modFile, file.info(modFile)$size)
    
    jagsModel <- tryCatch(
      {
        rjags::jags.model(
          file = textConnection(bugs.model),
          data = jagsData,
          inits = myinits,
          n.chains = 5,
          n.adapt = 1500,
          quiet = FALSE
        )
        },
     error = function(e){
       return(NULL)
     }
    )
    if(is.null(jagsModel)){
      next
    }
    
    updatedOK <- tryCatch(
      {
        update(jagsModel, 1000)
        TRUE
      },
      error = function(e) {
        message("update() failed: ", e$message)
        FALSE
      }
    )
    
    if (!updatedOK) {
      message("Retrying with K_max-1")
      jagsData$K_max <- jagsData$K_max - 1
      
      jagsModel <- tryCatch(
        {
          rjags::jags.model(
            file = textConnection(bugs.model),
            data = jagsData,
            inits = myinits,
            n.chains = 5,
            n.adapt = 1500,
            quiet = FALSE
          )
        },
        error = function(e) {
          message("Re-fit failed at model creation: ", e$message)
          return(NULL)
        }
      )
      
      if (is.null(jagsModel)) {
        next
      }
      
      updatedOK <- tryCatch(
        {
          update(jagsModel, 1000)
          TRUE
        },
        error = function(e) {
          message("Re-fit update() failed again: ", e$message)
          FALSE
        }
      )
      
      if (!updatedOK) {
        next
      }
      
    }
    
    
    
    s <- tryCatch(
      {
        rjags::coda.samples(
          model = jagsModel,
          variable.names = c("alpha","beta","K","x_cp","z","pk"),
          n.iter = 1000,
          quiet = FALSE
        )
      },
      error = function(e){
        return(NULL)
      }
    )
    if(is.null(s)){
      next
    }
    
    qs <- summary(s)$quantile
    # qs
    # plot(s[,"pk[2]"]) # check mixing
    
    # Subset to only the change points that are likely 
    # And if there are likely change points, only those that indicate a positive change
    K <- round(quantile(qs["K",], 0.5))
    if(grepl("ZIP", modFile)){
      K <- sum(0.5 <= qs[grepl("pk", rownames(qs)),"50%"])
    }
    
    if(K == 0){
      print("No significant change points")
      next
    }else{
      pk <- qs[grep("pk", rownames(qs)),"50%"] # probability for each change point
      x_cp <- qs[grep("x_cp", rownames(qs)),"50%"] # locations of each change point
      # arrange in order of decreasing probability to find the most K probable
      pkIndKeep <- order(pk, decreasing=T) 
      pkIndKeep <- pkIndKeep[1:K] 
      # then re-arrange in chronological order for calculating trends of segments
      pkIndKeep <- sort(pkIndKeep) 
      
      # Find which change points mark a positive change in trend
      trends <- vector("numeric", K+1) # store the trend for each segment
      
      # starts <- c(min(jagsData$x), round(x_cp[pkIndKeep])) 
      starts <- c(min(jagsData$x), sort(round(x_cp[pkIndKeep])))
      match(starts, round(x_cp[pkIndKeep]))
      ends <- c(round(x_cp[pkIndKeep]), max(jagsData$x))
      for(k in 1:(K+1)){
        if(grepl("Binom", modFile)){
          trends[k] <- jagsData$y[which.min(abs(jagsData$x-ends[k]))] - jagsData$y[which.min(abs(jagsData$x-starts[k]))]
        }else if(grepl("ZIP|Pois|Gamma", modFile)){
          trends[k] <- sum(jagsData$y[which.min(abs(jagsData$x-starts[k])):which.min(abs(jagsData$x-ends[k]))] )
        }
        
      }
      # only keep the positive changes in trends
      matchInd <- match(starts[-1], round(x_cp[pkIndKeep]))
      pkIndKeep <- pkIndKeep[matchInd[which(0 < diff(trends))]] 
      
      if(length(pkIndKeep) == 0){
        print("no changepoints marking increasing trend")
        next
      }else{
        pkIndNames <- names(x_cp[pkIndKeep])
        
        # # If there are two cut points within 3 years of each other, 
        # # keep most probable one 
        # if(1 < length(pkIndKeep)){
        #   if(diff(qs[pkIndNames,"50%"]) < 3){
        #     pkIndNames <- pkIndNames[1]
        #   }
        # }
        
        # save probability density of the cut points for plotting
        calc_density <- function(vals){
          dens <- density(vals, n=100)
          return(data.frame("year"=dens$x, "cp_density"=dens$y))
        }
        densTemp <- do.call(rbind, s)
        densTemp <- as.matrix(densTemp[,pkIndNames], ncol=length(pkIndNames))
        densTemp <- do.call(rbind.data.frame, apply(densTemp, 2, function(x) calc_density(x)))
        densTemp$cp_id <- rep(pkIndNames, 100)[sort(rep(1:length(pkIndNames),100))]
        
        
        # Save summary of quantiles
        dfTemp <- do.call(rbind, apply(qs[pkIndNames,,drop=FALSE], 1, function(x) as.data.frame(x)))
        dfTemp$quantile = rep(colnames(qs), length(pkIndNames))
        dfTemp$cp_id <- rep(pkIndNames, 5)[sort(rep(1:length(pkIndNames),5))] #rep(1:length(pkIndNames), 5) %>% sort
        rownames(dfTemp) <- NULL
        # Cap the distribution of cutpoints at the hard limits of the data
        colnames(dfTemp)[which(colnames(dfTemp) == "x")] <- "year"
        dfTemp$year <- ifelse(dfTemp$year < yearLims[1], yearLims[1], dfTemp$year)
        dfTemp$year <- ifelse(yearLims[2] < dfTemp$year, yearLims[2], dfTemp$year)
        # Add id variables
        addIdVariables <- function(df){
          df %>% mutate(
                        oro_type = oro,
                        component = dat$component[1],
                        variable_name = dat$variable_name[1])
        }
        dfTemp <- addIdVariables(dfTemp)
        densTemp <- addIdVariables(densTemp)
        
        ## Bind data to results
        cutpoint_results <- rbind(cutpoint_results, dfTemp)
        cutpoint_densities <- rbind(cutpoint_densities, densTemp)
        # remove jags model
        rm(jagsModel)
      }
    }

      

  } # end looping through ORO types
} # end looping through bugs models



## Save results
save(cutpoint_results,cutpoint_densities, file=here::here("outputs/cutpointResults_mitigation_alloros.RData"))
```



```{r plot all time series and international policy moments, eval=FALSE}
load(here::here("outputs/cutpointResults_mitigation_alloros.RData"))

savePlot = TRUE

oro_groups <- list(
  "mCDR" = c("CDR-BioPump","CDR-BC","CDR-OAE"),
  "MRE" = c("MRE-Ocean","MRE-Located","MRE-Bio"),
  "Other"= c("CCS","Efficiency"),
  "narrative" = c("MRE-Located", "CDR-OAE","Efficiency")
)


## Plot time series by group
ggps <- vector("list", length(oro_groups))

for(og in 1:length(oro_groups)){
  
  ggps[[og]] <- vector("list", length(oro_groups[[og]]))
  
  for(o in 1:length(oro_groups[[og]])){
    # get data
    tmpDat <- allComponentDat_model %>%
      filter(oro_type == oro_groups[[og]][o], !is.na(year), component %in% selectedComponents) %>%
      mutate(
        y = ifelse(component %in% 
                     c("deployment"), y, log(y))
      )
      
    
    actionMetric <- tmpDat$variable_name[tmpDat$component == "deployment"][1]
    facetLabels <- c("Action" = paste0("Action\n(", actionMetric, ")"),
                     "Publications" = "Publications\nlog(N articles)",
                     "Policy" = "Policy\nlog(N documents)",
                     "Legislation" = "Legislation\nlog(N documents)",
                     "Interest" = "Interest\nlog(N posts)",
                     "Support" = "Support\nlog(N positive posts + likes)"
                     # "Opposition" = "Opposition\nlog(N negative posts + likes)"
                     )
    
    tmpDat <- tmpDat %>%
      mutate(
        component_color = factor(component, levels = componentAES$level, labels = componentAES$colour),
        component = factor(component, levels = componentAES$level, labels = componentAES$label)
      ) %>%
      mutate(
        component = droplevels(component),
        component_color = droplevels(component_color)
      )%>%
      mutate(
        component = plyr::revalue(component, facetLabels)
      )
    
    # Get cutpoints
    changePointsQuantiles <- cutpoint_results %>%
      filter(oro_type == oro_groups[[og]][o], component %in% selectedComponents) %>%
      pivot_wider(names_from = quantile, values_from = year)%>%
      mutate(iqr = `97.5%`-`2.5%`)%>%
      filter(iqr < 5) %>%
      select(-iqr)%>%
      pivot_longer(cols = `2.5%`:`97.5%`, names_to = "quantile", values_to = "year") %>%
      mutate(
        component = factor(component, levels = componentAES$level, labels = componentAES$label)
      ) %>%
      mutate(
        component = plyr::revalue(component, facetLabels)
      )
    
    
    ggp_tmp <- ggplot()+
      
      # geom_vline(data = changePointsQuantiles %>%filter(quantile %in% c("2.5%","97.5%")),
      #          aes(xintercept=.data$year),
      #          linewidth = 0.5, linetype = "dashed")+
      geom_rect(data = changePointsQuantiles %>%filter(quantile %in% c("2.5%","97.5%")) %>% pivot_wider(names_from = "quantile", values_from = year),
               aes(xmin=.data$`2.5%`, xmax=.data$`97.5%`),
               ymin=-Inf, ymax=Inf,
               fill="darkgrey", alpha=0.5)+
      geom_line(data = tmpDat, aes(x=year, y=y, color=component_color))+
      scale_x_continuous(limits = year_lim)+
      scale_color_identity(guide="none")+
      facet_wrap(vars(component),ncol=1, scales = "free_y")+
      labs(
        y="",
        x="Year",
        title = paste(oro_groups[[og]][o])
      )+
      theme_minimal(base_size = 10)+
      theme(
        plot.margin = unit(c(0.1, 0.1,0.1,0.1), units = "cm")
      )
    ggp_tmp
    
    ggps[[og]][[o]] <- ggp_tmp
    
      
  }
  
  combined_patchwork <- wrap_plots(ggps[[og]], ncol = length(ggps[[og]])) +
    plot_annotation(tag_levels = 'a')
  
  if(savePlot){
    ggsave(here::here(paste0("figures/supplemental/allComponents_",names(oro_groups)[og],".pdf")),
         combined_patchwork, width = 3*length(ggps[[og]]), height = 7)
  }


  
}


## Plot with international policy moments as well
intPolMoments <- readxl::read_excel(
  here::here("data/raw-data/international_policy_moments.xlsx")
)%>%
  mutate(
    year_min = as.numeric(year_min),
    year_max = as.numeric(year_max),
    component = "International policy"
  )

tmpYlims <- c(min(intPolMoments$year_min), max(intPolMoments$year_max)+2)

ggp_intPolMom <- ggplot()+
  geom_rect(data = intPolMoments,
           aes(xmin=.data$year_min, xmax=.data$year_max),
           ymin = -Inf, ymax=Inf,
           fill="lightpink")+
  geom_vline(data = intPolMoments,
           aes(xintercept=.data$year_min),
           linewidth = 0.5, linetype = "solid", col="red")+
  geom_text(data = intPolMoments %>%
              mutate(
                y= rep(seq(0.1,0.7, length.out=3),ceiling(nrow(intPolMoments)/3))[1:nrow(intPolMoments)]
              ),
           aes(x=.data$year_min, y=.data$y, label= stringr::str_wrap(.data$label, width=10)),
           angle=30, hjust=0, col="black", size=2, nudge_x = 0.2)+
  scale_x_continuous(limits = tmpYlims)+
  scale_y_continuous(limits = c(0,1))+
  labs(
    y="",
    x="Year"
  )+
  theme_minimal(base_size = 10)+
  coord_cartesian(clip="off")+
  theme(
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    plot.margin = unit(c(0.1, 0.1,0.1,0.1), units = "cm")
  )

ggp_intPolMom



plotList <- lapply(ggps[[which(names(oro_groups)=="narrative")]],
                   function(x){
                     x+geom_vline(data = intPolMoments%>% select(year_min),
                         aes(xintercept=.data$year_min),
                         linewidth = 0.5, linetype = "solid", col="red")
                   })
plotList <- c(
  list(ggp_intPolMom),
  plotList
)

plotLayout <- "AAA
              BCD"

combined_patchwork <- wrap_plots(plotList, design = plotLayout, heights=c(0.2,1)) +
    plot_annotation(tag_levels = 'a')

ggsave(
  here::here("figures/main/allComponents_narrative_intPolicy.pdf"),
  width=8, height=8.5,
  plot=combined_patchwork
)
```



## Supporting plots for deeper investigation



### Increase in MRE-Ocean publications in from 2002 to 2003

This inflection was driven by a dramatic increase in publications by authors with Brazilian affiliations. Brazil experienced an energy crisis in 2001 due to droughts affecting hydroelectric grid – prompting diversification of renewable sources

```{r calculate all author affiliations for all relevant references - load in next chunk, eval=FALSE}

## connect to database
p1_db <- RSQLite::dbConnect(RSQLite::SQLite(),
                      here::here("data","sqlite-databases","product1.sqlite"),
                                 create=FALSE)

uniquerefs <- tbl(p1_db, "uniquerefs_update2025") %>%
  select(analysis_id, year, affiliation)

idCountry <- tbl(p1_db, "pred_oro_type_long") %>%
  select(analysis_id) %>%
  distinct(analysis_id) %>%
  left_join(uniquerefs %>% select(analysis_id, year, affiliation), by = "analysis_id") %>%
  collect()



# Get a list of country names and codes
load(here::here("data/derived-data/countries_ls.RData"))
# countries_ls <- data.frame(name_en = countrycode::codelist$country.name.en) %>%
#   mutate(country_iso = countrycode::countrycode(sourcevar   = name_en,
#                                        origin      = "country.name",
#                                        destination = "iso3c"),
#          country=name_en)

source(here::here("R", "extract_all_affiliation.R"))

idCountry$countries <- extract_all_affiliation(idCountry$affiliation, countries_ls)

dbWriteTable(conn=p1_db, 
             name= "oro_id_allCountryAffil_lookup",
             value=idCountry,
             append=FALSE, 
             overwrite = FALSE)


dbDisconnect(p1_db)


```


```{r plot publication timeseries -- by affiliation, eval=FALSE}

## Load data
p1_db <- RSQLite::dbConnect(RSQLite::SQLite(),
                      here::here("data","sqlite-databases","product1.sqlite"),
                                 create=FALSE)

# The author affiliations of each publication
idCountry <- tbl(p1_db, "oro_id_allCountryAffil_lookup") %>%
  select(analysis_id, countries, affiliation)%>%
  collect()

# publication year metadata
uniquerefs <- tbl(p1_db, "uniquerefs_update2025") %>%
  select(analysis_id, year) 

# The article ids and their ORO type prediction
predOroType <- tbl(p1_db, "pred_oro_type_mit_long") %>%
  left_join(uniquerefs, by = "analysis_id") %>%
  collect()%>%
  left_join(idCountry, by="analysis_id")


dbDisconnect(p1_db)



## Calculate per oro type, the frequency of country affiliation per year
# columns, analysis_id, oro_type, year, country, count
uniqueCountries <- strsplit(idCountry$countries, " ; ") %>% unlist() %>% unique()

affTypeTop10 <- strsplit(idCountry$countries, ", ") %>% unlist() %>% table %>% sort(decreasing = TRUE) %>% head(7) %>% names()


oro_year_count_long <- data.frame()

for(country in uniqueCountries){
  dfTemp <- predOroType %>%
    filter(grepl(country, countries)) %>%
    group_by(level, year) %>%
    summarise(
      count = n_distinct(analysis_id)
    ) %>%
    mutate(
      country_unique = country
    )
  oro_year_count_long <- oro_year_count_long %>%
    bind_rows(dfTemp)
}

oro_year_count_long <- oro_year_count_long %>%
  mutate(
    country = ifelse(country_unique %in% affTypeTop10, country_unique, "Other")
  )

totals <- oro_year_count_long %>%
  group_by(level, year) %>%
  summarise(
    total = sum(count, na.rm=T)
  )
oro_year_count_long <- oro_year_count_long %>%
  left_join(totals) %>%
  mutate(
    prop = count/total,
    oro_type = factor(
      gsub("[.]","-", level),
      levels = typeAES$level,
      labels = typeAES$label
    ),
    year = as.numeric(year)
  )%>%
  filter(
    !is.na(year)
  )%>%
  filter(
    1980 <= year, year <= 2024
  )

summary(oro_year_count_long)


## Overlay a lineplot of the interannual trend in N publications for each ORO type
oroPub01 <- predOroType %>%
  mutate(
    level = gsub("[.]","-", level),
    year= as.numeric(year)
    ) %>%
  filter(!is.na(year)) %>%
  filter(
    1980 <= year, year <= 2024
  )%>%
  mutate(
    lower_prediction = mean_prediction - std_prediction,
    upper_prediction = mean_prediction + std_prediction
  ) %>%
  group_by(level, year) %>%
  summarise(
    y_mean = n_distinct(analysis_id[0.5 <= mean_prediction]),
    y_lower = n_distinct(analysis_id[0.5 <= lower_prediction]),
    y_upper = n_distinct(analysis_id[0.5 <= upper_prediction])
  ) %>%
  group_by(level) %>%
  mutate(
    log_y_mean_01 = scales::rescale(log(y_mean), to = c(0,1), na.rm=T),
  ) %>%
  ungroup()%>%
  mutate(
    oro_type = factor(
      level,
      levels = typeAES$level,
      labels = typeAES$label
    )
  )



## Plot 1
# A stacked bar plot with year on x axis, proportion of publications by author country on the y
# overlain with the interannual trend in N publications 
# faceted by ORO type

# This shows that the inflection in N publications at ~2000 is due to MRE-Ocean

countryProp_ggp <- ggplot(
  data = oro_year_count_long,
  aes(x=year)
)+
  geom_col(position = "stack", aes(y=prop, fill = country))+
  geom_line(data = oroPub01,
            aes(y=log_y_mean_01), col="#00f3ff")+
  facet_wrap(vars(oro_type), ncol = 2, scales="free_x")+
  scale_x_continuous(limits = c(1980,2024))+
  scale_fill_brewer(type = "qual")+
  guides(fill = guide_legend(nrow=3))+
  labs(
    x="Year",
    y = "Proportion of total publications",
    fill = "Country"
  )+
  theme_minimal()+
  theme(
    legend.position = "bottom"
  )

countryProp_ggp


ggsave(
  here::here("figures/supplemental/propCountAffilByOroYear.pdf"),
  plot = countryProp_ggp,
  width = 6, height=7
)



## Plot 2: 
## zoom in on MRE-Ocean
mreO_countryProp_ggp <- ggplot(
  data = oro_year_count_long %>%
    filter(oro_type == "MRE-Ocean"),
  aes(x=year)
)+
  geom_col(position = "stack", aes(y=prop, fill = country))+
  geom_line(data = oroPub01%>%
    filter(oro_type == "MRE-Ocean"),
            aes(y=log_y_mean_01), col="#00f3ff")+
  scale_x_continuous(limits = c(1980,2024))+
  guides(fill = guide_legend(nrow=3))+
  scale_fill_brewer(type = "qual")+
  labs(
    x="Year",
    y = "Proportion of total publications",
    fill = "Country"
  )+
  theme_minimal(base_size = 10)+
  theme(
    legend.position = "bottom"
  )

mreO_countryProp_ggp


## Which countries in the 'Other' category are causing this increase?

oro_year_count_long %>%
    filter(oro_type == "MRE-Ocean", year %in% c(2002,2003)) %>% 
  pivot_wider(names_from = year, values_from = count, id_cols = c(country_unique))%>% 
  replace_na(list(`2002`=0, `2003`=0))%>%
  mutate(difference = (`2003`-`2002`)) %>%
  arrange(desc(difference)) %>%
  filter(!is.infinite(difference))%>%
  select(country_unique, `2002`, `2003`, difference) %>%
  filter(!(country_unique %in% affTypeTop10))%>%
  head()

#   country_unique `2002` `2003` difference
#   <chr>           <int>  <int>      <int>
# 1 Brazil              0     82         82
# 2 France              1      7          6
# 3 Canada              3      9          6
# 4 Italy               0      5          5
# 5 Norway              3      8          5
# 6 Spain               2      5          3

# looks like brazil went from 0 to 82 publications...

## Plot 3: Time series of Brazil publications
brazilPub_ggp <- oro_year_count_long %>%
    filter(oro_type == "MRE-Ocean", country_unique == "Brazil")%>%
  ggplot(aes(year, log(count)))+
  geom_line()+
  scale_y_continuous(
    name = "log(N MRE-Ocean publications)",
    sec.axis = sec_axis( transform=~exp(.), name="N publications")
  ) +
  labs(
    x = "Year"
    # y = "Brazil affiliated publications (N)"
  )+
  theme_minimal(base_size = 10)
brazilPub_ggp


ggarrange(
  plotlist = list(mreO_countryProp_ggp, brazilPub_ggp),
  labels = paste0(letters[1:2], ")"), 
  label.x = 0, vjust =1.2, font.label = list(size = 10),
  
  align = "h"
)  %>%
  ggpubr::ggexport(filename = here::here("figures/supplemental/MRE_brazil_drivers.pdf"),
                   nrow = 1, 
           width = 7, height= 4)



```



```{r, out.width="0.9\\linewidth", include=TRUE, fig.align="center", fig.cap=c("Brazil publications on MRE-Ocean contribute to inflection point in N publications"), echo=FALSE}

knitr::include_graphics(here::here("figures/supplemental/MRE_brazil_drivers.pdf"))
```


### Drivers of deployment break points



For CCS, Efficiency, CDR-OAE and MRE-Ocean, looks like change points in deployment time series -- what could have caused these increases? 

To explore this, fit a Bayesian multiple change point model (adapted from Cahill et al. 2015, doi: 10.1088/1748-9326/10/8/084002). This model estimates the number of probable change points, and then for each change point the posterior distribution. 

Use the following response variable distributions depending on response variable: 
CDR-OAE - N field trials/start ups - count data - poisson 
MRE-Ocean - Installed capacity (MW) - non-negative continuous - Gamma
Efficiency - Shipping carbon efficiency - non-negative continuous - Gamma
CCS - Storage capacity (Mt) - non-negative continuous - Gamma



Look for external drivers of deployment change points
```{r drivers of deployment change points, eval=FALSE}
# Load cut point results
load(here::here("outputs/cutpointResults_mitigation_alloros.RData"))

oroTypes <- c("CCS","Efficiency","MRE-Ocean","CDR-OAE") 


# Get a summary table of the quantiles of each cut point
deplChangePoints <- cutpoint_results %>%
    filter(component == "deployment", oro_type %in% oroTypes)%>%
  pivot_wider(names_from = "quantile", values_from = "year")


# Write table to csv
write.csv(deplChangePoints, file = here::here("outputs/cutpointSummaryTable_mitigation_deployment.csv"))



# investigate raw data for these years --------------

## MRE-Ocean -----------------
MREOcean_changeYears <- readxl::read_excel(here::here("data/raw-data/external/IRENA-electricity-statistics-by-country-year-2024.xlsx"),
                         sheet = "Country") %>% 
  mutate(year = as.numeric(Year), oro_type = "MRE-Ocean")%>%
  filter(
    Technology %in% c("Marine energy"),
    !is.na(`Electricity Installed Capacity (MW)`),
    deplChangePoints$`25%`[deplChangePoints$oro_type == "MRE-Ocean"]-5 <= year &
      year <= deplChangePoints$`75%`[deplChangePoints$oro_type == "MRE-Ocean"]+5
  ) 


topDiff <- MREOcean_changeYears %>%
  arrange(Country, year) %>%
  group_by(Region) %>%
  mutate(difference = `Electricity Installed Capacity (MW)` - first(`Electricity Installed Capacity (MW)`)) %>% 
  filter(0 < difference)%>%
  select(
    Region, Country, year, `Electricity Installed Capacity (MW)`, difference
  ) %>%
  arrange(desc(difference))
topDiff %>% head()

mreOceanTime_ggp<- ggplot(MREOcean_changeYears %>%
         group_by(Region, year) %>%
         summarise(y = sum(`Electricity Installed Capacity (MW)`, na.rm=T)),
       aes(as.integer(year), y, col = Region))+
  # geom_rect(
  #   xmin = deplChangePoints$`25%`[deplChangePoints$oro_type == "MRE-Ocean"],
  #   xmax = deplChangePoints$`75%`[deplChangePoints$oro_type == "MRE-Ocean"],
  #   ymin = -Inf, ymax = Inf,
  #   fill = "lightgrey", alpha = 0.5, col="transparent"
  # )+
  # geom_vline(xintercept = deplChangePoints$`50%`[deplChangePoints$oro_type == "MRE-Ocean"], col="red")+
  geom_line()+
  scale_x_continuous(breaks = scales::pretty_breaks())+
  geom_text(data = data.frame(
    year = topDiff$year[1]-3,
    y= 150,
    label = stringr::str_wrap("Sihwa Lake Tidal Power Station (255 MW), Republic of Korea", width = 20)
  ), aes(year,y,label = label),size=3, inherit.aes = FALSE, hjust=0.5)+
  labs(
    x="Year",
    y = "Electricity Installed Capacity (MW)",
    col = "Region"
  )+
  theme_minimal()+
  theme(
    legend.position = "right"
  )
mreOceanTime_ggp

# The jump in Asia was due to Republic of Korea -- Sihwa Lake Tidal Power Station 255 MW


## CCS -------------------------------
ccs_changeYears <- readxl::read_excel(here::here("data/raw-data/external/CRSC_CYCLE_4_FINAL_2024_160724.xlsx")) %>%
  filter(grepl("shore|sea", area, ignore.case=T), !is.na(year_of_publication)) %>%
  filter(project_spec == "YES") %>%
  mutate(
    year = as.numeric(year_of_publication),
    `Storage capacity (Mt)` = rowSums(select(., sum_low, sum_mid, sum_high), na.rm = TRUE)
    ) %>%
  filter(
    deplChangePoints$`25%`[deplChangePoints$oro_type == "CCS"]-5 <= year &
      year <= deplChangePoints$`75%`[deplChangePoints$oro_type == "CCS"]+5
  )

summary(ccs_changeYears)



ccsStorTime_ggp <- ggplot(data=ccs_changeYears %>%
         group_by(country, year) %>%
         summarise(y = sum(`Storage capacity (Mt)`, na.rm=T)))+
  # geom_vline(xintercept = deplChangePoints$`50%`[deplChangePoints$oro_type == "CCS"], col="red")+
  # geom_rect(
  #   xmin = deplChangePoints$`25%`[deplChangePoints$oro_type == "CCS"],
  #   xmax = deplChangePoints$`75%`[deplChangePoints$oro_type == "CCS"],
  #   ymin = -Inf, ymax = Inf,
  #   fill = "lightgrey", alpha = 0.5, col="grey"
  # )+
  geom_line(aes(year, y, col = country))+
  # geom_line(data=ccs_changeYears %>%
  #        group_by(region, year) %>%
  #        summarise(y = sum(`Storage capacity (Mt)`, na.rm=T)),
  #      aes(year, y, col = region))+
  geom_point(aes(year, y, col = country), size=3)+
  scale_color_brewer(type="qual")+
  guides(color=guide_legend(nrow=2))+
  labs(
    x="Year",
    y = "Storage capacity (Mt)",
    col = "Country"
  )+
  theme_minimal()+
  theme(
    legend.position = "bottom"
  )
ccsStorTime_ggp

norwayCCS <- ccs_changeYears %>%
  ungroup()%>%
  filter(country == "Norway", year == 2014) %>%
  as.data.frame()

library(ggOceanMaps)
norCCS2014 <- qmap(norwayCCS, size=`Storage capacity (Mt)`,bathymetry=TRUE)





## Efficiency ----------------------------

eff_changeYears <- readxl::read_excel(
  here::here("data/raw-data/external/IEA-shipping-energy-efficiency.xlsx"), 
  sheet = "Data") %>%
  mutate(
    y= 1/carbon_intensity_gCO2_per_tkm,
    year = as.numeric(format(as.Date(paste0(year,"-01-01")), "%Y")),
    variable_name = "Shipping carbon efficiency (tkm/gCO2)",
    component = "deployment",
    oro_type = "Efficiency"
    ) %>%
  filter(
    deplChangePoints$`25%`[deplChangePoints$oro_type == "Efficiency"]-5 <= year &
      year <= deplChangePoints$`75%`[deplChangePoints$oro_type == "Efficiency"]+5
  )

IMOLab <- data.frame(
    year = 2011,
    y= mean(eff_changeYears$y, na.rm=T),
    label = stringr::str_wrap("IMO Resolution MEPC.203(62) (2011)", width = 15))

effTime_ggp<- ggplot(eff_changeYears,
       aes(year, y))+
  geom_line()+
  geom_text(data = IMOLab, aes(year,y,label = label), size=3, inherit.aes = FALSE, hjust=1, nudge_x=-0.5)+
  geom_vline(xintercept = 2011, col="red")+
  labs(
    x="Year",
    y = "Shipping carbon efficiency (tkm/gCO2)"
  )+
  theme_minimal()+
  theme(
    legend.position = "none"
  )
effTime_ggp


## CDR-OAE ---------------------------------------
# Ocean visions field trials 
fieldTrials <- readxl::read_excel(
  here::here("data/raw-data/external/Ocean-visions-mCDR-field-trial-database.xlsx"),
  sheet = "Data") %>%
  mutate(
    `Start of Pilot` = replace(`Start of Pilot`, `Start of Pilot` %in% c("5.12.2023","2023"), as.numeric(as.Date("2023-01-01"))),
    trialID = row_number()
  ) %>%
  mutate(year = as.Date(as.numeric(`Start of Pilot`), origin = "1899-12-30")) %>%
  mutate(year = ifelse(year == as.Date("1952-12-30"), as.Date("2023-01-01"), year)) %>%
  mutate(year = as.Date(year, origin = as.Date("1970-01-01"))) %>%
  separate_rows(`All CDR Methods`, sep=",") %>% 
  mutate(
    oro_type = case_when(
      grepl("OAE|Alkalinity|Weathering", `All CDR Methods`) ~ "CDR-OAE",
      grepl("Upwelling", `All CDR Methods`) ~ "CDR-BioPump",
      TRUE ~ "Other"
    ),
    year = as.numeric(format(year, "%Y")),
    source = "Field trial"
  ) %>%
  filter(!is.na(year) & oro_type != "Other") 


# Founding year of mcdr startups
# Need to de-duplicate and add year  

GESAMP_companies <- readxl::read_excel(
  here::here("data/raw-data/external/GESAMP_wg41_ocean_climate_intervention_projects_31_may_2024.xlsx"), sheet="AllTables") %>%
  select(Company, Type, Website) %>%
  mutate(
    Company = trimws(gsub("[[:punct:]]", "", Company)),
    source = "GESAMP WG 41"
  )%>%
  filter(!is.na(Company))

# Load in OceanNET companies and de-deuplicate from GESAMP companies
OceanNET_companies <- read.csv(
  here::here("data/raw-data/external/OceanNETs_D18_oceanbased_CDR_companies/dataset/D1_8_database_oceanbased_companies.csv")) %>%
  mutate(
    Company = trimws(gsub("[[:punct:]]", "", Company)),
    source = "OceanNETs (2020) D1.8 database"
  )%>%
  filter(!is.na(Company), !is.na(Lat))%>%
  select(-c(contains("X"))) %>%
  filter(!(tolower(Company) %in% tolower(GESAMP_companies$Company)), Company != "Qilibrium")

## Join and write to csv file so I can look up the years on linkedIN
# # Join
# mCDRCompanies <- GESAMP_companies %>%
#   bind_rows(OceanNET_companies)
# 
# # write to file
# write.csv(mCDRCompanies, file = here::here("data/derived-data/mCDR-companies-dedup.csv"))
# 


## read in the years
mCDRCompanies <- readxl::read_excel(here::here("data/raw-data/mCDR-companies-linkedin.xlsx")) %>%
  filter(!is.na(`Company founded on (linkedin)`)) %>%
  rename(year = `Company founded on (linkedin)`) %>%
  select(Company, year) %>%
  left_join(GESAMP_companies %>% select(Company, Type), by="Company") %>%
  left_join(OceanNET_companies %>% select(Company, Type), by="Company") %>%
  mutate(
    Type = ifelse(is.na(Type.x), Type.y, Type.x)
  ) %>%
  select(Company, Type, year) %>%
  mutate(
    oro_type = case_when(
      grepl("Upwell|fertilization", Type, ignore.case=TRUE) ~ "CDR-BioPump",
      # grepl("Biomass sinking|Farming|Harvesting|Aquaculture", Type, ignore.case=TRUE) ~ "CDR-Cult",
      grepl("OAE|Alkalinity|weathering", Type) ~ "CDR-OAE",
      TRUE~"CDR-Other"
    ),
    source = "Startup",
    year = as.numeric(year)
  ) %>%
  filter(oro_type != "CDR-Other")
  
# Ocean visions community and www.cdr.fyi/leaderboards can't be scraped because they use private APIs
# Although the latter a list of suppliers can be found here without metadata
# https://www.cdr.fyi/api/search




## Combine data sources



oae_changeYears <- fieldTrials %>%
  bind_rows(mCDRCompanies) %>%
  filter(
    oro_type == "CDR-OAE",
    2020 <= year &
      year <= 2022
  )

# A combinationo f startups and field trials  
oae_changeYears %>%
  group_by(
    year, source
  )%>%
  summarise(
    n = n()
  )

# Whos conducting the field trials in 2022?
unique(oae_changeYears$`Leading Organization`[oae_changeYears$year == 2022]) # "Vesta"   "GEOMAR"  "Limenet"
fieldtrials_2022 <- unique(oae_changeYears$`Leading Organization`[oae_changeYears$year == 2022])
fieldtrials_2022 <- fieldtrials_2022[!is.na(fieldtrials_2022)]
fieldtrials_2022 <- c("Field trials (2022):", fieldtrials_2022)
fieldtrials_2022 <- paste(fieldtrials_2022, collapse = "\n")

# Who are the startups in 2021?
unique(oae_changeYears$Company[oae_changeYears$year == 2021])
startups_2021 <- unique(oae_changeYears$Company[oae_changeYears$year == 2021])
startups_2021 <- startups_2021[!is.na(startups_2021)]
startups_2021 <- c("Start ups (2021):", startups_2021)
startups_2021 <- paste(startups_2021, collapse = "\n")


# [1] NA                               "Ebb Carbon"                     "Cequest"                       
# [4] "The Charles Darwin Rescue Plan" "Skyology"     


oaeTime_ggp<- ggplot(fieldTrials %>%
                      bind_rows(mCDRCompanies) %>%
                      filter(
                        oro_type == "CDR-OAE",
                        2018 <= year &
                          year <= 2024
                      ) %>%
                      group_by(
                        year, source
                      )%>%
                      summarise(
                        n = n()
                      ),
       aes(year, n, fill = source))+
  geom_col()+
  scale_fill_brewer(
    name = "Data\nsource",
    breaks = c("Field trial","Startup"),
    palette = "Dark2"
  )+ 
  geom_text(
    data = data.frame(
      year = 2022, n=10, label = startups_2021
    ),
    aes(year, n, label = label), size=2.7, inherit.aes = FALSE, hjust=1, nudge_x=-0.5, check_overlap = TRUE)+
  geom_text(x=2021.5, y=13, label = fieldtrials_2022, size=2.7, inherit.aes = FALSE, hjust=0, nudge_x=-0.5,
            check_overlap=TRUE)+
  scale_x_continuous(breaks = 2018:2024)+
  geom_vline(xintercept = 2020.5, col="red")+
  labs(
    x="Year",
    y = "N Field trials/Start ups"
  )+
  theme_minimal()+
  coord_cartesian(clip="off")+
  theme(
    legend.position = "right"
  )
oaeTime_ggp



## Combine plots
plotList = list(
    brazilPub_ggp,
    mreOceanTime_ggp+theme(plot.margin = unit(c(1.2,0,0,0.5),"cm")), 
                  ccsStorTime_ggp+theme(plot.margin = unit(c(1.2,0,0,0.5),"cm")), 
                  norCCS2014+theme(plot.margin = unit(c(1.2,0,0,0.5),"cm")), 
                  effTime_ggp+theme(plot.margin = unit(c(1.2,0,0,0.5),"cm")),
                  oaeTime_ggp+theme(plot.margin = unit(c(1.2,0,0,0.5),"cm"))
                  )


p_all <- wrap_plots(plotList, ncol = 2, widths = c(1,1)) + 
  theme(plot.tag.position = c(0, 1),  # tags top-left
        plot.tag = element_text(size = 10))+ 
  plot_annotation(
  tag_levels = "a",
  tag_prefix = "",
  tag_suffix = ")"
)

# Save
ggsave(
  here::here("figures/main/allChangePointPlots.pdf"),
  p_all,
  width = 10, height = 12
)

```

```{r, out.width="0.9\\linewidth", include=TRUE, fig.align="center", fig.cap=c("Qualitative analysis of change points"), echo=FALSE}

knitr::include_graphics(here::here("figures/main/allChangePointPlots.pdf"))
```



# Quantitative analysis


## Match/mis-match (non-temporal)

*Key message* 
Which OROs recieve more/less attention in policy, legislation and social media, given their number of publications?


NB: Because compares OROs between each other, cannot include deployment because all the metrics are on different scales

*Kieran questions*

* Did I select the correct probability distributions for the response variables?
* Do you notice any errors in the analysis/interpretation?



```{r keep in back pocket - fit poisson glm between total N publication and all other components, eval = FALSE}
require(glmmTMB)


## Loop through the different components and fit glm
# sum metric values across years

# different components to loop through
components <- unique(allComponentDat_model$component) 
components <- components[!(components %in% c("publications","deployment","public opposition"))]

ggps_pubs <- vector("list", length(components))
names(ggps_pubs) <- components
modelResultsPubs <- vector("list", length(components))
for(c in 1:length(components)){
  
  ## Fit the model
  tmpDat <- allComponentDat_model %>%
    filter(component %in% c("publications", components[c])) %>%
    group_by(oro_type, component) %>%
    summarise(
      y = sum(y, na.rm=T)
    )%>%
    pivot_wider(names_from = component, values_from = y, id_cols = c(oro_type)) %>%
    na.omit() 
  colnames(tmpDat)[colnames(tmpDat) == components[c]] <- "y"
  tmpDat$y <- round(tmpDat$y)
  
  
  colnames(tmpDat) <- gsub(" ","_", colnames(tmpDat))
  # tmpDat$publications <- scale(tmpDat$publications)[,1] ## ?scale?
  
  
  mod <- glm(y ~ scale(publications), 
             data = tmpDat,
             family = poisson)
  
  # check for overdispersion, and if present, fit negative binomial
  od <- performance::check_overdispersion(mod)
  if(od$dispersion_ratio > 1 & od$p_value < 0.05){
    mod <- glmmTMB(y ~ scale(publications), 
             data = tmpDat,
             family=nbinom1)
    
  }
  
  # get a variable on the response scale for plotting
  tmpDat<- tmpDat %>%
    mutate(
      response_y = y
    )
  
  

  ## Get model predictions
  predDat <- data.frame(
    publications = seq(from = min(tmpDat$publications), to = max(tmpDat$publications), length.out = 50)
  )
  preds <- predict(mod, newdata = predDat, se.fit = TRUE, type = "response")
  predDat$mean_fit <- preds$fit
  predDat$lower_fit <- preds$fit - preds$se.fit
  predDat$upper_fit <- preds$fit + preds$se.fit
  

    
  ## store model results
  componentLabel <- componentAES$label[match(components[c], componentAES$level)]
  
  
  if(class(mod) == "glmmTMB"){
    mod_sum <- summary(mod)
    mod_sum <- as.data.frame(mod_sum$coefficients$cond) %>%
      rownames_to_column("Term") %>%
      mutate(Component = 
               paste0(
                 componentLabel
               ),
      Distribution = paste0(
        "Negative binomial (dispersion = ",
        scales::number(mod_sum$sigma, accuracy = 1, big.mark = ","),")"
      )) %>%
      relocate(Component) %>%
      mutate(
        Term = case_when(
          Term == "scale(publications)" ~ "Publications",
          TRUE ~ Term
        )
      )
  }else{
    mod_sum <- summary(mod)
    mod_sum <- as.data.frame(mod_sum$coefficients) %>%
      rownames_to_column("Term") %>%
      mutate(Component = 
               paste0(
                 componentLabel
               ),
      Distribution = paste0(
        "Poisson"
      )) %>%
      relocate(Component) %>%
      mutate(
        Term = case_when(
          Term == "scale(publications)" ~ "Publications",
          TRUE ~ Term
        )
      )
  }
  modelResultsPubs[[c]] <- mod_sum
  
  
  pval = data.frame(
    label = ufs::formatPvalue(mod_sum[mod_sum$Term=="Publications",grep("Pr", colnames(mod_sum))])
  )
 
  
  ## Plot 
  ggps_pubs[[c]] <- ggplot()+
    # geom_ribbon(data = predDat, aes(x=publications, ymin = lower_fit, ymax = upper_fit), alpha = 0.1)+
    geom_line(data = predDat, aes(x=publications, y=mean_fit), col="black", linetype="dotted")+
    # geom_text(data = tmpDat, aes(x=publications, y = public_support/(public_opposition+public_support), label = oro_type))+
    ggrepel::geom_text_repel(data = tmpDat, aes(x=publications, y = response_y, label = oro_type, col=oro_type), force = 0.3,
                             size=3)+ 
    ggrepel::geom_text_repel(data = pval, aes(label = stringr::str_wrap(label, 8), fontface=3), 
                             x=Inf, y=-Inf, hjust=1, vjust=0, col="black",
                             size=2.5)+
    scale_color_manual(
      breaks = typeAES$level,
      values = typeAES$colour,
      guide = "none"
    )+
    labs(
      x = "Publications (N)",
      y = componentAES$label[componentAES$level == components[c]]
    )+
    theme_minimal()+
    theme(
      axis.title = element_text(size=10),
      plot.margin = unit(c(0.7,0.3,0.3,0.3),"cm")
    )
  
  ggps_pubs[[c]]

  
}

modelResultsPubsTable <- do.call("bind_rows", modelResultsPubs)




## save

write.csv(
  modelResultsPubs,
  here::here("outputs/publications_otherDim_glmResults.csv")
)

ggpubr::ggarrange(
  plotlist = ggps_pubs,
  labels = paste0(letters[1:length(ggps_pubs)],") ", componentAES$label[match(names(ggps_pubs), componentAES$level)]),
  label.x = 0, vjust =1.2, font.label = list(size = 10),
  align = "hv"

)%>%
  ggpubr::ggexport(filename = here::here("figures/main/GlmFitPlots_Ntotal.pdf"),
                   ncol = 2,
           width = 8, height= 7)


```



But this is slightly different -- what is the public interest on oros after publications and year are accounted for?
"Given their share of publications in a given year, do some OROs attract disproportionately high/low shares of public interest?"

This is more to the point of joachim -- how does scientific interest compare to public interest?

But we want to know, does increasing knowledge increase public interest? This doesn't show that relationship. 

But what if publications incrase but overall posts on oros go down, this would be lost because it's all about a share.
Also if there are lags between the time series we won't see this either -- this is more getting into transfer entropy analysis. 

It doens't answer the simple question -- are the OROs that are most evidenced also the most interested/appear in policy/etc?
```{r fit glm proportional share of component ~ proportional share of publication + year + oro, eval = FALSE}
library(ggeffects)

# “For each ORO, how many public-interest posts do you get per publication?”
# EG the probability of getting a post per publication.
# Include year as a predictor to account for the temporal trend.
# Because temporal 


## Loop through the different components and fit glm
# sum metric values across years

# different components to loop through
components <- unique(allComponentDat_model$component) 
components <- components[!(components %in% c("publications","deployment"))]

ggps_prop <- vector("list", length(components))
names(ggps_prop) <- components
modelResultsProp <- vector("list", length(components))
for(c in 1:length(components)){
  
  ## Fit the model
  tmpDat_annualSum <- allComponentDat_model %>%
    filter(component %in% c("publications", components[c])) %>%
    group_by(year, component) %>%
    summarise(
      total = round(sum(y, na.rm = T))
    ) %>%
    pivot_wider(names_from = component, values_from = total)

  tmpDat <- allComponentDat_model %>%
    mutate(
      y= round(y)
    )%>%
    filter(component %in% c("publications", components[c])) %>%
    pivot_wider(names_from = component, values_from = y, id_cols = c("oro_type","year")) %>%
    left_join(tmpDat_annualSum, by = c("year"), suffix = c("","_total")) %>%
    mutate(across(where(is.numeric), ~ ifelse(is.na(.x), 0, .x))) %>%
    mutate(
      prop_pub = publications/publications_total,
      oro_type = factor(
        oro_type,
        levels = typeAES$level
      )
    )
  
  colnames(tmpDat) <- gsub(components[c],"y", colnames(tmpDat))
  tmpDat$prop_y <- tmpDat$y/tmpDat$y_total
  tmpDat$prop_y[is.na(tmpDat$prop_y)] <- 0
  tmpDat$fail <- as.integer(tmpDat$y_total-tmpDat$y)
  tmpDat$y <- as.integer(tmpDat$y)
  tmpDat<- tmpDat[0<tmpDat$y_total,]

  mod <- glm(cbind(y, fail) ~ prop_pub + oro_type + year, 
                 data = tmpDat ,
                 family = binomial)
  
  od <- performance::check_overdispersion(mod)
  if(od$dispersion_ratio > 1 & od$p_value < 0.05){
    
    mod <- glm(cbind(y, fail) ~ prop_pub + oro_type + year, 
                 data = tmpDat,
                 family = quasibinomial)
    
  }
  
  
  plot_model(mod, type = "pred", terms = c("oro_type"))
  plot_model(mod, type = "pred", terms = c("prop_pub"))
  
  
  # The coefficients table for oro_type (exclude intercept and year)
  mod_sum <- summary(mod)
  
  if(mod_sum$family$family == "quasibinomial"){
    coef_table <- as.data.frame(mod_sum$coefficients) %>%
    rownames_to_column("term") %>%
    filter(grepl("prop_pub|oro_type", term)) %>%
    mutate(
      oro_type = gsub("oro_type", "", term),  # clean factor names
      p_value = `Pr(>|t|)`,
      p_label = ufs::formatPvalue(p_value)
    ) %>%
      mutate(
        oro_type = case_when(
          oro_type == "prop_pub" ~ "MRE-Ocean",
          TRUE ~ oro_type
        )
      )%>%
    select(oro_type, p_value, p_label)
    
    distributionLabel <- paste0(
      "Quasibinomial (dispersion = ",
      scales::number(mod_sum$dispersion, accuracy = 1, big.mark = ","),")"
    )
    
  }else{
    coef_table <- as.data.frame(mod_sum$coefficients) %>%
    rownames_to_column("term") %>%
    filter(grepl("prop_pub|oro_type", term)) %>%
    mutate(
      oro_type = gsub("oro_type", "", term),  # clean factor names
      p_value = `Pr(>|z|)`,
      p_label = ufs::formatPvalue(p_value)
    ) %>%
      mutate(
        oro_type = case_when(
          oro_type == "prop_pub" ~ "MRE-Ocean",
          TRUE ~ oro_type
        )
      )%>%
    select(oro_type, p_value, p_label)
    
    distributionLabel <- "Binomial"
  }
  
  ## store model results
  componentLabel <- componentAES$label[match(components[c], componentAES$level)]
  
  modSummary <- as.data.frame(mod_sum$coefficients) %>%
    rownames_to_column("Term") %>%
    mutate(Component = 
             paste0(
               componentLabel
             ),
    Distribution = distributionLabel) %>%
    relocate(Component) %>%
    mutate(
      Term = gsub("prop_pub","MRE-Ocean", gsub("oro_type", "", Term))
    )

  modelResultsProp[[c]] <- modSummary

  

  
  ## Plot just the oro term
  
  pred <- ggpredict(mod, terms = "oro_type") #condition = c(publications = 1)
  pred <- merge(pred, coef_table, by.x = "x", by.y = "oro_type", all.x = TRUE)
  avg_effect <- mean(pred$predicted, na.rm = TRUE)
  
  ggps_prop[[c]] <- ggplot(pred, aes(x = x, y = predicted)) +
    # predicted points
    geom_point(size = 3, color = "blue") +
    # 95% CI bars
    geom_errorbar(aes(ymin = conf.low, ymax = conf.high),
                  width = 0.2, color = "blue") +
    # raw observed ratios
    geom_jitter(data = tmpDat,
                aes(x = oro_type, y = prop_y),
                width = 0.2, height = 0, color = "red", alpha = 0.3) +
    # p-value labels above predicted points
    geom_text(aes(label = p_label),
              y=Inf,
              size = 3, color = "black", vjust=1, angle=45, hjust=1) +
    # Average effect
    geom_hline(yintercept = avg_effect, linetype = "dashed", color = "black") +
    scale_y_continuous(limits = c(NA, max(pred$conf.high, na.rm=T)),
                       expand = expansion(mult=c(0,0.2)))+
    labs(y = "Marginal effect",
         x = "ORO type") +
    theme_minimal(base_size = 10)+
    theme(
      axis.text.x = element_text(angle = 45, hjust=1, vjust=1),
      axis.title = element_text(size=10),
      plot.margin = unit(c(0.7,0.3,0.3,0.3),"cm")
    )
  
  ggps_prop[[c]]

  
}

modelResultsPropTable <- do.call("bind_rows", modelResultsProp)







## save

write.csv(
  modelResultsPropTable,
  here::here("outputs/publications_otherDim_glmResults_proportion.csv")
)



ggpubr::ggarrange(
  plotlist = ggps_prop,
  labels = paste0(letters[1:length(ggps_prop)],") ", componentAES$label[match(names(ggps_prop), componentAES$level)]),
  label.x = 0, vjust =1.2, font.label = list(size = 10),
  align = "hv"

)%>%
  ggpubr::ggexport(filename = here::here("figures/main/GlmFitPlots_proportion.pdf"),
                   ncol = 2,
           width = 8, height= 6.5)



```


```{r winner -- fit glm proportional share of component ~ proportional share of publication + year, eval = FALSE}
library(ggeffects)

# “For each ORO, how many public-interest posts do you get per publication?”
# EG the probability of getting a post per publication.
# Include year as a predictor to account for the temporal trend.
# Because temporal 


## Loop through the different components and fit glm
# sum metric values across years

# different components to loop through
components <- unique(allComponentDat_model$component) 
components <- components[!(components %in% c("publications","deployment"))]

ggps_prop <- vector("list", length(components))
names(ggps_prop) <- components
modelResultsProp <- vector("list", length(components))
for(c in 1:length(components)){
  
  ## Fit the model
  tmpDat_annualSum <- allComponentDat_model %>%
    filter(component %in% c("publications", components[c])) %>%
    group_by(year, component) %>%
    summarise(
      total = round(sum(y, na.rm = T))
    ) %>%
    pivot_wider(names_from = component, values_from = total)

  tmpDat <- allComponentDat_model %>%
    mutate(
      y= round(y)
    )%>%
    filter(component %in% c("publications", components[c])) %>%
    pivot_wider(names_from = component, values_from = y, id_cols = c("oro_type","year")) %>%
    left_join(tmpDat_annualSum, by = c("year"), suffix = c("","_total")) %>%
    mutate(across(where(is.numeric), ~ ifelse(is.na(.x), 0, .x))) %>%
    mutate(
      prop_pub = publications/publications_total,
      oro_type = factor(
        oro_type,
        levels = typeAES$level
      )
    )
  
  colnames(tmpDat) <- gsub(components[c],"y", colnames(tmpDat))
  tmpDat$prop_y <- tmpDat$y/tmpDat$y_total
  tmpDat$prop_y[is.na(tmpDat$prop_y)] <- 0
  tmpDat$fail <- as.integer(tmpDat$y_total-tmpDat$y)
  tmpDat$y <- as.integer(tmpDat$y)
  tmpDat<- tmpDat[0<tmpDat$y_total,]

  mod <- glm(cbind(y, fail) ~ prop_pub + year, 
                 data = tmpDat ,
                 family = binomial)
  
  od <- performance::check_overdispersion(mod)
  if(od$dispersion_ratio > 1 & od$p_value < 0.05){
    
    mod <- glm(cbind(y, fail) ~ prop_pub + year, 
                 data = tmpDat,
                 family = quasibinomial)
    
  }
  
  
  plot_model(mod, type = "pred", terms = c("prop_pub"))
  
  
  # The coefficients table for oro_type (exclude intercept and year)
  mod_sum <- summary(mod)
  
  if(mod_sum$family$family == "quasibinomial"){
    distributionLabel <- paste0(
      "Quasibinomial (dispersion = ",
      scales::number(mod_sum$dispersion, accuracy = 1, big.mark = ","),")"
    )
    
  }else{
    distributionLabel <- "Binomial"
  }
  
  ## store model results
  componentLabel <- componentAES$label[match(components[c], componentAES$level)]
  
  modSummary <- as.data.frame(mod_sum$coefficients) %>%
    rownames_to_column("Term") %>%
    mutate(Component = 
             paste0(
               componentLabel
             ),
    Distribution = distributionLabel) %>%
    relocate(Component) %>%
    mutate(
      Term = gsub("prop_pub","Proportion of publications", gsub("year", "Year", Term))
    )

  modelResultsProp[[c]] <- modSummary

  pval = data.frame(
    estimate = paste(signif(modSummary[grepl("publications",modSummary$Term),grep("Estimate", colnames(modSummary))],
                                    digits = 2)),
    pval = ufs::formatPvalue(modSummary[grepl("publications",modSummary$Term),grep("Pr", colnames(modSummary))])
  ) %>%
    mutate(
      label = paste0(
        "\u03b2 = ", estimate,"\n",pval
      )
    )
  
  
  # add a shape variable
  shapelookup <- tmpDat %>%
    distinct(oro_type)%>%
     mutate(
       oro_branch = case_when(
         grepl("MRE", oro_type)~"MRE",
         grepl("CDR",oro_type)~"CDR",
         TRUE~"Other"
       )
     )%>%
    group_by(oro_branch)%>%
    mutate(
      shape_type = factor(row_number(), levels = 1:3, labels = c(15, 16, 17))
    )
  tmpDatPlot <- tmpDat%>%
    left_join(shapelookup, by = "oro_type")
  
  
  tmpDatSums <- tmpDat %>%
    group_by(oro_type) %>%
    summarise(
      prop_y = mean(prop_y, na.rm = T),
      prop_pub = mean(prop_pub, na.rm=T)
    )
    
 
  
  ## Plot 
  pred <- ggpredict(mod, terms = "prop_pub [all]")
  
  ggps_prop[[c]] <- ggplot(data = pred)+
    geom_line(aes(x, predicted), col="black",linetype = "solid", size=0.7)+
    geom_ribbon(aes(x=x, ymin = conf.low, ymax = conf.high), alpha = 0.1)+
    geom_point(data = tmpDatPlot,
               aes(x=prop_pub, y = prop_y, shape = shape_type, col=oro_type),
                             size=3, alpha = 0.25)+
    ggrepel::geom_text_repel(data = tmpDatSums, aes(x=prop_pub, y = prop_y, label = oro_type), force = 0.3,
                             size=3)+
    ggrepel::geom_text_repel(data = pval, aes(label = label, fontface=3), 
                             x=Inf, y=-Inf, hjust=1, vjust=0, col="black",
                             size=3.5)+
    scale_color_manual(
      breaks = typeAES$level,
      values = typeAES$colour
    )+
    scale_shape_manual(
      breaks = levels(shapelookup$shape_type),
      values = c(15, 16, 17)
    ) +
    guides(
      color = guide_legend(override.aes = list(shape = as.numeric(as.character(shapelookup$shape_type[order(shapelookup$oro_type)])))),
      shape = "none" 
    )+
    scale_x_continuous(expand = expansion(mult = c(0.15,0)))+
    scale_y_continuous(expand = expansion(mult = c(0.15,0)))+
    labs(
      x = "Proportion of publications",
      y = paste("Proportion of",tolower(componentAES$label[componentAES$level == components[c]])),
      color = "ORO type"
    )+
    theme_minimal()+
    theme(
      axis.title = element_text(size=9),
      plot.margin = unit(c(0.7,0.3,0.3,0.3),"cm")
    )
  
  ggps_prop[[c]]
  
  leg <- ggpubr::get_legend(ggps_prop[[c]])
  ggps_prop[[c]] <- ggps_prop[[c]]+theme(legend.position = "none")
  

 
  
}

modelResultsPropTable <- do.call("bind_rows", modelResultsProp)







## save

write.csv(
  modelResultsPropTable,
  here::here("outputs/publications_otherDim_glmResults_proportion.csv")
)






require(patchwork)


ggps_prop_tagged <- mapply(
  function(p, tag) p + 
    labs(tag = tag) + 
    theme(legend.position = "none",
          plot.tag.position = c(0, 1.1),  # top-left like ggpubr
          plot.tag = element_text(size = 10, hjust = 0, vjust = 1.2),
          plot.margin = unit(c(1,0.3,0,0.3),"cm")
          ),
  ggps_prop,
  paste0(letters[1:length(ggps_prop)], ") ", 
         componentAES$label[match(names(ggps_prop), componentAES$level)]),
  SIMPLIFY = FALSE
)

combined <- wrap_plots(ggps_prop_tagged, ncol = 2)


# Combine plots with legend on the right
final_plot <- combined + leg +theme(plot.margin = unit(c(0.1,0,0,0.1),"cm")) + 
  plot_layout(ncol = 3, widths = c(4,4, 2), byrow=FALSE)  # adjust widths as needed


ggsave(
  filename = here::here("figures/main/GlmFitPlots_proportion.pdf"),
  plot = final_plot,
  width = 10,
  height = 6.5
)
```


```{r fit glm proportional share of component ~ proportional share of publication*oro + year, eval = FALSE}
library(ggeffects)

# “For each ORO, how many public-interest posts do you get per publication?”
# EG the probability of getting a post per publication.
# Include year as a predictor to account for the temporal trend.
# Because temporal 


## Loop through the different components and fit glm
# sum metric values across years

# different components to loop through
components <- unique(allComponentDat_model$component) 
components <- components[!(components %in% c("publications","deployment"))]

ggps <- vector("list", length(components))
names(ggps) <- components
modelResults <- vector("list", length(components))
for(c in 1:length(components)){
  
  componentLabel <- componentAES$label[match(components[c], componentAES$level)]
  
  ## Fit the model
  tmpDat_annualSum <- allComponentDat_model %>%
    filter(component %in% c("publications", components[c])) %>%
    group_by(year, component) %>%
    summarise(
      total = round(sum(y, na.rm = T))
    ) %>%
    pivot_wider(names_from = component, values_from = total)

  tmpDat <- allComponentDat_model %>%
    mutate(
      y= round(y)
    )%>%
    filter(component %in% c("publications", components[c])) %>%
    pivot_wider(names_from = component, values_from = y, id_cols = c("oro_type","year")) %>%
    left_join(tmpDat_annualSum, by = c("year"), suffix = c("","_total")) %>%
    mutate(across(where(is.numeric), ~ ifelse(is.na(.x), 0, .x))) %>%
    mutate(
      prop_pub = publications/publications_total,
      oro_type = factor(
        oro_type,
        levels = typeAES$level
      )
    )
  colnames(tmpDat) <- gsub(components[c],"y", colnames(tmpDat))
  tmpDat$prop_y <- tmpDat$y/tmpDat$y_total
  tmpDat$prop_y[is.na(tmpDat$prop_y)] <- 0
  tmpDat$fail <- as.integer(tmpDat$y_total-tmpDat$y)
  tmpDat$y <- as.integer(tmpDat$y)


  mod <- glm(cbind(y, fail) ~ prop_pub*oro_type + year, 
                 data = tmpDat,
                 family = quasibinomial)

  
  plot_model(mod, type = "pred", terms = c("prop_pub [all]","oro_type"))
  # plot_model(mod, type = "pred",terms = "oro_type")
  R2 <- rcompanion::nagelkerke(mod)
  
  # The coefficients table for oro_type (exclude intercept and year)
  mod_sum <- summary(mod)
  coef_table <- as.data.frame(mod_sum$coefficients) %>%
    rownames_to_column("term") %>%
    mutate(term = ifelse(term == "prop_pub","prop_pub:oro_typeMRE-Ocean", term)) %>%
    filter(grepl("prop_pub:oro_type", term)) %>%
    mutate(
      oro_type = gsub("prop_pub:oro_type", "", term),  # clean factor names
      p_value = `Pr(>|t|)`,
      p_label = ufs::formatPvalue(p_value),
      estimate = signif(Estimate, digits = 3),
      signif = ifelse(`Pr(>|t|)`< 0.05, "*", "")
    ) %>%
    select(oro_type, p_value, p_label, estimate, signif)

  
  ## Plot just the oro term
  
  # pred <- ggpredict(mod, terms = c("prop_pub","oro_type"))
  raw_ranges <- tmpDat %>%
    group_by(oro_type) %>%
    summarise(
      minval = min(prop_pub, na.rm=T),
      maxval = max(prop_pub, na.rm=T)
    )
  
  pred <- ggpredict(mod, terms = c("prop_pub [all]","oro_type"))
  pred <- pred %>%
    left_join(coef_table, by = c("group" = "oro_type")) %>%
    left_join(raw_ranges, by = c("group" = "oro_type")) %>%
    ungroup 
  
  pred$extrapolated <- ifelse(pred$minval <= pred$x & pred$x <= pred$maxval, "Data range","Extrapolated")
  pred$signif <- ifelse(pred$p_value< 0.05, "*", "")
  pred$line_type <- ifelse(pred$p_value < 0.05, "solid", "dashed")
  pred$legend_label <- paste0(pred$group, pred$signif, " (\u03b2 =", pred$estimate, ")")
  pred$group <- factor(
    pred$group,
    levels = typeAES$level
  )
  pred$linegroup <- paste(pred$group, pred$extrapolated)
  
  legend_label_levels <- pred %>%
    as.data.frame() %>%
    distinct(group, legend_label) %>%
    left_join(typeAES, by = c("group"="level")) %>%
    arrange(order)
  pred$legend_label <- factor(
    pred$legend_label,
    levels = legend_label_levels$legend_label
  )
  pred$color <- legend_label_levels$colour[match(pred$legend_label, legend_label_levels$legend_label)]
  
  
  # extract the reference year actually used by ggpredict
  year_ref <- attributes(pred)$constant.values$year
  coef_all <- coef(mod)
  intercept <- coef_all["(Intercept)"]
  beta_year <- coef_all["year"]   # if "year" is numeric; if factor-coded, grab the right coef
  beta_pub  <- coef_all["prop_pub"]

  avg_beta <- mean(coef_table$estimate, na.rm = TRUE)

  avg_effect <- data.frame(
    x = seq(min(tmpDat$prop_pub, na.rm = TRUE),
            max(tmpDat$prop_pub, na.rm = TRUE),
            length.out = 100)
  ) %>%
    mutate(
      # intercept already corresponds to reference year (factor)
      logit = intercept + avg_beta * x,
      predicted = plogis(logit),
      legend_label = paste0("Average (β=", signif(avg_beta, 3), ")"),
      color = "black"
    )
  
  if (is.numeric(tmpDat$year)) {
    year_ref <- attributes(pred)$constant.values$year
    avg_effect$logit <- intercept + coef(mod)["year"] * year_ref + avg_beta * avg_effect$x
  } else {
    avg_effect$logit <- intercept + avg_beta * avg_effect$x
  }
  avg_effect$predicted <- plogis(avg_effect$logit)
  
  # plot_data <- bind_rows(
  #   pred %>% as.data.frame() %>% mutate(type = "oro"),
  #   avg_effect %>% mutate(type = "average")
  # )
  plot_data <- pred %>% as.data.frame() %>% mutate(type = "oro")

  # avg_effect <- pred %>%
  #   group_by(x) %>%
  #   summarise(
  #     predicted = mean(predicted, na.rm=T)
  #   )
  
  ggps[[c]] <- ggplot(plot_data, aes(x = x, y = predicted, 
                                   color = legend_label, 
                                   fill = legend_label,
                                   size = signif)) +
    geom_line(
      linetype="dotted"
      )+
  
  geom_line(
    data = filter(plot_data, type == "oro" & extrapolated == "Data range"),
    linetype = "solid"
  ) +
  # geom_line(
  #   data = filter(plot_data, type == "average"),
  #   linetype = "longdash",
  #   size=0.8
  # ) +

  geom_ribbon(
    data = filter(plot_data, type == "oro" & signif == "*" & extrapolated == "Data range"),
    aes(ymin = conf.low, ymax = conf.high, fill = legend_label),
    alpha = 0.2, color = NA
  ) +
    
    scale_x_continuous(limits = range(tmpDat$prop_pub, na.rm=T))+
  scale_size_manual(
    breaks = c("*",""),
    values = c(1,0.5),
    guide = "none"
  )+
  scale_color_manual(
    values = setNames(plot_data$color, plot_data$legend_label)
  ) +
  scale_fill_manual(
    values = setNames(plot_data$color, plot_data$legend_label)
  ) +
  
  labs(y = paste("Proportion of", tolower(componentLabel)),
       x = "Proportion of publications",
       color = "ORO type",
       fill = "ORO type"
       # size = "Significance"
       ) +
  theme_minimal(base_size = 10)

  ggps[[c]]

  
  
  
  ## store model results
  modSummary <- as.data.frame(mod_sum$coefficients) %>%
    rownames_to_column("Term") %>%
    mutate(Component = 
             paste0(
               componentLabel
    #            " (R2=",
    #            scales::number(
    # R2$Pseudo.R.squared.for.model.vs.null["McFadden",1],
    #   accuracy = 0.01
    # ),
    # ")"
             ),
    Distribution = paste0(
      "Quasibinomial (dispersion = ",
      scales::number(mod_sum$dispersion, accuracy = 1, big.mark = ","),")"
    )) %>%
    relocate(Component) 

  modSummary
  modelResults[[c]] <- modSummary
  
  
}

modelResultsTable <- do.call("bind_rows", modelResults) %>%
  mutate(Term = gsub("prop_pub","Proportion of publications", Term)) %>%
  mutate(Term = gsub("year","Year", Term)) %>%
  mutate(Term = gsub(":","*", Term)) %>%
  mutate(Term = gsub("oro_type","ORO type - ", Term))






## save

write.csv(
  modelResults,
  here::here("outputs/publications_otherDim_glmResults_proportionInteraction.csv")
)



ggpubr::ggarrange(
  plotlist = ggps,
  labels = paste0(letters[1:length(ggps)],") ", componentAES$label[match(names(ggps), componentAES$level)]),
  label.x = 0, vjust =1.2, font.label = list(size = 10),
  align = "hv", ncol=1

)%>%
  ggpubr::ggexport(filename = here::here("figures/main/GlmFitPlots_proportionInteraction.pdf"),
                   ncol = 1,nrow = length(ggps),
           width = 6, height= 11)



```



```{r, out.width="0.9\\linewidth", include=TRUE, fig.align="center", fig.cap=c("Match/mis-match glm fits"), echo=FALSE}

knitr::include_graphics(here::here("figures/main/selectedGlmFitPlots.pdf"))
```

