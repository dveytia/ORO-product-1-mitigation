---
title: "2.0_trends-analysis_explore"
author: "Devi Veytia"
date: "2025-07-08"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
```




```{r load libraries, results = 'hide'}

# general data handing
library(dplyr)
library(dbplyr)
library(RSQLite)
library(R.utils)
library(ggplot2)
library(ggalluvial)
library(tidyr)
library(stringr)
library(viridis)
library(countrycode)
library(broom)
library(conflicted)
library(tidyverse)
library(cowplot)
library(ggpubr)
library(patchwork)
library(igraph)
library(ggraph)
library(tidygraph)
library(ggplot2)
require(VLTimeCausality) # remotes::install_github("DarkEyes/VLTimeSeriesCausality")
library(QPress) # remotes::install_github("SWotherspoon/QPress")
library(reshape2)
library(ggeffects)
library(sjPlot)
library(magick)
library(grid)


conflict_prefer("select", "dplyr")
conflicts_prefer(dplyr::filter)



## AESTHETICS
factor_aes <- readxl::read_excel(here::here("R/mitigation_factor_aesthetics2.xlsx"))
typeAES <- factor_aes[which(factor_aes$variable == "oro_type"),]
typeAES <- typeAES[order(typeAES$order),]

componentAES <- factor_aes[which(factor_aes$variable == "component"),]
componentAES <- componentAES[order(componentAES$order),]

## Set seed
addTaskCallback(function(...) {set.seed(123);TRUE})

```


# Load data

Data structure:

each data frame has the following id variables: oro_type, component (publication, policy, deployment, etc), variable_name,
each data frame has the following response variable: y

```{r load data}

# Load data for specific OROs
load(here::here("data", "derived-data", "mitigationORO_pubs.RData")) #pubs
load(here::here("data", "derived-data", "n_nonBindPolicy_docs.RData")) # legDat
load(here::here("data", "derived-data", "n_legislation_docs.RData")) # polDat
load(here::here("data/derived-data/mitigationDeployDat.RData")) # allDeployDat
load(here::here("data/derived-data/mitigationPostsDat.RData")) # postsDat



year_lim <- c(2000, 2024) # years to analyse

# components to analyse
selectedComponents <- c("publications", "deployment","policy","legislation","public interest","public support") 

# bind all data together
allComponentDat_model <- pubs %>%
  bind_rows(polDat) %>%
  bind_rows(legDat) %>%
  bind_rows(allDeployDat) %>%
  bind_rows(postsDat) %>%
  mutate(
    component = replace(component, component == "non-binding policy", "policy"),
    # y = scale(y, center=TRUE, scale=TRUE)
  ) %>%
  filter(year_lim[1] <= year & year <= year_lim[2],
         oro_type %in% typeAES$level) %>%
  filter(
    component %in% selectedComponents
  )


```


# Entry point: The distribution of and extent of scientific evidence, and its temporal variation, varies according to the type of ocean-related option (ORO) 

*Key message*
Mitigation ORO publication effort is weighted towards marine renewable energy (ocean, located). Over time, we observe varying rates of increase, with inflection points – notably in MRE-Ocean occurring in 2001. This inspired this analysis: What are the drivers of changes in publication? And how do these relate to outcomes in action? 


*To contextualize our publication data, we have added the following metrics:*

Number of policy and legislative documents - Calculated by web scraping ECOLEX & FAOLEX databases then keyword searching full document pdfs for keywords relevant for each type of ORO. Document type metadata used to determine legislation vs policy

Interest - N posts returned from keyword searches (reddit, youtube, bluesky, linked in), weighted by number of search queries

Support - N posts (from above) that are positive sentiment (predicted using a pre-trained sentiment classification LLM) weighted by # of likes

Action - various metrics (see below)


*Note that chunks save figures and output files, so run line by line rather than whole chunk to avoid over-writing*

```{r plot publication timeseries for all the OROs, eval = FALSE}

## Get data
p1_db <- RSQLite::dbConnect(RSQLite::SQLite(),
                      here::here("data","sqlite-databases","product1.sqlite"),
                                 create=FALSE)

uniquerefs <- tbl(p1_db, "uniquerefs_update2025") %>%
  select(analysis_id, year) 

predOroType <- tbl(p1_db, "pred_oro_type_mit_long") %>%
  left_join(uniquerefs, by = "analysis_id") %>%
  collect()

dbDisconnect(p1_db)

## total number of articles relevant
length(unique(predOroType$analysis_id)) # 47830
# number of unclassified ORO articles
58952-length(unique(predOroType$analysis_id)) #  11122


## Sumamrise and format to count number of articles/ORO/year
oroPub <- predOroType %>%
  mutate(
    std_prediction = ifelse(is.na(std_prediction), 0, std_prediction),
    level = gsub("[.]","-", level),
    year= as.numeric(year)
    ) %>%
  filter(!is.na(year)) %>%
  mutate(
    lower_prediction = mean_prediction - std_prediction,
    upper_prediction = mean_prediction + std_prediction
  ) %>%
  group_by(level, year) %>%
  summarise(
    y_mean = n_distinct(analysis_id[0.5 <= mean_prediction]),
    y_lower = n_distinct(analysis_id[0.5 <= lower_prediction]),
    y_upper = n_distinct(analysis_id[0.5 <= upper_prediction])
  ) 

# Calculate for all mitigation OROs
allPub <- predOroType %>%
  mutate(
    std_prediction = ifelse(is.na(std_prediction), 0, std_prediction),
    level = gsub("[.]","-", level),
    year= as.numeric(year)
    ) %>%
  filter(!is.na(year)) %>%
  mutate(
    lower_prediction = mean_prediction - std_prediction,
    upper_prediction = mean_prediction + std_prediction
  ) %>%
  group_by(year) %>%
  summarise(
    y_mean = n_distinct(analysis_id[0.5 <= mean_prediction]),
    y_lower = n_distinct(analysis_id[0.5 <= lower_prediction]),
    y_upper = n_distinct(analysis_id[0.5 <= upper_prediction])
  )%>%
  mutate(level = paste("All mitigation OROs"))

# Bind all mitigation OROs with individual ORO counts
allMit <- oroPub %>%
  bind_rows(allPub) %>%
    mutate(
    level = factor(
      level,
      levels = c("All mitigation OROs", typeAES$level),
      labels = c("All mitigation OROs", typeAES$label)
    )
  )



## Plot
oroTimeseries_ggp<- ggplot(allMit%>% filter(1980 <= year, year <= 2024), aes(x=year))+
  geom_rect(xmin = 2001, xmax = 2005, ymin=-Inf, ymax=Inf, alpha = 0.1, fill="grey")+
  geom_line(aes(y=log(y_mean), col=level))+
  geom_ribbon(aes(ymin = log(y_lower), ymax = log(y_upper), fill = level), alpha = 0.5)+
  facet_wrap(vars(level), scales = "free_y")+
  scale_color_manual(
    breaks = c("All mitigation OROs", typeAES$label),
    values = c("#35a7d9", typeAES$colour),
    labels = c("All mitigation OROs", typeAES$label)
  )+
  scale_fill_manual(
    breaks = c("All mitigation OROs", typeAES$label),
    values = c("#35a7d9", typeAES$colour),
    labels = c("All mitigation OROs", typeAES$label)
  )+
  scale_y_continuous()+
  scale_x_continuous(limits = c(1980,2024))+
  # guides(col=guide_legend(nrow=2))+
  labs(x="Year", y = "log(N publications)", col = "ORO type")+
  theme_minimal()+
  theme(
    legend.position = "none",
    axis.text.x = element_text(angle=45, hjust=1)
  )

oroTimeseries_ggp


## Save
ggsave(
  here::here("figures/main/nPublicationsTimeseriesPlots.pdf"),
  plot = oroTimeseries_ggp,
  width = 7, height=5
)

```

```{r combine publication timeseries with conceptual model figure and plot}


# Import slide of conceptual model
slide_img <- image_read_pdf("figures/main/conceptual_model/conceptual_model.pdf", density = 600)
slide_grob <- rasterGrob(as.raster(slide_img), interpolate = FALSE)



## plot all together
plotList <- list(oroTimeseries_ggp, 
                 slide_grob
                 )

combined <- wrap_plots(plotList, nrow=2, heights=c(1,1))+
  plot_annotation(tag_levels = "a", tag_suffix = ")")&theme(plot.tag.position = c(0, 1))


## Save
ggsave(
  here::here("figures/main/nPublicationsTimeseries_conceptualModel.pdf"),
  plot = combined,
  width = 7, height=9
)

```

```{r, out.width="0.9\\linewidth", include=TRUE, fig.align="center", fig.cap=c("Entrypoint - Distribution of N publications (predicted relevant by LLM classifier) and the conceptual model", echo=FALSE}

knitr::include_graphics(here::here("figures/main/nPublicationsTimeseries_conceptualModel.pdf"))
```


Introduce complementary data sets:

```{r introduce complementary data - plot proportions by oro type, eval = FALSE}

## Publications
p1_db <- RSQLite::dbConnect(RSQLite::SQLite(),
                      here::here("data","sqlite-databases","product1.sqlite"),
                                 create=FALSE)

uniquerefs <- tbl(p1_db, "uniquerefs_update2025") %>%
  select(analysis_id, year) 

predOroTypeYear <- tbl(p1_db, "pred_oro_type_mit_long") %>%
  left_join(uniquerefs, by = "analysis_id") %>%
  filter(!is.na(year)) %>%
  group_by(level, year) %>%
  summarise(
    N = n_distinct(analysis_id[0.5 <= (mean_prediction)])
  ) %>%
  collect()%>%
  mutate(
    year = as.numeric(year),
    oro_type = gsub("[.]","-",level),
    component = "publications",
    variable_name = "Publications (N)"
  ) %>%
  select(oro_type, component, variable_name, year, N)

nPubs_oro <- tbl(p1_db, "pred_oro_type_mit_long") %>%
  group_by(level)%>%
  summarise(
    N = n_distinct(analysis_id[0.5 <= (mean_prediction)])
  )%>%
  collect()%>%
  mutate(
    oro_type = gsub("[.]","-",level),
    component = "publications",
    variable_name = "Publications (N)"
  ) %>%
  select(oro_type, component, variable_name, N)

dbDisconnect(p1_db)



## Sumarise by component overall proportional share of each ORO
prop_oro_summary_df <- allComponentDat_model %>%
  filter(!(component %in% c("deployment","publications"))) %>%
  group_by(component, variable_name, oro_type) %>%
  summarise(
    N = sum(y, na.rm=T)
  ) %>%
  bind_rows(
    nPubs_oro
  ) %>%
  group_by(component) %>%
  mutate(
    total_component = sum(N, na.rm=T)
  ) %>%
  ungroup() %>%
  mutate(
    prop_component = N/total_component,
    component = factor(component, 
                       levels = componentAES$level, 
                       labels = componentAES$label_varname)
  )%>%
  mutate(
    component = droplevels(component),
    oro_type = factor(
      oro_type, 
      levels = typeAES$level,
      labels = typeAES$label
    )
  )
  



# Plot proportion of each dimension per ORO
dist_component_ggp <- ggplot(data = prop_oro_summary_df, aes(x=component))+
  geom_col(aes(y=prop_component, fill=oro_type), position="stack")+
  geom_text(aes(label = scales::number(total_component, accuracy = 1, big.mark = ",")), check_overlap = TRUE, vjust=0, y=1)+
  scale_fill_manual(
    breaks = typeAES$label,
    values = typeAES$colour
  )+
  scale_y_continuous(expand = expansion(mult=c(0,0.1)))+
  labs(
    x="Dimension (i.e. Node)",
    y="Proportion",
    fill="ORO type"
  )+
  guides(fill=guide_legend(ncol=2))+
  theme_minimal(base_size = 10)+
  theme(
    axis.text.x = element_text(angle=45, hjust=1)
  )

dist_component_ggp


## Plot the time series as well
year_oro_summary_df <- allComponentDat_model %>%
  filter(!(component %in% c("deployment","publications"))) %>%
  group_by(component, variable_name, oro_type, year) %>%
  summarise(
    N = sum(y, na.rm=T)
  ) %>%
  bind_rows(
    predOroTypeYear
  ) %>%
  filter(
    year_lim[1] <= year, year <= year_lim[2]
  ) %>%
  group_by(component, year) %>%
  mutate(
    total_component = sum(N, na.rm=T)
  ) %>%
  ungroup() %>%
  mutate(
    prop_component = N/total_component,
    component = factor(component, 
                       levels = componentAES$level, 
                       labels = componentAES$label_varname)
  )%>%
  mutate(
    component = droplevels(component),
    oro_type = factor(
      oro_type, 
      levels = typeAES$level,
      labels = typeAES$label
    )
    # component = fct_recode(component, "Publications (log(N))"="Publications (N)")
  )%>%
  select(oro_type, component, year, prop_component, N) 
  


# Annual plot of each dimension per ORO
year_component_ggp <- ggplot(data = year_oro_summary_df, aes(x=year))+
  facet_wrap(vars(component), ncol=1, scales = "free_y")+
  geom_area(aes(y=N, fill=oro_type), position="stack")+
  scale_fill_manual(
    breaks = typeAES$label,
    values = typeAES$colour
  )+
  scale_y_continuous(expand = expansion(mult=c(0,0.1)))+
  labs(
    x="Year",
    y="N",
    fill="ORO type"
  )+
  theme_minimal(base_size = 10)+
  theme(
    legend.position = "bottom"
  )

year_component_ggp

year_component_prop_ggp <- ggplot(data = year_oro_summary_df, aes(x=year))+
  facet_wrap(vars(component), ncol=1, scales = "free_y")+
  geom_col(aes(y=prop_component, fill=oro_type), position="stack", width=1)+
  scale_fill_manual(
    breaks = typeAES$label,
    values = typeAES$colour
  )+
  labs(
    x="Year",
    y="Proportion",
    fill="ORO type"
  )+
  theme_minimal(base_size = 10)+
  theme(
    legend.position = "bottom"
  )

year_component_prop_ggp

oro_leg <- ggpubr::get_legend(dist_component_ggp)



## Plot all together

plotList <- list(
  dist_component_ggp+theme(plot.margin = unit(rep(0.1, 4),"cm")),
  year_component_ggp+
    theme(legend.position = "none",
          plot.margin = unit(rep(0.1, 4),"cm")
          ),
  year_component_prop_ggp+
    theme(legend.position = "none",
          plot.margin = unit(rep(0.1, 4),"cm")
          )
)

plotLay <- 
"#AA#
BBCC"

combined <- wrap_plots(plotList,
                       design = plotLay,
                       widths = c(1,5,0.2,0.2),
                       heights = c(1,2)
                       )+
  plot_annotation(tag_levels = "a", tag_suffix = ")")


## Save
ggsave(
  here::here("figures/main/distribution_by_component_plots.pdf"),
  width=6.5, height=7,
  plot = combined
)


```



```{r, out.width="0.9\\linewidth", include=TRUE, fig.align="center", fig.cap=c("ORO types across the complementary data dimensions"), echo=FALSE}

knitr::include_graphics(here::here("figures/main/distribution_by_component_plots.pdf"))
```




A closer look at the ORO-specific 'action' metrics:

MRE - installed capacity (MW) - IRENA renewable electricity statistics

Efficiency - domestic freight transport energy efficiency (gCO2/tkm) - IEA

CCS - Storage Capacity associated with a project- oil and gas climate initiative

CDR-BC - # restoration projects - Duarte et al 2020

CDR-OAE/BioPump - # Field trials/startup companies - Ocean Visions Field trials, GESAMP climate intervention projects, OceanNETs ocean-based CDR companies


```{r plot all deployment time series, eval = FALSE}

allDep_ggp <- ggplot(
  data = allComponentDat_model %>%
    filter(component == "deployment") %>%
    mutate(
    oro_type = factor(oro_type, levels = typeAES$level, labels = typeAES$label)
  )
)+
  geom_line(aes(year, y, col = oro_type))+
  geom_text(aes(label = stringr::str_wrap(variable_name, width = 25),
                fontface=3), x=-Inf, y=Inf,
            check_overlap = TRUE, vjust=1, hjust=0, size=3)+
  facet_wrap(vars(oro_type), scales="free")+
  scale_colour_manual(
    breaks = typeAES$label,
    values = typeAES$colour
  )+
  labs(
    x="Year",
    y="Action metric",
  )+
  theme_minimal()+
  theme(
    legend.position = "none"
  )

allDep_ggp


## Save
# ggsave(here::here("figures/supplemental/mitigationDeploymentIndicators.pdf"), plot = allDep_ggp, width = 7, height=6)

```

```{r plot all dimension timeseries together in multipanel figure}
## Plot only timeseries
plotList <- list(
  year_component_ggp+
    theme(legend.position = "none",
          plot.margin = unit(rep(0.1, 4),"cm")
          ),
  year_component_prop_ggp+
    theme(legend.position = "right",
          plot.margin = unit(rep(0.1, 4),"cm")
          ),
  allDep_ggp +theme(plot.margin = unit(rep(0.1, 4),"cm"))
)

plotLay <-
"ABC"

combined <- wrap_plots(plotList,
                       design = plotLay,
                       widths = c(1,1,2.5)
                       )+
  plot_annotation(tag_levels = "a", tag_suffix = ")")


## Save
ggsave(
  here::here("figures/main/distribution_by_component_alltimeseries_plots.pdf"),
  width=14, height=7,
  plot = combined
)
```

```{r, out.width="0.9\\linewidth", include=TRUE, fig.align="center", fig.cap=c("ORO-specific action metrics"), echo=FALSE}

knitr::include_graphics(here::here("figures/supplemental/mitigationDeploymentIndicators.pdf"))
```
  


# Network analysis


Method steps:

Step 1. For each ORO, determine if an edge exists between two nodes using variable lag transfer entropy
Step 2. Generalize these findings across groups of similar OROs by pooling edges into a meta-network. If an edge exists for an ORO, it is included in the meta network. Then, use the network to build a qualitative network model, and simulate press perturbations to the different nodes and record the impact on action.



## Step 1: Variable-lag transfer entropy

For each ORO, determine whether there are links between the time series of the different nodes in our conceptual model (publications, policy, legislation, public interest metrics, action). However, there are likely time lags between the different time series and I am uncertain of how long these lags are; and the relationships may be nonlinear. 

To investigate what might be the best analysis, I think transfer entropy is a good option (see the publications below):


* Behrendt et al. 2019. RTransferEntropy— Quantifying information flow between different time series using effective transfer entropy
* Amornbunchornvej et al 2021. Variable-lag Granger Causality and Transfer Entropy for Time Series Analysis

Uses: To detect significance of information flow from X -> Y with a time lag. unlike Granger Causaility, can detect non-linear relationships.

Data Assumptions:

* evenly spaced, no NA (fill NAs with 0 in our time series)
* stationary data: refers to the idea that the statistical properties of a time series do not change over time. More specifically, a stationary time series is one in which the mean, variance, and autocorrelation structure are constant over time (no noticible changing levels, increasing variance). (use Box-Cox transform to make data normal)
* time lag is fixed (except see variable lag-Transfer entropy with bootstrapping, as bootstrapping approach increases the performance of transfer entropy methods in this task - install.packages("VLTimeCausality"))


VL-TE Pipeline:

* correct for heteroscedasticity using a Box-Cox transformation to make data normal
* get an iterable list of the pairwise edges for the different nodes in the time series
* Use VL-TE (bootstrapped) from the VLTTimeCausality package to get TE ratio and p-value (higher the ratio, stronger evidence for causality)
* Plot the TE ratio for each of the edges to determine the evidence for the relationship 




```{r VL-TE analysis, eval = FALSE}
## Load in results in next chunk

# Define levels to loop through

# Loop 1 -- different oros
oros <- unique(allComponentDat_model$oro_type)

# Loop 2 -- process all possible edges
# Define nodes and edges to process
nodes <- unique(allComponentDat_model$component)
edges <- expand.grid(nodes, nodes)
edges <- edges[edges[,1] != edges[,2],]

# remove edge pairs whose calculations are inter-related (all the public interest metrics)
publicEdges <- grepl("public", edges[,1]) & grepl("public", edges[,2])
edges <- edges[!publicEdges,]


# Function to make BoxCoxTransformation -- make data normal
BCTransform <- function(series){
  BCMod <- caret::BoxCoxTrans(series)
  series_trans <- predict(BCMod, series)
}

# Wrapper to try the box-cox transformation with error handling
try_BCTransform <- function(series){
  tryCatch(
    {BCTransform(series)},
    error = function(e){
      return(series)
    }
  )
}



## Loop 1 -- loop through OROs
TE_results <- data.frame()
TE_results_summary <- data.frame()
for(oro in oros){
  # oro <- "MRE-Ocean" # for testing
  
  # Data processing
  # Fillin missing values with a 0 -- so that same years are represented across time series
  # correct for heteroskedasiticy
  oroDat <- allComponentDat_model %>%
    filter(oro_type == oro) %>%
    complete(component, year=year_lim[1]:year_lim[2], 
             fill = list(y=0), explicit= FALSE) %>%
    arrange(component, year)%>%
    group_by(component) %>%
    mutate(y_trans = try_BCTransform(y)) %>%
    ungroup()
  # ggplot(oroDat, aes(x=year, y=y_trans))+geom_line()+facet_wrap(vars(component), scales="free_y")
  
  
  edgesResults <- data.frame(
    oro_type = rep(oro, nrow(edges)),
    NodeX = edges[,1],
    NodeY = edges[,2],
    TE_XCauseY = rep(NA, nrow(edges)),
    TE_pval = rep(NA, nrow(edges)),
    TE_ratio = rep(NA, nrow(edges)),
    optDelay = rep(NA, nrow(edges)),
    optCor = rep(NA, nrow(edges)),
    dtw=rep(NA, nrow(edges))
  )
  
  # Loop through the edges and test for causality
  for(e in 1:nrow(edges)){
    
    X = oroDat$y_trans[oroDat$component == edges[e,1]]
    Y = oroDat$y_trans[oroDat$component == edges[e,2]]
    if(length(X) != length(Y)){next}
    
    # remove leading or trailing zeros
    edge_dat <- data.frame(
      X = X,
      Y = Y
    )
    # trim leading and trailing zeros
    first_nonzero <- min(which(rowSums(edge_dat != 0) ==ncol(edge_dat)))
    last_nonzero  <- max(which(rowSums(edge_dat != 0) ==ncol(edge_dat)))
    edge_dat <- edge_dat[first_nonzero:last_nonzero, ]
    
    if(nrow(edge_dat) == 0){next}
    

    TE_out <- tryCatch(
      {VLTransferEntropy(
      X= edge_dat$X,
      Y= edge_dat$Y,
      maxLag = 5,
      VLflag=TRUE,nboot=100, alpha = 0.05)},
      error = function(e){
        return(NULL)
      }
    )
    
    if(is.null(TE_out)){
      edgesResults$TE_pval[e] <- NA
      edgesResults$TE_XCauseY[e] <- NA
      edgesResults$TE_ratio[e] <- NA
      edgesResults$optDelay[e] <- NA
      edgesResults$optCor[e] <- NA
      edgesResults$dtw[e] <- NA
    }else{
      
      ## Save outputs into data frame
      edgesResults$TE_pval[e] <- TE_out$pval
      # TS$X causes TS$Y TRUE/FALSE
      edgesResults$TE_XCauseY[e] <- TE_out$XgCsY_trns 
      # Transfer entropy ratio -- If it is greater than one , then X causes Y.
      edgesResults$TE_ratio[e] <- TE_out$TEratio
      # optimal time delay inferred by cross-correlation of X,Y. It is positive if Y is simply just a time-shift of X (e.g. Y[t]=X[t-optDelay]).
      edgesResults$optDelay[e] <- TE_out$follOut$optDelay 
      # time series of optimal warping-path from DTW that is corrected by cross correlation. It is approximately that Y[t]=X[t-optIndexVec[t]])
      edgesResults$dtw[e] <- mean(TE_out$follOut$optIndexVec[,1], na.rm=T)
      # optimal correlation of Y[t]=X[t-optDelay] for all t
      # Maybe if optCor is negative, that can give me the sign 
      edgesResults$optCor[e] <- TE_out$follOut$optCor
    }
    
  }
  
  # summarize results to most causal direction if a two-way effect found
  # ie. When TRUE for both directions, resolve by picking the direction with the highest TE ratio
  edgesResultsSummary <- data.frame()
  uniqueEdges <- edges[!apply(edges, 1, is.unsorted), ]
  for(ue in 1:nrow(uniqueEdges)){
    
    incl = vector(length = nrow(edgesResults))
    for(i in 1:nrow(edgesResults)){
      tmp <- c(edgesResults[i,c("NodeX","NodeY")]) %in% uniqueEdges[ue,]
      incl[i] <- sum(tmp)==2
    }
    tmpDat <- edgesResults[incl,] %>%
      filter(TE_XCauseY) %>%
      arrange(desc(TE_ratio)) %>%
      slice_head(n=1)
    
    edgesResultsSummary <- edgesResultsSummary %>%
      bind_rows(tmpDat)
  }
  
    
  # Bind results
  TE_results <- TE_results %>%
    bind_rows(edgesResults)
  TE_results_summary <- TE_results_summary %>%
    bind_rows(edgesResultsSummary)
  
}

# clean
# rm(oroDat, nodes, edges, TE_out, edgesResults)




save(TE_results, TE_results_summary, file = here::here("data/derived-data/TE_results.RData"))
```



```{r load TE results}

load(here::here("data/derived-data/TE_results.RData")) # TE_results, TE_results_summary

```


```{r plot of VLTE time series transform for method figure}
oro = "Efficiency" #"MRE-Located"
varx= "policy" # publications
vary = "deployment"

oroDat <- allComponentDat_model %>%
    filter(oro_type == oro) %>%
    complete(component, year=year_lim[1]:year_lim[2], 
             fill = list(y=0), explicit= FALSE) %>%
    group_by(component) %>%
    arrange(component, year)%>%
    mutate(y_trans = try_BCTransform(y)) %>%
    ungroup()

year = oroDat$year[oroDat$component == varx]
X = oroDat$y_trans[oroDat$component == varx] # cause
Y = oroDat$y_trans[oroDat$component == vary] # effect



# remove leading or trailing zeros
edge_dat <- data.frame(
  X = X,
  Y = Y,
  year=year
)
# trim leading and trailing zeros


first_nonzero <- min(which(rowSums(edge_dat != 0) ==ncol(edge_dat)))
last_nonzero  <- max(which(rowSums(edge_dat != 0) ==ncol(edge_dat)))
edge_dat <- edge_dat[first_nonzero:last_nonzero, ]


# year = oroDat$year[oroDat$component == "publications"][1:24]
# X = oroDat$y_trans[oroDat$component == "publications"][1:24] # cause
# Y = oroDat$y_trans[oroDat$component == "deployment"][1:24] # effect

plot(edge_dat$Y)
plot(edge_dat$X)
  
TE_out <- tryCatch(
  {VLTransferEntropy(
  Y = edge_dat$Y, 
  X = edge_dat$X, 
  maxLag = 5,
  VLflag=TRUE,nboot=100, alpha = 0.05)},
  error = function(e){
    return(NULL)
  }
)

TE_out

vlte_df <- rbind(
  data.frame(
    series = paste("X"),
    x = edge_dat$year,
    y=edge_dat$X,
    dtw_lag = NA
  ),
  data.frame(
    series = paste("Y"),
    x = edge_dat$year,
    y = edge_dat$Y,
    dtw_lag = NA
  ),
  data.frame(
    series = paste("X (modified lag to match Y)"),
    x = edge_dat$year,
    y = edge_dat$X[1:length(edge_dat$X)-TE_out$follOut$optIndexVec[,1]],
    dtw_lag = -TE_out$follOut$optIndexVec[,1]
  )
) %>%
  mutate(
    series = factor(series,
                    levels=c(
                      "X","Y","X (modified lag to match Y)"
                    )
                    # labels = c(
                    #   "X (Policy)",
                    #   "Y (Action)",
                    #   "X (modified lag to match Y)"
                    # )
                    )
  )%>%
  arrange(series, y)

vlte_ggp <- ggplot(data=vlte_df, aes(x=x, y=y, color = series))+
  geom_line()+
  geom_text(aes(label = dtw_lag), vjust=0)+
  # scale_color_brewer(type="qual", palette = "Dark2")+
  scale_color_manual(
    values = componentAES$colour[
      c(
        which(componentAES$level == varx),
        which(componentAES$level == vary),
        which(componentAES$level == varx)
      )
    ]
  )+
  facet_wrap(vars(series),ncol=1, scales = "free_y")+
  labs(
    x="Year", y="Value"
  )+
  theme_minimal(base_size=10)+
  theme(
    legend.position = "none"
  )

vlte_ggp

  
ggsave(
  here::here("figures/supplemental/VLTE_demonstration_plot.pdf"),
  width = 4, height=3,
  plot = vlte_ggp
)

ggsave(
  here::here("figures/supplemental/VLTE_demonstration_plot.png"),
  width = 4, height=3,
  dpi=600,
  plot = vlte_ggp
)
```


## Step 2: Create Network by ORO group from TE edges

Aggregate Causal edges (identified from VL transfer entropy) Across oro types

Aggregate the TE results across similar oro_types to build a meta-causal graph:

For each unique edge, compute:

* N significant: how many oro_types had significant causality. This indicates agreement/consistency
* Mean TE_ratio: average causal strength across significant cases.
* sign (direction of edge effect): from optimal correlation sign
* Optimal lag (not the same as all the lags for each time point from dynamic time warping, but the optimal lag over the whole time series)


Visualize the aggregated meta-network where:

* Edge width = mean TE ratio.
* Edge transparency = consistency (e.g. normalized N significant).
* Edge color = sign of correlation

```{r group oro networks conceptually and plot combined networks}
require(ggraph)
require(tidygraph)



## Get diagraph objects for each defined cluster
# Create a data.frame for lookup
concept_cluster_df <- data.frame(
  oro_type = c(
    "CCS",
    "CDR-BC","CDR-BioPump","CDR-OAE",
    "Efficiency",
    "MRE-Bio","MRE-Located", "MRE-Ocean"
  ),
  cluster = c(1,rep(2,3),3,rep(4,3))
)

n_clusters <- length(unique(concept_cluster_df$cluster))

# List to hold one meta-graph per cluster
concept_meta_graphs <- vector("list", n_clusters)
concept_cluster_names <- vector("list", n_clusters)

for (k in 1:n_clusters) {
  # Get oro_types in this cluster
  types_in_cluster <- concept_cluster_df %>%
    filter(cluster == k) %>%
    pull(oro_type)
  
  # Filter TE results for these oro_types and significant edges
  cluster_te <- TE_results %>%
    filter(oro_type %in% types_in_cluster, 
           NodeX %in% selectedComponents,
           NodeY %in% selectedComponents,
           TE_pval <= 0.05,
           !is.na(TE_ratio),
           0 < TE_ratio
           )
  cluster_te$TE_ratio[cluster_te$TE_ratio==Inf] <- max(cluster_te$TE_ratio[is.finite(cluster_te$TE_ratio)])
  
  # Aggregate TE_ratio per NodeX → NodeY pair
  edge_summary <- cluster_te %>%
    ungroup()%>%
    group_by(NodeX, NodeY) %>%
    summarise(
      n_sig = n(),
      avg_ratio = mean(TE_ratio, na.rm = TRUE),
      avg_correlation = mean(optCor, na.rm=T),
      med_correlation = quantile(optCor, na.rm=T, probs = 0.5),
      avg_delay = mean(optDelay, na.rm=T),
      sign = ifelse(avg_correlation >= 0, 1, -1),
      oro_types = paste(unique(oro_type), collapse = ", "), 
      .groups="drop"
    )%>%
    mutate(
      relative_nsig = n_sig / max(n_sig, na.rm=T)
    )
  
  # Build igraph object
  g <- graph_from_data_frame(edge_summary, directed = TRUE)
  
  # Set edge weights and labels
  E(g)$weight <- edge_summary$avg_ratio
  E(g)$width <- log1p(E(g)$weight) * 1.5
  # E(g)$color <- scales::alpha("black", edge_summary$relative_nsig)
  E(g)$color <- scales::alpha(ifelse(E(g)$avg_correlation >= 0, "#4dac26", "#d01c8b"),edge_summary$relative_nsig)
  E(g)$sign <- edge_summary$sign

  
  concept_meta_graphs[[k]] <- g
  concept_cluster_names[[k]] <- paste(typeAES$label[match(types_in_cluster, typeAES$level)], collapse = ", ")
}






## Plot the networks of the oro types


nodeTextSize <- 3.5
titleSize <- 12



# Convert network to ggplot
source(here::here("R/compute_negative_edge_dots.R"))

plot_ggraph_network <- function(g, cluster_id, cluster_label, end_cap_mm = 10, end_cap_mult_factor=4) {
  
  # maybe plot separately by positive and negative so I can make arrows and dots?
  g_tidy <- as_tbl_graph(g) 
  
  # Code for plotting dots at end of negative effects
  # --- Create layout (needed to access node positions) ---
  graph_layout <- create_layout(g_tidy, layout = "circle")
  
  # --- Extract negative edge targets ---
  neg_edges <- g_tidy %>%
    activate(edges) %>%
    filter(sign == -1) %>%  # Filter for negative edges
    as_tibble() %>%
    mutate(
      from_name = V(g_tidy)$name[from],
      to_name = V(g_tidy)$name[to]
    )
  
  # Get positions of source and target nodes for negative edges
  edge_positions <- graph_layout %>%
    filter(name %in% c(neg_edges$from_name, neg_edges$to_name))
  
  dot_positions <- compute_negative_edge_dots(g, layout = "circle", end_cap_mm, end_cap_mult_factor)

  
  ## plot
  ggraph(g_tidy, layout = "circle") +  # You can try "kk" or "circle" or "fr" for spacing
    
    # # --- Positive edges with arrows ---
    geom_edge_fan(
      aes(
        width = weight,
        alpha = relative_nsig,
        label = paste0("lag = ", scales::number(avg_delay, accuracy = 1), ", ", oro_types),
        color = color,
        filter = sign==1
      ),
      arrow = arrow(length = unit(7, 'mm')),
      end_cap = circle(end_cap_mm, 'mm'),
      label_dodge = unit(0, 'mm'),
      label_push = unit(2, 'mm'),
      angle_calc = 'along',
      label_size=3,
      show.legend = FALSE
    ) +
  
    # --- Negative edges with dot ends (no arrow) ---
    geom_edge_fan(
      aes(
        width = abs(weight),  # abs for width scaling
        alpha = relative_nsig,
        label = paste0("lag = ", scales::number(avg_delay, accuracy = 1), ", ", oro_types),
        color = color,
        filter = sign==-1
      ),
      end_cap = circle(end_cap_mm, 'mm'),  # no arrow
      lineend = "round",   
      label_dodge = unit(0, 'mm'),
      label_push = unit(2, 'mm'),
      angle_calc = 'along',
      label_size=3,
      show.legend = FALSE
    ) +
    
    # --- Draw a point (dot) at target node of negative edges ---
    geom_point(
      data = dot_positions,
      aes(x = dot_x, y = dot_y, size = 2*log1p(weight),alpha = relative_nsig),
      shape = 21,             
      fill = "#d01c8b",
      color="transparent",
      show.legend = FALSE
    ) +
    scale_size_identity()+
    
    # Plot nodes
    # geom_node_point(size = 6, aes(color = name)) +
    
    scale_color_manual(values = componentAES$colour,
                       breaks = componentAES$level, 
                       guide="none")+
    scale_fill_manual(values = componentAES$colour,
                       breaks = componentAES$level, 
                       guide="none")+
    scale_edge_color_identity()+
    scale_x_continuous(expand = expansion(mult=0.1))+
    scale_y_continuous(expand = expansion(mult=0.1))+

    geom_node_label(aes(label = componentAES$label[match(name, componentAES$level)], fill = name), #repel = TRUE,
                   label.size = nodeTextSize, alpha=0.8
                   ) +

    labs(title = paste(cluster_label), tag = letters[cluster_id]) +
    theme_void() +
    theme(
      plot.title = element_text(size = titleSize, hjust = 0.5),
      plot.margin = unit(c(5,5,5,5), units = "mm"),
      legend.position = "none"
      )
}

cluster_order <- 1:n_clusters
concept_cluster_names2 <- list(
  "CCS","mCDR","Efficiency","MRE"
)
concept_ggraph_plots <- lapply(cluster_order, function(k) {
  plot_ggraph_network(concept_meta_graphs[[k]], k, concept_cluster_names2[[k]], end_cap_mm = 10, end_cap_mult_factor=1.8)
})



# Arrange the ggraph plots horizontally according to cluster number
final_plot <- wrap_plots(concept_ggraph_plots, nrow = 2, widths = c(1,1.2))


# Show or save
ggsave(filename=here::here("figures/main/conceptual_cluster_networks.pdf"), final_plot, width = 10, height = 8)

```

```{r plot just efficiency network for method figure}
ggsave(
  here::here("figures/supplemental/networks_efficiency.png"),
  width = 4, height=4, dpi=600,
  plot = concept_ggraph_plots[[which(concept_cluster_names2 == "Efficiency")]]
)

```



```{r for mCDR and MRE and Efficiency calculate node importance and influence}
plot_network_metrics <- function(network_graph){
  # causal influencers - legislation, policy
  influencers <- degree(network_graph, mode = "out") %>% 
    sort(decreasing = T) %>% 
    as.data.frame() %>%
    tibble::rownames_to_column("component") %>%
    mutate(metric = "Out-degree (N effects given)")
    
  # causal responders - deployment - 8
  responders <- degree(network_graph, mode = "in") %>% 
    sort(decreasing = T) %>% 
    as.data.frame() %>%
    tibble::rownames_to_column("component") %>%
    mutate(metric = "In-degree (N effects received)")
  
  
  # general influence - legislation                    policy           public interest
  eigenvalues <- sort(eigen_centrality(network_graph)$vector, decreasing =TRUE)  %>%
    as.data.frame() %>%
    tibble::rownames_to_column("component") %>%
    mutate(metric = "Eigenvector centrality")
  
  # mediators - deployment - 14
  mediators <- sort(betweenness(network_graph), decreasing=TRUE)%>%
    as.data.frame() %>%
    tibble::rownames_to_column("component") %>%
    mutate(metric = "Betweenness (N shortest paths)")
  
  
  ## format results 
  metaNetCentralityMetrics_tmp <- rbind(
    influencers,
    responders,
    eigenvalues,
    mediators
  )%>%
    mutate(
      component = factor(
        component, 
        levels = componentAES$level[componentAES$level %in% selectedComponents],
        labels = componentAES$label[componentAES$level %in% selectedComponents]
      )
    )%>%
    complete(component, metric)
  
  summary(metaNetCentralityMetrics_tmp)
  
  colnames(metaNetCentralityMetrics_tmp)[!(colnames(metaNetCentralityMetrics_tmp) %in% c("component","metric"))] <- "Value"
  
  ## make plot
  ## Plot of a heatmap of each metric value
  networkMetricstmp_ggp <- ggplot(
    data = metaNetCentralityMetrics_tmp %>%
      group_by(metric) %>%
      mutate(
        Value_scaled = scales::rescale(Value, to = c(0,1))
      ),
    aes(x=component, y=metric)
  )+
    geom_tile(aes(fill = Value_scaled))+
    # geom_text(aes(label = scales::number(Value, accuracy=0.1)))+
    scale_y_discrete(
      labels = function(x) stringr::str_wrap(x, width=15)
    )+
    scale_x_discrete(
      breaks = levels(metaNetCentralityMetrics_tmp$component),
      drop=FALSE
    )+
    scale_fill_distiller(palette = "Blues", direction=1, na.value = "grey")+
    labs(
      x = "Node",
      y="Network metric",
      fill = "Metric\n(scaled)"
    )+
    theme_minimal(base_size = 12)+
    theme(
      axis.text.x = element_text(angle = 45, hjust=1)
      # legend.position = "bottom"
    )
  # networkMetricstmp_ggp
  return(networkMetricstmp_ggp)
}


concept_metrics_heatmaps <- lapply(c(2,3,4), function(k) {
  plot_network_metrics(concept_meta_graphs[[k]])
})


```

```{r SKIP and load in next chunk - for mCDR and MRE and Efficiency compute QNM , eval=FALSE}
source(here::here("R/impact.barplot.myMod"))
source(here::here("R/extend.vector"))
source(here::here("R/my.QPress.R"))
addTaskCallback(function(...) {set.seed(123);TRUE})



# Function to run press perturbation with timeout if it freezes
safe_impact_barplot <- function(ModSim, press, timeout = 30) {
  tryCatch(
    withTimeout(
      {
        impact.barplot.myMod(ModSim,
            perturb = press,
            plot=FALSE, percentage = TRUE
        )
      },
      timeout = timeout,
      onTimeout = "silent"   # do not throw error on timeout
    ),
    error = function(e) NULL,
    TimeoutException = function(e) NULL
  )
}

plot_QNM <- function(k, nSims = 5000, weightSimulation = FALSE, uncertainThreshold=0.5){
  
  # Get oro_types in this cluster
  oroTypes <- concept_cluster_df %>%
    filter(cluster == k) %>%
    pull(oro_type)

  ## 1. Pool the transfer entropy results
  sig_te_edges <- TE_results %>%
    filter(oro_type %in% oroTypes,
           NodeX %in% selectedComponents,
             NodeY %in% selectedComponents,
             TE_pval <= 0.05,
             !is.na(TE_ratio),
             0 < TE_ratio
             ) %>%
    group_by(NodeX, NodeY) %>%
    summarise(
      n_sig = n(),
      avg_ratio = mean(TE_ratio, na.rm = TRUE),
      avg_correlation = mean(optCor, na.rm=T),
      med_correlation = quantile(optCor, na.rm=T, probs = 0.5),
      avg_delay = mean(optDelay, na.rm=T),
      sign = ifelse(avg_correlation >= 0, 1, -1),
      oro_types = paste(unique(oro_type), collapse = ", "), 
      .groups = 'drop'
    )%>%
    mutate(
      Probability = n_sig / max(n_sig),
      Group = ifelse(uncertainThreshold < Probability,0,1),
      Type = ifelse(sign==1,"P","N")
    ) %>%
    arrange(desc(Probability)) %>%
    select(From=NodeX, To=NodeY, Group, Probability, Type)
  

  
  
  
  # Format the data frame for Q Press
  sig_nodes <- as.character(unique(c(sig_te_edges$From, sig_te_edges$To)))
  
  sig_te_edges <- sig_te_edges %>%
    mutate(
      From = factor(From, levels = sig_nodes),
      To = factor(To, levels=sig_nodes),
      Type = factor(Type, levels = c("N","P","U","Z")),
      # Group = factor(Group)
    ) %>%
    ungroup() %>%
    ## add Pair
    mutate(
      # Convert From and To to character to avoid issues with factors
      From_char = as.character(From),
      To_char = as.character(To),
      # Sort From and To alphabetically
      PairKey = pmin(From_char, To_char),
      PairKey = paste(PairKey, pmax(From_char, To_char), sep = "-")
    ) %>%
    mutate(Pair = as.integer(factor(PairKey)))
  
  
  
  
  allEdgesLabels <- edge.labels(sig_te_edges) 
  
  Mod <- enforce.limitation(sig_te_edges%>% select(From, To, Group, Type, Pair)) 
  
  
  ## 2. Build Qualitative network model
  # Optional: Simulate with weights?
  if(weightSimulation){
    s <- community.sampler(Mod)
    s$select(mean(uncertainSampleProbsDf$Probability))
    ModSim <- tryCatch(
      {system.simulate(nSims, Mod, s)},
      error = function(e){NULL}
    )
   
  }else{
    ModSim <- tryCatch(
      {system.simulate(nSims, Mod)},
      error = function(e){NULL}
    )
  }
  
  ## If Mod sim Null, return nothing, else calculate presses
  if(is.null(ModSim)){
    return(NULL)
  }else{
    
    ## 3. For each node (except action), simulate a press perturbation and record the response of action.
  
    # Loop over interventions on each node → observe effect on "deployment"
    all_nodes <- sig_nodes[sig_nodes != "deployment"]
    qp_results_df <- data.frame()
    for (node in all_nodes) {
      press <- c(1)
      names(press) <- node
    
      # Set evidence and query deployment
      temp <- safe_impact_barplot(ModSim, press, timeout = 30)
      if(is.null(temp)){
        return(tibble(Negative=NA, `No Change`=NA, Positive=NA, Press_perturb=node))
      }else{
        temp <- as.data.frame(t(temp["deployment",]))
        temp$Press_perturb <- node
      }
      
      qp_results_df <- qp_results_df %>% bind_rows(temp)
      
    } # end of calculating presses
    
    

    # Plot results
    uniquePP <- unique(qp_results_df$Press_perturb)
    
    qnm_ggp <- ggplot(qp_results_df, 
           aes(x=Press_perturb))+
      geom_col(aes(y=Positive), fill="#4dac26")+ 
      geom_col(aes(y=-Negative), fill = "#d01c8b")+
      
      geom_text(aes(y=Positive, label = Positive), vjust=0)+
      geom_text(aes(y=-Negative, label = Negative), vjust=1)+
      
      geom_hline(yintercept=0, col="red")+
      # scale_fill_stepsn(name = stringr::str_wrap("Positive effect on action (% sims)", width=25), n.breaks=5, colours = viridis(5))+
      scale_x_discrete(
        breaks = componentAES$level[
          componentAES$level %in% uniquePP
        ],
        labels = componentAES$label[
          componentAES$level %in% uniquePP
        ]
      )+
      scale_y_continuous(expand = expansion(mult=0.15))+
      labs(y="Effect on action (% sims)",x="Positive press perturbation node")+
      theme_minimal(base_size = 10)+
      theme(
        legend.position = "bottom",
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.x=element_text(angle=45, hjust=1)
      )
    
    return(qnm_ggp)
    
  } # end of !is.null(ModSim)
  
    
}


concept_qnm_barplots <- lapply(c(2,3,4), function(k){
  plot_QNM(k=k)
})

save(concept_qnm_barplots, file=here::here("data/derived-data/concept_qnm_barplots.RData"))

```

```{r LOAD QNM for different ORO networks}
load(here::here("data/derived-data/concept_qnm_barplots.RData"))

```

```{r plot networks and network metrics heatmaps and qnm results for conceptual clusters}

concept_ggraph_plots2 <- lapply(cluster_order, function(k) {
  plot_ggraph_network(concept_meta_graphs[[k]], k, concept_cluster_names2[[k]], end_cap_mm = 10, end_cap_mult_factor=2.5)
})

plotList <- c(
  # MRE
  concept_ggraph_plots2[[4]],
  concept_metrics_heatmaps[[3]],
  concept_qnm_barplots[[3]],
  
  # Efficiency
  concept_ggraph_plots2[[3]],
  concept_metrics_heatmaps[[2]],
  concept_qnm_barplots[[2]],
  
  # mCDR
  concept_ggraph_plots2[[2]],
  concept_metrics_heatmaps[[1]],
  concept_qnm_barplots[[1]]
  
)

combined <- wrap_plots(plotList, ncol=3,
                       guides="collect", widths = c(1.6, 1,1))+
  plot_annotation(tag_levels = "a", tag_suffix = ")")

ggsave(
  here::here("figures/main/conceptual_networks_heatmaps_qnm.pdf"),
  width=12, height=12, 
  plot=combined
)

```

```{r, out.width="0.9\\linewidth", include=TRUE, fig.align="center", fig.cap=c("Networks and associated metrics grouped by type of ORO", echo=FALSE}

knitr::include_graphics(here::here("figures/main/conceptual_networks_heatmaps_qnm.pdf"))
```



# Match/mis-match (regression)

 
Which OROs recieve more/less attention in policy, legislation and social media, given their number of publications?


NB: Because compares OROs between each other, cannot include deployment because all the metrics are on different scales

*Kieran questions*

* Did I select the correct probability distributions for the response variables?
* Do you notice any errors in the analysis/interpretation?



```{r fit glm of proportional share of component ~ proportional share of publication + year, eval = FALSE}


# “For each ORO, how many public-interest posts do you get per publication?”
# EG the probability of getting a post per publication.
# Include year as a predictor to account for the temporal trend.
# Because temporal 


## Loop through the different components and fit glm
# sum metric values across years

# different components to loop through
components <- unique(allComponentDat_model$component) 
components <- components[!(components %in% c("publications","deployment"))]

ggps_prop <- vector("list", length(components))
names(ggps_prop) <- components
modelResultsProp <- vector("list", length(components))
for(c in 1:length(components)){
  
  ## Fit the model
  tmpDat_annualSum <- allComponentDat_model %>%
    filter(component %in% c("publications", components[c])) %>%
    group_by(year, component) %>%
    summarise(
      total = round(sum(y, na.rm = T))
    ) %>%
    pivot_wider(names_from = component, values_from = total)

  tmpDat <- allComponentDat_model %>%
    mutate(
      y= round(y)
    )%>%
    filter(component %in% c("publications", components[c])) %>%
    pivot_wider(names_from = component, values_from = y, id_cols = c("oro_type","year")) %>%
    left_join(tmpDat_annualSum, by = c("year"), suffix = c("","_total")) %>%
    mutate(across(where(is.numeric), ~ ifelse(is.na(.x), 0, .x))) %>%
    mutate(
      prop_pub = publications/publications_total,
      oro_type = factor(
        oro_type,
        levels = typeAES$level,
        labels = typeAES$label
      )
    )
  
  colnames(tmpDat) <- gsub(components[c],"y", colnames(tmpDat))
  tmpDat$prop_y <- tmpDat$y/tmpDat$y_total
  tmpDat$prop_y[is.na(tmpDat$prop_y)] <- 0
  tmpDat$fail <- as.integer(tmpDat$y_total-tmpDat$y)
  tmpDat$y <- as.integer(tmpDat$y)
  tmpDat<- tmpDat[0<tmpDat$y_total,]

  mod <- glm(cbind(y, fail) ~ prop_pub + year, 
                 data = tmpDat ,
                 family = binomial)
  
  od <- performance::check_overdispersion(mod)
  if(od$dispersion_ratio > 1 & od$p_value < 0.05){
    
    mod <- glm(cbind(y, fail) ~ prop_pub + year, 
                 data = tmpDat,
                 family = quasibinomial)
    
  }
  
  
  plot_model(mod, type = "pred", terms = c("prop_pub"))
  
  
  # The coefficients table for oro_type (exclude intercept and year)
  mod_sum <- summary(mod)
  
  if(mod_sum$family$family == "quasibinomial"){
    distributionLabel <- paste0(
      "Quasibinomial (dispersion = ",
      scales::number(mod_sum$dispersion, accuracy = 1, big.mark = ","),")"
    )
    
  }else{
    distributionLabel <- "Binomial"
  }
  
  ## store model results
  componentLabel <- componentAES$label[match(components[c], componentAES$level)]
  
  modSummary <- as.data.frame(mod_sum$coefficients) %>%
    rownames_to_column("Term") %>%
    mutate(Component = 
             paste0(
               componentLabel
             ),
    Distribution = distributionLabel) %>%
    relocate(Component) %>%
    mutate(
      Term = gsub("prop_pub","Proportion of publications", gsub("year", "Year", Term))
    )
  

  modelResultsProp[[c]] <- modSummary

  pval = data.frame(
    estimate = paste(signif(modSummary[grepl("publications",modSummary$Term),grep("Estimate", colnames(modSummary))],
                                    digits = 2)),
    pval = ufs::formatPvalue(modSummary[grepl("publications",modSummary$Term),grep("Pr", colnames(modSummary))])
  ) %>%
    mutate(
      label = paste0(
        "\u03b2 = ", estimate,"\n",pval
      )
    )
  
  
  # add a shape variable
  shapelookup <- tmpDat %>%
    distinct(oro_type)%>%
     mutate(
       oro_branch = case_when(
         grepl("MRE", oro_type)~"MRE",
         grepl("CDR",oro_type)~"CDR",
         TRUE~"Other"
       )
     )%>%
    group_by(oro_branch)%>%
    mutate(
      shape_type = factor(row_number(), levels = 1:3, labels = c(15, 16, 17))
    )
  tmpDatPlot <- tmpDat%>%
    left_join(shapelookup, by = "oro_type")
  
  
  tmpDatSums <- tmpDat %>%
    group_by(oro_type) %>%
    summarise(
      prop_y = mean(prop_y, na.rm = T),
      prop_pub = mean(prop_pub, na.rm=T)
    )
    
 
  
  ## Plot 
  pred <- ggpredict(mod, terms = "prop_pub [all]")
  
  ggps_prop[[c]] <- ggplot(data = pred)+
    geom_line(aes(x, predicted), col="black",linetype = "solid", size=0.7)+
    geom_ribbon(aes(x=x, ymin = conf.low, ymax = conf.high), alpha = 0.1)+
    geom_point(data = tmpDatPlot,
               aes(x=prop_pub, y = prop_y, shape = shape_type, col=oro_type),
                             size=3, alpha = 0.25)+
    ggrepel::geom_text_repel(data = tmpDatSums, aes(x=prop_pub, y = prop_y, label = oro_type), force = 0.3,
                             size=3)+
    ggrepel::geom_text_repel(data = pval, aes(label = label, fontface=3), 
                             x=Inf, y=-Inf, hjust=1, vjust=0, col="black",
                             size=3.5)+
    scale_color_manual(
      breaks = typeAES$label,
      values = typeAES$colour
    )+
    scale_shape_manual(
      breaks = levels(shapelookup$shape_type),
      values = c(15, 16, 17)
    ) +
    guides(
      color = guide_legend(override.aes = list(shape = as.numeric(as.character(shapelookup$shape_type[order(shapelookup$oro_type)])))),
      shape = "none" 
    )+
    scale_x_continuous(expand = expansion(mult = c(0.15,0)))+
    scale_y_continuous(expand = expansion(mult = c(0.15,0)))+
    labs(
      x = "Proportion of publications",
      y = paste("Proportion of",tolower(componentAES$label[componentAES$level == components[c]])),
      color = "ORO type"
    )+
    theme_minimal()+
    theme(
      axis.title = element_text(size=9),
      plot.margin = unit(c(0.7,0.3,0.3,0.3),"cm")
    )
  
  ggps_prop[[c]]
  
  leg <- ggpubr::get_legend(ggps_prop[[c]])
  ggps_prop[[c]] <- ggps_prop[[c]]+theme(legend.position = "none")
  

 
  
}

modelResultsPropTable <- do.call("bind_rows", modelResultsProp)

modelResultsPropTable[,3:6] <- apply(modelResultsPropTable[,3:6],1:2, function(x) format(as.numeric(x),digits = 2, scientific=TRUE))





# ## save
# 
# write.csv(
#   modelResultsPropTable,row.names = FALSE,
#   here::here("outputs/publications_otherDim_glmResults_proportion.csv")
# )









ggps_prop_tagged <- mapply(
  function(p, tag) p + 
    labs(tag = tag) + 
    theme(legend.position = "none",
          plot.tag.position = c(0, 1.1),  # top-left like ggpubr
          plot.tag = element_text(size = 10, hjust = 0, vjust = 1.2),
          plot.margin = unit(c(1,0.3,0,0.3),"cm")
          ),
  ggps_prop,
  paste0(letters[1:length(ggps_prop)], ") ", 
         componentAES$label[match(names(ggps_prop), componentAES$level)]),
  SIMPLIFY = FALSE
)

combined <- wrap_plots(ggps_prop_tagged, ncol = 2)


# Combine plots with legend on the right
final_plot <- combined + leg +theme(plot.margin = unit(c(0.1,0,0,0.1),"cm")) + 
  plot_layout(ncol = 3, widths = c(4,4, 2), byrow=FALSE)  # adjust widths as needed


ggsave(
  filename = here::here("figures/main/GlmFitPlots_proportion.pdf"),
  plot = final_plot,
  width = 10,
  height = 6.5
)

```

```{r plot just significant regression plots}

ggps_prop_tagged_signif <- mapply(
  function(p, varname, tag) p + 
    labs(y=varname, tag = tag) + 
    theme(#legend.position = "none",
          plot.tag.position = c(0, 1.1),  # top-left like ggpubr
          plot.tag = element_text(size = 10, hjust = 0, vjust = 1.2),
          plot.margin = unit(c(1,0.3,0,0.3),"cm")
          ),
  ggps_prop[c("legislation","public support")],
  c("Relative N legislation documents","Relative N positive posts"),
  paste0(letters[1:2], ") ", 
         # componentAES$label[match(c("legislation","public support"), componentAES$level)]),
         c("Legislation","Positive social media posts")),
  SIMPLIFY = FALSE
)

combined <- wrap_plots(ggps_prop_tagged_signif, ncol = 2)


# Combine plots with legend on the right
final_plot <- combined + leg +theme(plot.margin = unit(c(0.1,0,0,0.1),"cm")) + 
  plot_layout(ncol = 3, widths = c(4,4, 2), byrow=FALSE)  # adjust widths as needed


ggsave(
  filename = here::here("figures/poster/GlmFitPlots_signif_proportion.png"),
  plot = final_plot,
  width = 9,
  height = 4,
  dpi=2000
)


```

```{r combine regression plots with plots of distribution by ORO type}

## Get data and plot distribution by oro for the different dimensions 

p1_db <- RSQLite::dbConnect(RSQLite::SQLite(),
                      here::here("data","sqlite-databases","product1.sqlite"),
                                 create=FALSE)

nPubs_oro <- tbl(p1_db, "pred_oro_type_mit_long") %>%
  group_by(level)%>%
  summarise(
    N = n_distinct(analysis_id[0.5 <= (mean_prediction)])
  )%>%
  collect()%>%
  mutate(
    oro_type = gsub("[.]","-",level),
    component = "publications",
    variable_name = "Publications (N)"
  ) %>%
  select(oro_type, component, variable_name, N)

dbDisconnect(p1_db)



## Sumarise by component overall proportional shar of each ORO
prop_oro_summary_df <- allComponentDat_model %>%
  filter(!(component %in% c("deployment","publications"))) %>%
  group_by(component, variable_name, oro_type) %>%
  summarise(
    N = sum(y, na.rm=T)
  ) %>%
  bind_rows(
    nPubs_oro
  ) %>%
  group_by(component) %>%
  mutate(
    total_component = sum(N, na.rm=T)
  ) %>%
  ungroup() %>%
  mutate(
    prop_component = N/total_component,
    component = factor(component, 
                       levels = componentAES$level, 
                       labels = componentAES$label_varname)
  )%>%
  mutate(
    component = droplevels(component),
    oro_type = factor(
      oro_type,
      levels = typeAES$level,
      labels = typeAES$label
    )
  )
  


# Plot proportion of each dimension per ORO
dist_component_ggp_bottom <- ggplot(data = prop_oro_summary_df, aes(x=component))+
  geom_col(aes(y=prop_component, fill=oro_type), position="stack")+
  geom_text(aes(label = scales::number(total_component, accuracy = 1, big.mark = ",")), check_overlap = TRUE, vjust=0, y=1)+
  scale_fill_manual(
    breaks = typeAES$label,
    values = typeAES$colour
  )+
  scale_y_continuous(expand = expansion(mult=c(0,0.1)))+
  labs(
    x="Dimension (i.e. Node)",
    y="Proportion",
    fill="ORO type",
    tag = "a"
  )+
  guides(fill=guide_legend(ncol=2))+
  theme_minimal(base_size = 10)+
  theme(
    axis.text.x = element_text(angle=45, hjust=1),
    legend.position = "bottom",
    legend.title.position = "top"
  )

dist_component_ggp_bottom


## Combine plots
ggps_prop_leg <- lapply(ggps_prop, function(x) x+theme(legend.position="right"))

combined <- wrap_plots(ggps_prop_leg, guides = "collect")


plotLayout<-"AB
#B"

combined2 <- dist_component_ggp_bottom + combined + plot_annotation(tag_levels = "a", tag_suffix = ")")+plot_layout(widths = c(1.2,3), heights=c(1,0.2), design = plotLayout)


ggsave(
  filename = here::here("figures/main/GlmFitPlots_proportion_andBarplots.pdf"),
  plot = combined2,
  width = 12,
  height = 7
)

```






```{r, out.width="0.9\\linewidth", include=TRUE, fig.align="center", fig.cap=c("Match/mis-match glm fits"), echo=FALSE}

knitr::include_graphics(here::here("figures/main/GlmFitPlots_proportion.pdf"))
```



# Narrative synthesis

*Key message*
Key changes in policy/legislation, catalysed by the right socio-political environment, can have enormous impact on publication & action.


Steps:
1. Run a baysean change point analysis to determine likely inflection points in each time series
2. Choose three case study OROs (MRE-Located, mCDR-OAE, Efficiency) and construct a qualitative narrative (discussing as well alongside international policy moments)

```{r changepoint analysis all oros, eval=FALSE}

# The oro types to analyse 
oroTypes = unique(allComponentDat_model$oro_type)


# Jags initialization parameters
jagsInits <- list(
  # Maximum number of changepoints to look for
  "K_max"=3,
  # Time buffer for looking for change points. 
  # i.e. don't find a change point at the first or last year
  "cpmin"="jagsData$MINX+1", 
  "cpmax"="jagsData$MAXX-1")

# Compile all data into a list to iterate through
dataList <- list()
for(c in selectedComponents){
  dataList[[c]] <- allComponentDat_model %>% filter(oro_type %in% oroTypes, component == c)
 
}

# Compile all model inputs into a list to iterate through
modelInputs <- list(
  "data" = dataList,
  "bugsModFiles" = list(
    "deployment" = "R/bugs-models/GammaModMultCP", # continuous non-negative
    "publications" = "R/bugs-models/PoissonModMultCP", 
    "policy" ="R/bugs-models/PoissonModMultCP", #"R/bugs-models/ZIPModMultCP", # zero-inflated poisson
    "legislation" = "R/bugs-models/PoissonModMultCP", #"R/bugs-models/ZIPModMultCP"
    "public interest" = "R/bugs-models/PoissonModMultCP",
    "public support" = "R/bugs-models/PoissonModMultCP",
    "public opposition"= "R/bugs-models/PoissonModMultCP"
  ),
  "jagsData" = list(jagsInits)[rep(1,length(components))]
)

# The different time series components to analyse (e.g. deployment, legislation, policy, posts, etc)
components <- names(modelInputs$bugsModFiles)

## Loop through all oro types and time series (components) to run change point analysis
cutpoint_results <- data.frame()
cutpoint_densities <- data.frame()

for(oro in oroTypes){
  # oro = "CCS"
  
  for(i in 1:length(components)){
    # i = 4
      
    print(paste(oro,":", components[i]))
    
    # get model to fit
    modFile <- modelInputs$bugsModFiles[[components[i]]]
    
    # If OAE deployment, y is poisson not Gamma so change
    if(components[i] == "deployment" & oro %in% c("CDR-OAE")){
      modFile <- "R/bugs-models/PoissonModMultCP"
    }
    
    # Format data
    jagsData <- modelInputs$jagsData[[i]]
    myinits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 123)
    dat <- modelInputs$data[[components[i]]][modelInputs$data[[components[i]]]$oro_type == oro,]
    if(nrow(dat) < 5){
      print("Insufficient data")
      next
    }
    yearLims <- range(modelInputs$data[[components[i]]][
      modelInputs$data[[components[i]]]$oro_type == oro,"year"
    ], na.rm=T)
    
    # Format specific inputs depending on the model
    if(grepl("pubBinom", modFile)){ 
      trialVar <- ifelse(grepl("ORO", modelInputs$y_variable[[i]]),
                         "n_OC","total_posts")
      successVar <- ifelse(grepl("ORO", modelInputs$y_variable[[i]]),
                         "n_ORO","n_posts")
      # if a proportion, only keep proportions of total values above 100 # modelInputs$y_variable[[i]] == "prop_ORO"
      keepYears <- dat$year[dat$variable_name == trialVar & 100 < dat$y]
      dat <- dat[dat$year %in% keepYears,]
      dat <- dat %>% arrange(variable_name, year)
      nTrial <- dat$y[dat$variable_name == trialVar]
      jagsData$nTrial <- nTrial
      jagsData$nSuccess <- dat$y[dat$variable_name == successVar]
      # datCast <- reshape2::dcast(dat, ... ~ variable_name, value.var = "y")
    }
    
    if(grepl("ZIP", modFile)){ 
      # if the model is zero inflated poisson, 
      # Fill missing years with 0 values
      dat <- dat %>% tidyr::complete(year = yearLims[1]:yearLims[2])
      dat$y[is.na(dat$y)] <- 0
      dat <- dat %>% tidyr::fill(component, oro_type, variable_name)
      # add the latent binomial indicator, as to whether there are no documents at all
      myinits$w <- dat$y
      myinits$w[myinits$w > 0] <- 1
      
    }
    
    if(grepl("Poisson", modFile)){ 
      # Fill missing years with 0 values
      dat <- dat %>% tidyr::complete(year = yearLims[1]:yearLims[2])
      dat$y[is.na(dat$y)] <- 0
      dat$y <- round(dat$y) # force discrete y values
      dat <- dat %>% tidyr::fill(component, oro_type, variable_name)
      
    }
    
    if(grepl("Gamma", modFile)){
      # Fill missing years with 0 values
      dat <- dat %>% tidyr::complete(year = yearLims[1]:yearLims[2])
      dat$y[is.na(dat$y)] <- 0
      dat$y[dat$y==0] <- 0.1
      dat <- dat %>% tidyr::fill(component, oro_type, variable_name)
    }
    
    # make sure no NAs
    dat <- na.omit(dat)
    
    # Skip if after subsetting data there is insufficient data points
    if(nrow(dat) < 5){
      print("Insufficient data after subsetting")
      next
    }
    #with(dat, plot(year, y))
    
    jagsData$y <- dat$y
    jagsData$x <- dat$year 
    jagsData$MINX <- min(jagsData$x)
    jagsData$MAXX <- max(jagsData$x)
    jagsData$cpmin <- eval(parse(text = jagsData$cpmin))
    jagsData$cpmax <- eval(parse(text = jagsData$cpmax))
    
    if((jagsData$cpmax-jagsData$cpmin)<5){
      jagsData$cpmin <- jagsData$MINX
      jagsData$cpmax <- jagsData$MAXX
    }
    
    # jagsData$log_y <- log(dat$y)
    # jagsData$N <- length(dat$y)
    
    ## Fit mdel
    bugs.model <- readChar(modFile, file.info(modFile)$size)
    
    jagsModel <- tryCatch(
      {
        rjags::jags.model(
          file = textConnection(bugs.model),
          data = jagsData,
          inits = myinits,
          n.chains = 5,
          n.adapt = 1500,
          quiet = FALSE
        )
        },
     error = function(e){
       return(NULL)
     }
    )
    if(is.null(jagsModel)){
      next
    }
    
    updatedOK <- tryCatch(
      {
        update(jagsModel, 1000)
        TRUE
      },
      error = function(e) {
        message("update() failed: ", e$message)
        FALSE
      }
    )
    
    if (!updatedOK) {
      message("Retrying with K_max-1")
      jagsData$K_max <- jagsData$K_max - 1
      
      jagsModel <- tryCatch(
        {
          rjags::jags.model(
            file = textConnection(bugs.model),
            data = jagsData,
            inits = myinits,
            n.chains = 5,
            n.adapt = 1500,
            quiet = FALSE
          )
        },
        error = function(e) {
          message("Re-fit failed at model creation: ", e$message)
          return(NULL)
        }
      )
      
      if (is.null(jagsModel)) {
        next
      }
      
      updatedOK <- tryCatch(
        {
          update(jagsModel, 1000)
          TRUE
        },
        error = function(e) {
          message("Re-fit update() failed again: ", e$message)
          FALSE
        }
      )
      
      if (!updatedOK) {
        next
      }
      
    }
    
    
    
    s <- tryCatch(
      {
        rjags::coda.samples(
          model = jagsModel,
          variable.names = c("alpha","beta","K","x_cp","z","pk"),
          n.iter = 1000,
          quiet = FALSE
        )
      },
      error = function(e){
        return(NULL)
      }
    )
    if(is.null(s)){
      next
    }
    
    qs <- summary(s)$quantile
    # qs
    # plot(s[,"pk[2]"]) # check mixing
    
    # Subset to only the change points that are likely 
    # And if there are likely change points, only those that indicate a positive change
    K <- round(quantile(qs["K",], 0.5))
    if(grepl("ZIP", modFile)){
      K <- sum(0.5 <= qs[grepl("pk", rownames(qs)),"50%"])
    }
    
    if(K == 0){
      print("No significant change points")
      next
    }else{
      pk <- qs[grep("pk", rownames(qs)),"50%"] # probability for each change point
      x_cp <- qs[grep("x_cp", rownames(qs)),"50%"] # locations of each change point
      # arrange in order of decreasing probability to find the most K probable
      pkIndKeep <- order(pk, decreasing=T) 
      pkIndKeep <- pkIndKeep[1:K] 
      # then re-arrange in chronological order for calculating trends of segments
      pkIndKeep <- sort(pkIndKeep) 
      
      # Find which change points mark a positive change in trend
      trends <- vector("numeric", K+1) # store the trend for each segment
      
      # starts <- c(min(jagsData$x), round(x_cp[pkIndKeep])) 
      starts <- c(min(jagsData$x), sort(round(x_cp[pkIndKeep])))
      match(starts, round(x_cp[pkIndKeep]))
      ends <- c(round(x_cp[pkIndKeep]), max(jagsData$x))
      for(k in 1:(K+1)){
        if(grepl("Binom", modFile)){
          trends[k] <- jagsData$y[which.min(abs(jagsData$x-ends[k]))] - jagsData$y[which.min(abs(jagsData$x-starts[k]))]
        }else if(grepl("ZIP|Pois|Gamma", modFile)){
          trends[k] <- sum(jagsData$y[which.min(abs(jagsData$x-starts[k])):which.min(abs(jagsData$x-ends[k]))] )
        }
        
      }
      # only keep the positive changes in trends
      matchInd <- match(starts[-1], round(x_cp[pkIndKeep]))
      pkIndKeep <- pkIndKeep[matchInd[which(0 < diff(trends))]] 
      
      if(length(pkIndKeep) == 0){
        print("no changepoints marking increasing trend")
        next
      }else{
        pkIndNames <- names(x_cp[pkIndKeep])
        
        # # If there are two cut points within 3 years of each other, 
        # # keep most probable one 
        # if(1 < length(pkIndKeep)){
        #   if(diff(qs[pkIndNames,"50%"]) < 3){
        #     pkIndNames <- pkIndNames[1]
        #   }
        # }
        
        # save probability density of the cut points for plotting
        calc_density <- function(vals){
          dens <- density(vals, n=100)
          return(data.frame("year"=dens$x, "cp_density"=dens$y))
        }
        densTemp <- do.call(rbind, s)
        densTemp <- as.matrix(densTemp[,pkIndNames], ncol=length(pkIndNames))
        densTemp <- do.call(rbind.data.frame, apply(densTemp, 2, function(x) calc_density(x)))
        densTemp$cp_id <- rep(pkIndNames, 100)[sort(rep(1:length(pkIndNames),100))]
        
        
        # Save summary of quantiles
        dfTemp <- do.call(rbind, apply(qs[pkIndNames,,drop=FALSE], 1, function(x) as.data.frame(x)))
        dfTemp$quantile = rep(colnames(qs), length(pkIndNames))
        dfTemp$cp_id <- rep(pkIndNames, 5)[sort(rep(1:length(pkIndNames),5))] #rep(1:length(pkIndNames), 5) %>% sort
        rownames(dfTemp) <- NULL
        # Cap the distribution of cutpoints at the hard limits of the data
        colnames(dfTemp)[which(colnames(dfTemp) == "x")] <- "year"
        dfTemp$year <- ifelse(dfTemp$year < yearLims[1], yearLims[1], dfTemp$year)
        dfTemp$year <- ifelse(yearLims[2] < dfTemp$year, yearLims[2], dfTemp$year)
        # Add id variables
        addIdVariables <- function(df){
          df %>% mutate(
                        oro_type = oro,
                        component = dat$component[1],
                        variable_name = dat$variable_name[1])
        }
        dfTemp <- addIdVariables(dfTemp)
        densTemp <- addIdVariables(densTemp)
        
        ## Bind data to results
        cutpoint_results <- rbind(cutpoint_results, dfTemp)
        cutpoint_densities <- rbind(cutpoint_densities, densTemp)
        # remove jags model
        rm(jagsModel)
      }
    }

      

  } # end looping through ORO types
} # end looping through bugs models



## Save results
save(cutpoint_results,cutpoint_densities, file=here::here("outputs/cutpointResults_mitigation_alloros.RData"))
```



```{r plot all time series and international policy moments, eval=FALSE}
load(here::here("outputs/cutpointResults_mitigation_alloros.RData"))

savePlot = TRUE

oro_groups <- list(
  "mCDR" = c("CDR-BioPump","CDR-BC","CDR-OAE"),
  "MRE" = c("MRE-Ocean","MRE-Located","MRE-Bio"),
  "Other"= c("CCS","Efficiency"),
  "narrative" = c("MRE-Located", "CDR-OAE","Efficiency")
)

## View summary table of cutpoints
View(cutpoint_results %>% filter(oro_type %in% oro_groups$narrative) %>% pivot_wider(id_cols = c(oro_type, cp_id, component, variable_name), names_from = quantile, values_from = year))


## Plot time series by group
ggps <- vector("list", length(oro_groups))

for(og in 1:length(oro_groups)){
  
  ggps[[og]] <- vector("list", length(oro_groups[[og]]))
  
  for(o in 1:length(oro_groups[[og]])){
    # get data
    tmpDat <- allComponentDat_model %>%
      filter(oro_type == oro_groups[[og]][o], !is.na(year), component %in% selectedComponents) %>%
      mutate(
        y = ifelse(component %in% 
                     c("deployment"), y, log(y))
      )
      
    
    actionMetric <- tmpDat$variable_name[tmpDat$component == "deployment"][1]
    facetLabels <- c("Action" = paste0("Action\n(", actionMetric, ")"),
                     "Publications" = "Publications\nlog(N articles)",
                     "Policy" = "Policy\nlog(N documents)",
                     "Legislation" = "Legislation\nlog(N documents)",
                     "Interest" = "Interest\nlog(N posts)",
                     "Support" = "Support\nlog(N positive posts + likes)"
                     # "Opposition" = "Opposition\nlog(N negative posts + likes)"
                     )
    
    tmpDat <- tmpDat %>%
      mutate(
        component_color = factor(component, levels = componentAES$level, labels = componentAES$colour),
        component = factor(component, levels = componentAES$level, labels = componentAES$label)
      ) %>%
      mutate(
        component = droplevels(component),
        component_color = droplevels(component_color)
      )%>%
      mutate(
        component = plyr::revalue(component, facetLabels)
      )
    
    # Get cutpoints
    changePointsQuantiles <- cutpoint_results %>%
      filter(oro_type == oro_groups[[og]][o], component %in% selectedComponents) %>%
      pivot_wider(names_from = quantile, values_from = year)%>%
      mutate(iqr = `97.5%`-`2.5%`)%>%
      filter(iqr < 5) %>%
      select(-iqr)%>%
      pivot_longer(cols = `2.5%`:`97.5%`, names_to = "quantile", values_to = "year") %>%
      mutate(
        component = factor(component, levels = componentAES$level, labels = componentAES$label)
      ) %>%
      mutate(
        component = plyr::revalue(component, facetLabels)
      )
    
    
    ggp_tmp <- ggplot()+
      
      # geom_vline(data = changePointsQuantiles %>%filter(quantile %in% c("2.5%","97.5%")),
      #          aes(xintercept=.data$year),
      #          linewidth = 0.5, linetype = "dashed")+
      geom_rect(data = changePointsQuantiles %>%filter(quantile %in% c("2.5%","97.5%")) %>% pivot_wider(names_from = "quantile", values_from = year),
               aes(xmin=.data$`2.5%`, xmax=.data$`97.5%`),
               ymin=-Inf, ymax=Inf,
               fill="darkgrey", alpha=0.5)+
      geom_line(data = tmpDat, aes(x=year, y=y, color=component_color))+
      scale_x_continuous(limits = year_lim)+
      scale_color_identity(guide="none")+
      facet_wrap(vars(component),ncol=1, scales = "free_y")+
      labs(
        y="",
        x="Year",
        title = typeAES$label[match(oro_groups[[og]][o], typeAES$level)]
      )+
      theme_minimal(base_size = 10)+
      theme(
        plot.margin = unit(c(0.1, 0.1,0.1,0.1), units = "cm")
      )
    ggp_tmp
    
    ggps[[og]][[o]] <- ggp_tmp
    
      
  }
  
  combined_patchwork <- wrap_plots(ggps[[og]], ncol = length(ggps[[og]])) +
    plot_annotation(tag_levels = 'a')
  
  if(savePlot){
    ggsave(here::here(paste0("figures/supplemental/allComponents_",names(oro_groups)[og],".pdf")),
         combined_patchwork, width = 3*length(ggps[[og]]), height = 7)
  }


  
}

## Plot just efficiency for method figure
eff_allComponents <- ggps[[which(names(oro_groups)=="Other")]][which(oro_groups$Other == "Efficiency")][[1]]
eff_allComponents$layers[[which(names(eff_allComponents$layers)=="geom_rect")]]<- NULL


ggsave(
  here::here("figures/supplemental/allComponents_efficiency.png"),
  width=3.5, height=6, dpi = 600,
  plot = eff_allComponents
)


## Plot with international policy moments as well
intPolMoments <- readxl::read_excel(
  here::here("data/raw-data/international_policy_moments.xlsx")
)%>%
  mutate(
    year_min = as.numeric(year_min),
    year_max = as.numeric(year_max),
    component = "International policy"
  )

tmpYlims <- c(min(intPolMoments$year_min), max(intPolMoments$year_max)+2)

ggp_intPolMom <- ggplot()+
  geom_rect(data = intPolMoments,
           aes(xmin=.data$year_min, xmax=.data$year_max),
           ymin = -Inf, ymax=Inf,
           fill="lightpink")+
  geom_vline(data = intPolMoments,
           aes(xintercept=.data$year_min),
           linewidth = 0.5, linetype = "solid", col="red")+
  geom_text(data = intPolMoments %>%
              mutate(
                y= rep(seq(0.1,0.7, length.out=3),ceiling(nrow(intPolMoments)/3))[1:nrow(intPolMoments)]
              ),
           aes(x=.data$year_min, y=.data$y, label= stringr::str_wrap(.data$label, width=10)),
           angle=30, hjust=0, col="black", size=2, nudge_x = 0.2)+
  scale_x_continuous(limits = tmpYlims)+
  scale_y_continuous(limits = c(0,1))+
  labs(
    y="",
    x="Year"
  )+
  theme_minimal(base_size = 10)+
  coord_cartesian(clip="off")+
  theme(
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    plot.margin = unit(c(0.1, 0.1,0.1,0.1), units = "cm")
  )

ggp_intPolMom



plotList <- lapply(ggps[[which(names(oro_groups)=="narrative")]],
                   function(x){
                     x+geom_vline(data = intPolMoments%>% select(year_min),
                         aes(xintercept=.data$year_min),
                         linewidth = 0.5, linetype = "solid", col="red")
                   })
plotList <- c(
  list(ggp_intPolMom),
  plotList
)

plotLayout <- "AAA
              BCD"

combined_patchwork <- wrap_plots(plotList, design = plotLayout, heights=c(0.2,1)) +
    plot_annotation(tag_levels = 'a', tag_suffix = ")")

ggsave(
  here::here("figures/main/allComponents_narrative_intPolicy.pdf"),
  width=8, height=8.5,
  plot=combined_patchwork
)

```



## Supporting plots for deeper investigation



### Increase in MRE-Ocean publications in from 2002 to 2003

This inflection was driven by a dramatic increase in publications by authors with Brazilian affiliations. Brazil experienced an energy crisis in 2001 due to droughts affecting hydroelectric grid – prompting diversification of renewable sources

```{r calculate all author affiliations for all relevant references - load in next chunk, eval=FALSE}

## connect to database
p1_db <- RSQLite::dbConnect(RSQLite::SQLite(),
                      here::here("data","sqlite-databases","product1.sqlite"),
                                 create=FALSE)

uniquerefs <- tbl(p1_db, "uniquerefs_update2025") %>%
  select(analysis_id, year, affiliation)

idCountry <- tbl(p1_db, "pred_oro_type_long") %>%
  select(analysis_id) %>%
  distinct(analysis_id) %>%
  left_join(uniquerefs %>% select(analysis_id, year, affiliation), by = "analysis_id") %>%
  collect()



# Get a list of country names and codes
load(here::here("data/derived-data/countries_ls.RData"))
# countries_ls <- data.frame(name_en = countrycode::codelist$country.name.en) %>%
#   mutate(country_iso = countrycode::countrycode(sourcevar   = name_en,
#                                        origin      = "country.name",
#                                        destination = "iso3c"),
#          country=name_en)

source(here::here("R", "extract_all_affiliation.R"))

idCountry$countries <- extract_all_affiliation(idCountry$affiliation, countries_ls)

dbWriteTable(conn=p1_db, 
             name= "oro_id_allCountryAffil_lookup",
             value=idCountry,
             append=FALSE, 
             overwrite = FALSE)


dbDisconnect(p1_db)


```


```{r plot publication timeseries -- by affiliation, eval=FALSE}

## Load data
p1_db <- RSQLite::dbConnect(RSQLite::SQLite(),
                      here::here("data","sqlite-databases","product1.sqlite"),
                                 create=FALSE)

# The author affiliations of each publication
idCountry <- tbl(p1_db, "oro_id_allCountryAffil_lookup") %>%
  select(analysis_id, countries, affiliation)%>%
  collect()

# publication year metadata
uniquerefs <- tbl(p1_db, "uniquerefs_update2025") %>%
  select(analysis_id, year) 

# The article ids and their ORO type prediction
predOroType <- tbl(p1_db, "pred_oro_type_mit_long") %>%
  left_join(uniquerefs, by = "analysis_id") %>%
  collect()%>%
  left_join(idCountry, by="analysis_id")


dbDisconnect(p1_db)



## Calculate per oro type, the frequency of country affiliation per year
# columns, analysis_id, oro_type, year, country, count
uniqueCountries <- strsplit(idCountry$countries, " ; ") %>% unlist() %>% unique()

affTypeTop10 <- strsplit(idCountry$countries, ", ") %>% unlist() %>% table %>% sort(decreasing = TRUE) %>% head(7) %>% names()


oro_year_count_long <- data.frame()

for(country in uniqueCountries){
  dfTemp <- predOroType %>%
    filter(grepl(country, countries)) %>%
    group_by(level, year) %>%
    summarise(
      count = n_distinct(analysis_id)
    ) %>%
    mutate(
      country_unique = country
    )
  oro_year_count_long <- oro_year_count_long %>%
    bind_rows(dfTemp)
}

oro_year_count_long <- oro_year_count_long %>%
  mutate(
    country = ifelse(country_unique %in% affTypeTop10, country_unique, "Other")
  )

totals <- oro_year_count_long %>%
  group_by(level, year) %>%
  summarise(
    total = sum(count, na.rm=T)
  )
oro_year_count_long <- oro_year_count_long %>%
  left_join(totals) %>%
  mutate(
    prop = count/total,
    oro_type = factor(
      gsub("[.]","-", level),
      levels = typeAES$level,
      labels = typeAES$label
    ),
    year = as.numeric(year)
  )%>%
  filter(
    !is.na(year)
  )%>%
  filter(
    1980 <= year, year <= 2024
  )

summary(oro_year_count_long)


## Overlay a lineplot of the interannual trend in N publications for each ORO type
oroPub01 <- predOroType %>%
  mutate(
    level = gsub("[.]","-", level),
    year= as.numeric(year)
    ) %>%
  filter(!is.na(year)) %>%
  filter(
    1980 <= year, year <= 2024
  )%>%
  mutate(
    lower_prediction = mean_prediction - std_prediction,
    upper_prediction = mean_prediction + std_prediction
  ) %>%
  group_by(level, year) %>%
  summarise(
    y_mean = n_distinct(analysis_id[0.5 <= mean_prediction]),
    y_lower = n_distinct(analysis_id[0.5 <= lower_prediction]),
    y_upper = n_distinct(analysis_id[0.5 <= upper_prediction])
  ) %>%
  group_by(level) %>%
  mutate(
    log_y_mean_01 = scales::rescale(log(y_mean), to = c(0,1), na.rm=T),
  ) %>%
  ungroup()%>%
  mutate(
    oro_type = factor(
      level,
      levels = typeAES$level,
      labels = typeAES$label
    )
  )



## Plot 1
# A stacked bar plot with year on x axis, proportion of publications by author country on the y
# overlain with the interannual trend in N publications 
# faceted by ORO type

# This shows that the inflection in N publications at ~2000 is due to MRE-Ocean

countryProp_ggp <- ggplot(
  data = oro_year_count_long,
  aes(x=year)
)+
  geom_col(position = "stack", aes(y=prop, fill = country))+
  geom_line(data = oroPub01,
            aes(y=log_y_mean_01), col="#00f3ff")+
  facet_wrap(vars(oro_type), ncol = 2, scales="free_x")+
  scale_x_continuous(limits = c(1980,2024))+
  scale_fill_brewer(type = "qual")+
  guides(fill = guide_legend(nrow=3))+
  labs(
    x="Year",
    y = "Proportion of total publications",
    fill = "Country"
  )+
  theme_minimal()+
  theme(
    legend.position = "bottom"
  )

countryProp_ggp


ggsave(
  here::here("figures/supplemental/propCountAffilByOroYear.pdf"),
  plot = countryProp_ggp,
  width = 6, height=7
)



## Plot 2: 
## zoom in on MRE-Ocean
mreO_countryProp_ggp <- ggplot(
  data = oro_year_count_long %>%
    filter(oro_type == "MRE-Ocean"),
  aes(x=year)
)+
  geom_col(position = "stack", aes(y=prop, fill = country))+
  geom_line(data = oroPub01%>%
    filter(oro_type == "MRE-Ocean"),
            aes(y=log_y_mean_01), col="#00f3ff")+
  scale_x_continuous(limits = c(1980,2024))+
  guides(fill = guide_legend(nrow=3))+
  scale_fill_brewer(type = "qual")+
  labs(
    x="Year",
    y = "Proportion of total publications",
    fill = "Country"
  )+
  theme_minimal(base_size = 10)+
  theme(
    legend.position = "bottom"
  )

mreO_countryProp_ggp


## Which countries in the 'Other' category are causing this increase?

oro_year_count_long %>%
    filter(oro_type == "MRE-Ocean", year %in% c(2002,2003)) %>% 
  pivot_wider(names_from = year, values_from = count, id_cols = c(country_unique))%>% 
  replace_na(list(`2002`=0, `2003`=0))%>%
  mutate(difference = (`2003`-`2002`)) %>%
  arrange(desc(difference)) %>%
  filter(!is.infinite(difference))%>%
  select(country_unique, `2002`, `2003`, difference) %>%
  filter(!(country_unique %in% affTypeTop10))%>%
  head()

#   country_unique `2002` `2003` difference
#   <chr>           <int>  <int>      <int>
# 1 Brazil              0     82         82
# 2 France              1      7          6
# 3 Canada              3      9          6
# 4 Italy               0      5          5
# 5 Norway              3      8          5
# 6 Spain               2      5          3

# looks like brazil went from 0 to 82 publications...

## Plot 3: Time series of Brazil publications
brazilPub_ggp <- oro_year_count_long %>%
    filter(oro_type == "MRE-Ocean", country_unique == "Brazil")%>%
  ggplot(aes(year, log(count)))+
  geom_line()+
  scale_y_continuous(
    name = "log(N MRE-Ocean publications)",
    sec.axis = sec_axis( transform=~exp(.), name="N publications")
  ) +
  labs(
    x = "Year"
    # y = "Brazil affiliated publications (N)"
  )+
  theme_minimal(base_size = 10)
brazilPub_ggp


ggarrange(
  plotlist = list(mreO_countryProp_ggp, brazilPub_ggp),
  labels = paste0(letters[1:2], ")"), 
  label.x = 0, vjust =1.2, font.label = list(size = 10),
  
  align = "h"
)  %>%
  ggpubr::ggexport(filename = here::here("figures/supplemental/MRE_brazil_drivers.pdf"),
                   nrow = 1, 
           width = 7, height= 4)



```



```{r, out.width="0.9\\linewidth", include=TRUE, fig.align="center", fig.cap=c("Brazil publications on MRE-Ocean contribute to inflection point in N publications"), echo=FALSE}

knitr::include_graphics(here::here("figures/supplemental/MRE_brazil_drivers.pdf"))
```


### Drivers of deployment break points



For CCS, Efficiency, CDR-OAE and MRE-Ocean, looks like change points in deployment time series -- what could have caused these increases? 

To explore this, fit a Bayesian multiple change point model (adapted from Cahill et al. 2015, doi: 10.1088/1748-9326/10/8/084002). This model estimates the number of probable change points, and then for each change point the posterior distribution. 

Use the following response variable distributions depending on response variable: 
CDR-OAE - N field trials/start ups - count data - poisson 
MRE-Ocean - Installed capacity (MW) - non-negative continuous - Gamma
Efficiency - Shipping carbon efficiency - non-negative continuous - Gamma
CCS - Storage capacity (Mt) - non-negative continuous - Gamma



Look for external drivers of deployment change points
```{r drivers of deployment change points, eval=FALSE}
# Load cut point results
load(here::here("outputs/cutpointResults_mitigation_alloros.RData"))

oroTypes <- c("CCS","Efficiency","MRE-Ocean","CDR-OAE") 


# Get a summary table of the quantiles of each cut point
deplChangePoints <- cutpoint_results %>%
    filter(component == "deployment", oro_type %in% oroTypes)%>%
  pivot_wider(names_from = "quantile", values_from = "year")


# Write table to csv
write.csv(deplChangePoints, file = here::here("outputs/cutpointSummaryTable_mitigation_deployment.csv"))



# investigate raw data for these years --------------

## MRE-Ocean -----------------
MREOcean_changeYears <- readxl::read_excel(here::here("data/raw-data/external/IRENA-electricity-statistics-by-country-year-2024.xlsx"),
                         sheet = "Country") %>% 
  mutate(year = as.numeric(Year), oro_type = "MRE-Ocean")%>%
  filter(
    Technology %in% c("Marine energy"),
    !is.na(`Electricity Installed Capacity (MW)`),
    deplChangePoints$`25%`[deplChangePoints$oro_type == "MRE-Ocean"]-5 <= year &
      year <= deplChangePoints$`75%`[deplChangePoints$oro_type == "MRE-Ocean"]+5
  ) 


topDiff <- MREOcean_changeYears %>%
  arrange(Country, year) %>%
  group_by(Region) %>%
  mutate(difference = `Electricity Installed Capacity (MW)` - first(`Electricity Installed Capacity (MW)`)) %>% 
  filter(0 < difference)%>%
  select(
    Region, Country, year, `Electricity Installed Capacity (MW)`, difference
  ) %>%
  arrange(desc(difference))
topDiff %>% head()

mreOceanTime_ggp<- ggplot(MREOcean_changeYears %>%
         group_by(Region, year) %>%
         summarise(y = sum(`Electricity Installed Capacity (MW)`, na.rm=T)),
       aes(as.integer(year), y, col = Region))+
  # geom_rect(
  #   xmin = deplChangePoints$`25%`[deplChangePoints$oro_type == "MRE-Ocean"],
  #   xmax = deplChangePoints$`75%`[deplChangePoints$oro_type == "MRE-Ocean"],
  #   ymin = -Inf, ymax = Inf,
  #   fill = "lightgrey", alpha = 0.5, col="transparent"
  # )+
  # geom_vline(xintercept = deplChangePoints$`50%`[deplChangePoints$oro_type == "MRE-Ocean"], col="red")+
  geom_line()+
  scale_x_continuous(breaks = scales::pretty_breaks())+
  geom_text(data = data.frame(
    year = topDiff$year[1]-3,
    y= 150,
    label = stringr::str_wrap("Sihwa Lake Tidal Power Station (255 MW), Republic of Korea", width = 20)
  ), aes(year,y,label = label),size=3, inherit.aes = FALSE, hjust=0.5)+
  labs(
    x="Year",
    y = "Electricity Installed Capacity (MW)",
    col = "Region"
  )+
  theme_minimal()+
  theme(
    legend.position = "right"
  )
mreOceanTime_ggp

# The jump in Asia was due to Republic of Korea -- Sihwa Lake Tidal Power Station 255 MW


## CCS -------------------------------
ccs_changeYears <- readxl::read_excel(here::here("data/raw-data/external/CRSC_CYCLE_4_FINAL_2024_160724.xlsx")) %>%
  filter(grepl("shore|sea", area, ignore.case=T), !is.na(year_of_publication)) %>%
  filter(project_spec == "YES") %>%
  mutate(
    year = as.numeric(year_of_publication),
    `Storage capacity (Mt)` = rowSums(select(., sum_low, sum_mid, sum_high), na.rm = TRUE)
    ) %>%
  filter(
    deplChangePoints$`25%`[deplChangePoints$oro_type == "CCS"]-5 <= year &
      year <= deplChangePoints$`75%`[deplChangePoints$oro_type == "CCS"]+5
  )

summary(ccs_changeYears)



ccsStorTime_ggp <- ggplot(data=ccs_changeYears %>%
         group_by(country, year) %>%
         summarise(y = sum(`Storage capacity (Mt)`, na.rm=T)))+
  # geom_vline(xintercept = deplChangePoints$`50%`[deplChangePoints$oro_type == "CCS"], col="red")+
  # geom_rect(
  #   xmin = deplChangePoints$`25%`[deplChangePoints$oro_type == "CCS"],
  #   xmax = deplChangePoints$`75%`[deplChangePoints$oro_type == "CCS"],
  #   ymin = -Inf, ymax = Inf,
  #   fill = "lightgrey", alpha = 0.5, col="grey"
  # )+
  geom_line(aes(year, y, col = country))+
  # geom_line(data=ccs_changeYears %>%
  #        group_by(region, year) %>%
  #        summarise(y = sum(`Storage capacity (Mt)`, na.rm=T)),
  #      aes(year, y, col = region))+
  geom_point(aes(year, y, col = country), size=3)+
  scale_color_brewer(type="qual")+
  guides(color=guide_legend(nrow=2))+
  labs(
    x="Year",
    y = "Storage capacity (Mt)",
    col = "Country"
  )+
  theme_minimal()+
  theme(
    legend.position = "bottom"
  )
ccsStorTime_ggp

norwayCCS <- ccs_changeYears %>%
  ungroup()%>%
  filter(country == "Norway", year == 2014) %>%
  as.data.frame()

library(ggOceanMaps)
norCCS2014 <- qmap(norwayCCS, size=`Storage capacity (Mt)`,bathymetry=TRUE)





## Efficiency ----------------------------

eff_changeYears <- readxl::read_excel(
  here::here("data/raw-data/external/IEA-shipping-energy-efficiency.xlsx"), 
  sheet = "Data") %>%
  mutate(
    y= 1/carbon_intensity_gCO2_per_tkm,
    year = as.numeric(format(as.Date(paste0(year,"-01-01")), "%Y")),
    variable_name = "Shipping carbon efficiency (tkm/gCO2)",
    component = "deployment",
    oro_type = "Efficiency"
    ) %>%
  filter(
    deplChangePoints$`25%`[deplChangePoints$oro_type == "Efficiency"]-5 <= year &
      year <= deplChangePoints$`75%`[deplChangePoints$oro_type == "Efficiency"]+5
  )

IMOLab <- data.frame(
    year = 2011,
    y= mean(eff_changeYears$y, na.rm=T),
    label = stringr::str_wrap("IMO Resolution MEPC.203(62) (2011)", width = 15))

effTime_ggp<- ggplot(eff_changeYears,
       aes(year, y))+
  geom_line()+
  geom_text(data = IMOLab, aes(year,y,label = label), size=3, inherit.aes = FALSE, hjust=1, nudge_x=-0.5)+
  geom_vline(xintercept = 2011, col="red")+
  labs(
    x="Year",
    y = "Shipping carbon efficiency (tkm/gCO2)"
  )+
  theme_minimal()+
  theme(
    legend.position = "none"
  )
effTime_ggp


## CDR-OAE ---------------------------------------
# Ocean visions field trials 
fieldTrials <- readxl::read_excel(
  here::here("data/raw-data/external/Ocean-visions-mCDR-field-trial-database.xlsx"),
  sheet = "Data") %>%
  mutate(
    `Start of Pilot` = replace(`Start of Pilot`, `Start of Pilot` %in% c("5.12.2023","2023"), as.numeric(as.Date("2023-01-01"))),
    trialID = row_number()
  ) %>%
  mutate(year = as.Date(as.numeric(`Start of Pilot`), origin = "1899-12-30")) %>%
  mutate(year = ifelse(year == as.Date("1952-12-30"), as.Date("2023-01-01"), year)) %>%
  mutate(year = as.Date(year, origin = as.Date("1970-01-01"))) %>%
  separate_rows(`All CDR Methods`, sep=",") %>% 
  mutate(
    oro_type = case_when(
      grepl("OAE|Alkalinity|Weathering", `All CDR Methods`) ~ "CDR-OAE",
      grepl("Upwelling", `All CDR Methods`) ~ "CDR-BioPump",
      TRUE ~ "Other"
    ),
    year = as.numeric(format(year, "%Y")),
    source = "Field trial"
  ) %>%
  filter(!is.na(year) & oro_type != "Other") 


# Founding year of mcdr startups
# Need to de-duplicate and add year  

GESAMP_companies <- readxl::read_excel(
  here::here("data/raw-data/external/GESAMP_wg41_ocean_climate_intervention_projects_31_may_2024.xlsx"), sheet="AllTables") %>%
  select(Company, Type, Website) %>%
  mutate(
    Company = trimws(gsub("[[:punct:]]", "", Company)),
    source = "GESAMP WG 41"
  )%>%
  filter(!is.na(Company))

# Load in OceanNET companies and de-deuplicate from GESAMP companies
OceanNET_companies <- read.csv(
  here::here("data/raw-data/external/OceanNETs_D18_oceanbased_CDR_companies/dataset/D1_8_database_oceanbased_companies.csv")) %>%
  mutate(
    Company = trimws(gsub("[[:punct:]]", "", Company)),
    source = "OceanNETs (2020) D1.8 database"
  )%>%
  filter(!is.na(Company), !is.na(Lat))%>%
  select(-c(contains("X"))) %>%
  filter(!(tolower(Company) %in% tolower(GESAMP_companies$Company)), Company != "Qilibrium")

## Join and write to csv file so I can look up the years on linkedIN
# # Join
# mCDRCompanies <- GESAMP_companies %>%
#   bind_rows(OceanNET_companies)
# 
# # write to file
# write.csv(mCDRCompanies, file = here::here("data/derived-data/mCDR-companies-dedup.csv"))
# 


## read in the years
mCDRCompanies <- readxl::read_excel(here::here("data/raw-data/mCDR-companies-linkedin.xlsx")) %>%
  filter(!is.na(`Company founded on (linkedin)`)) %>%
  rename(year = `Company founded on (linkedin)`) %>%
  select(Company, year) %>%
  left_join(GESAMP_companies %>% select(Company, Type), by="Company") %>%
  left_join(OceanNET_companies %>% select(Company, Type), by="Company") %>%
  mutate(
    Type = ifelse(is.na(Type.x), Type.y, Type.x)
  ) %>%
  select(Company, Type, year) %>%
  mutate(
    oro_type = case_when(
      grepl("Upwell|fertilization", Type, ignore.case=TRUE) ~ "CDR-BioPump",
      # grepl("Biomass sinking|Farming|Harvesting|Aquaculture", Type, ignore.case=TRUE) ~ "CDR-Cult",
      grepl("OAE|Alkalinity|weathering", Type) ~ "CDR-OAE",
      TRUE~"CDR-Other"
    ),
    source = "Startup",
    year = as.numeric(year)
  ) %>%
  filter(oro_type != "CDR-Other")
  
# Ocean visions community and www.cdr.fyi/leaderboards can't be scraped because they use private APIs
# Although the latter a list of suppliers can be found here without metadata
# https://www.cdr.fyi/api/search




## Combine data sources



oae_changeYears <- fieldTrials %>%
  bind_rows(mCDRCompanies) %>%
  filter(
    oro_type == "CDR-OAE",
    2020 <= year &
      year <= 2022
  )

# A combinationo f startups and field trials  
oae_changeYears %>%
  group_by(
    year, source
  )%>%
  summarise(
    n = n()
  )

# Whos conducting the field trials in 2022?
unique(oae_changeYears$`Leading Organization`[oae_changeYears$year == 2022]) # "Vesta"   "GEOMAR"  "Limenet"
fieldtrials_2022 <- unique(oae_changeYears$`Leading Organization`[oae_changeYears$year == 2022])
fieldtrials_2022 <- fieldtrials_2022[!is.na(fieldtrials_2022)]
fieldtrials_2022 <- c("Field trials (2022):", fieldtrials_2022)
fieldtrials_2022 <- paste(fieldtrials_2022, collapse = "\n")

# Who are the startups in 2021?
unique(oae_changeYears$Company[oae_changeYears$year == 2021])
startups_2021 <- unique(oae_changeYears$Company[oae_changeYears$year == 2021])
startups_2021 <- startups_2021[!is.na(startups_2021)]
startups_2021 <- c("Start ups (2021):", startups_2021)
startups_2021 <- paste(startups_2021, collapse = "\n")


# [1] NA                               "Ebb Carbon"                     "Cequest"                       
# [4] "The Charles Darwin Rescue Plan" "Skyology"     


oaeTime_ggp<- ggplot(fieldTrials %>%
                      bind_rows(mCDRCompanies) %>%
                      filter(
                        oro_type == "CDR-OAE",
                        2018 <= year &
                          year <= 2024
                      ) %>%
                      group_by(
                        year, source
                      )%>%
                      summarise(
                        n = n()
                      ),
       aes(year, n, fill = source))+
  geom_col()+
  scale_fill_brewer(
    name = "Data\nsource",
    breaks = c("Field trial","Startup"),
    palette = "Dark2"
  )+ 
  geom_text(
    data = data.frame(
      year = 2022, n=10, label = startups_2021
    ),
    aes(year, n, label = label), size=2.7, inherit.aes = FALSE, hjust=1, nudge_x=-0.5, check_overlap = TRUE)+
  geom_text(x=2021.5, y=13, label = fieldtrials_2022, size=2.7, inherit.aes = FALSE, hjust=0, nudge_x=-0.5,
            check_overlap=TRUE)+
  scale_x_continuous(breaks = 2018:2024)+
  geom_vline(xintercept = 2020.5, col="red")+
  labs(
    x="Year",
    y = "N Field trials/Start ups"
  )+
  theme_minimal()+
  coord_cartesian(clip="off")+
  theme(
    legend.position = "right"
  )
oaeTime_ggp



## Combine plots
plotList = list(
    brazilPub_ggp,
    mreOceanTime_ggp+theme(plot.margin = unit(c(1.2,0,0,0.5),"cm")), 
                  ccsStorTime_ggp+theme(plot.margin = unit(c(1.2,0,0,0.5),"cm")), 
                  norCCS2014+theme(plot.margin = unit(c(1.2,0,0,0.5),"cm")), 
                  effTime_ggp+theme(plot.margin = unit(c(1.2,0,0,0.5),"cm")),
                  oaeTime_ggp+theme(plot.margin = unit(c(1.2,0,0,0.5),"cm"))
                  )


p_all <- wrap_plots(plotList, ncol = 2, widths = c(1,1)) + 
  theme(plot.tag.position = c(0, 1),  # tags top-left
        plot.tag = element_text(size = 10))+ 
  plot_annotation(
  tag_levels = "a",
  tag_prefix = "",
  tag_suffix = ")"
)

# Save
ggsave(
  here::here("figures/supplemental/allChangePointPlots.pdf"),
  p_all,
  width = 10, height = 12
)

```

```{r, out.width="0.9\\linewidth", include=TRUE, fig.align="center", fig.cap=c("Qualitative analysis of change points"), echo=FALSE}

knitr::include_graphics(here::here("figures/supplemental/allChangePointPlots.pdf"))
```



