---
title: "3.0_analyses-and-figures"
author: "Devi Veytia"
date: "2025-07-08"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
```




```{r load libraries, results = 'hide'}

# general data handing
library(dplyr)
library(dbplyr)
library(RSQLite)
library(R.utils)
library(ggplot2)
library(ggalluvial)
library(tidyr)
library(stringr)
library(viridis)
library(countrycode)
library(broom)
library(conflicted)
library(tidyverse)
library(cowplot)
library(ggpubr)
library(patchwork)
library(igraph)
library(ggraph)
library(tidygraph)
library(ggplot2)
require(VLTimeCausality) # remotes::install_github("DarkEyes/VLTimeSeriesCausality")
library(QPress) # remotes::install_github("SWotherspoon/QPress")
library(reshape2)
library(ggeffects)
library(sjPlot)
library(magick)
library(grid)


conflict_prefer("select", "dplyr")
conflicts_prefer(dplyr::filter)



## AESTHETICS
factor_aes <- readxl::read_excel(here::here("R/mitigation_factor_aesthetics2.xlsx"))
typeAES <- factor_aes[which(factor_aes$variable == "oro_type"),]
typeAES <- typeAES[order(typeAES$order),]

componentAES <- factor_aes[which(factor_aes$variable == "component"),]
componentAES <- componentAES[order(componentAES$order),]
# actually make all components grey
componentAES$colour <- paste("grey20")

## Set seed
addTaskCallback(function(...) {set.seed(123);TRUE})

```


# Load data

Data structure:

each data frame has the following id variables: oro_type, component (publication, policy, deployment, etc), variable_name,
each data frame has the following response variable: y

```{r load data}

# Load data for specific OROs
load(here::here("data", "derived-data", "mitigationORO_pubs.RData")) #pubs
load(here::here("data", "derived-data", "n_nonBindPolicy_docs.RData")) # legDat
load(here::here("data", "derived-data", "n_legislation_docs.RData")) # polDat
load(here::here("data/derived-data/mitigationDeployDat.RData")) # allDeployDat
load(here::here("data/derived-data/mitigationPostsDat.RData")) # postsDat




year_lim <- c(2000, 2024) # years to analyse

# components to analyse
selectedComponents <- c("publications", "deployment","policy","legislation","public interest","public support") 

# bind all data together
allComponentDat_model <- pubs %>%
  bind_rows(polDat) %>%
  bind_rows(legDat) %>%
  bind_rows(allDeployDat) %>%
  bind_rows(postsDat) %>%
  mutate(
    component = replace(component, component == "non-binding policy", "policy"),
    # y = scale(y, center=TRUE, scale=TRUE)
  ) %>%
  filter(year_lim[1] <= year & year <= year_lim[2],
         oro_type %in% typeAES$level) %>%
  filter(
    component %in% selectedComponents
  )




```


# Entry point: The distribution of and extent of scientific evidence, and its temporal variation, varies according to the type of ocean-related option (ORO) 

*Key message*
Mitigation ORO publication effort is weighted towards marine renewable energy (ocean, located). Over time, we observe varying rates of increase, with inflection points – notably in MRE-Ocean occurring in 2001. This inspired this analysis: What are the drivers of changes in publication? And how do these relate to outcomes in action? 


*To contextualize our publication data, we have added the following metrics:*

Number of policy and legislative documents - Calculated by web scraping ECOLEX & FAOLEX databases then keyword searching full document pdfs for keywords relevant for each type of ORO. Document type metadata used to determine legislation vs policy

Interest - N posts returned from keyword searches (reddit, youtube, bluesky, linked in), weighted by number of search queries

Support - N posts (from above) that are positive sentiment (predicted using a pre-trained sentiment classification LLM) weighted by # of likes

Action - various metrics (see below)


*Note that chunks save figures and output files, so run line by line rather than whole chunk to avoid over-writing*

```{r plot publication timeseries for all the OROs, eval = FALSE}

## Get data
p1_db <- RSQLite::dbConnect(RSQLite::SQLite(),
                      here::here("data","sqlite-databases","product1.sqlite"),
                                 create=FALSE)

uniquerefs <- tbl(p1_db, "uniquerefs_update2025") %>%
  select(analysis_id, year) 

predOroType <- tbl(p1_db, "pred_oro_type_mit_long") %>%
  left_join(uniquerefs, by = "analysis_id") %>%
  collect()

dbDisconnect(p1_db)

## total number of articles relevant
length(unique(predOroType$analysis_id)) # 47830
# number of unclassified ORO articles
58952-length(unique(predOroType$analysis_id)) #  11122


## Sumamrise and format to count number of articles/ORO/year
oroPub <- predOroType %>%
  mutate(
    std_prediction = ifelse(is.na(std_prediction), 0, std_prediction),
    level = gsub("[.]","-", level),
    year= as.numeric(year)
    ) %>%
  filter(!is.na(year)) %>%
  mutate(
    lower_prediction = mean_prediction - std_prediction,
    upper_prediction = mean_prediction + std_prediction
  ) %>%
  group_by(level, year) %>%
  summarise(
    y_mean = n_distinct(analysis_id[0.5 <= mean_prediction]),
    y_lower = n_distinct(analysis_id[0.5 <= lower_prediction]),
    y_upper = n_distinct(analysis_id[0.5 <= upper_prediction])
  ) 

# Calculate for all mitigation OROs
allPub <- predOroType %>%
  mutate(
    std_prediction = ifelse(is.na(std_prediction), 0, std_prediction),
    level = gsub("[.]","-", level),
    year= as.numeric(year)
    ) %>%
  filter(!is.na(year)) %>%
  mutate(
    lower_prediction = mean_prediction - std_prediction,
    upper_prediction = mean_prediction + std_prediction
  ) %>%
  group_by(year) %>%
  summarise(
    y_mean = n_distinct(analysis_id[0.5 <= mean_prediction]),
    y_lower = n_distinct(analysis_id[0.5 <= lower_prediction]),
    y_upper = n_distinct(analysis_id[0.5 <= upper_prediction])
  )%>%
  mutate(level = paste("All mitigation OROs"))

# Bind all mitigation OROs with individual ORO counts
allMit <- oroPub %>%
  bind_rows(allPub) %>%
    mutate(
    level = factor(
      level,
      levels = c("All mitigation OROs", typeAES$level),
      labels = c("All mitigation OROs", typeAES$label)
    )
  )



## Plot
oroTimeseries_ggp<- ggplot(allMit%>% filter(1980 <= year, year <= 2024), aes(x=year))+
  geom_rect(xmin = 2001, xmax = 2005, ymin=-Inf, ymax=Inf, alpha = 0.1, fill="grey")+
  geom_line(aes(y=log(y_mean), col=level))+
  geom_ribbon(aes(ymin = log(y_lower), ymax = log(y_upper), fill = level), alpha = 0.5)+
  facet_wrap(vars(level), scales = "free_y")+
  scale_color_manual(
    breaks = c("All mitigation OROs", typeAES$label),
    values = c("#35a7d9", typeAES$colour),
    labels = c("All mitigation OROs", typeAES$label)
  )+
  scale_fill_manual(
    breaks = c("All mitigation OROs", typeAES$label),
    values = c("#35a7d9", typeAES$colour),
    labels = c("All mitigation OROs", typeAES$label)
  )+
  scale_y_continuous()+
  scale_x_continuous(limits = c(1980,2024))+
  # guides(col=guide_legend(nrow=2))+
  labs(x="Year", y = "log(N publications)", col = "ORO type")+
  theme_minimal()+
  theme(
    legend.position = "none",
    axis.text.x = element_text(angle=45, hjust=1)
  )

oroTimeseries_ggp


## Save
ggsave(
  here::here("figures/main/nPublicationsTimeseriesPlots.pdf"),
  plot = oroTimeseries_ggp,
  width = 7, height=5
)

```

```{r combine publication timeseries with conceptual model figure and plot}


# Import slide of conceptual model
slide_img <- image_read_pdf("figures/main/conceptual_model/conceptual_model.pdf", density = 600)
slide_img <- image_trim(slide_img)
slide_ras <- as.raster(slide_img)
# plot(slide_ras)
slide_grob <- rasterGrob(slide_ras, interpolate = FALSE)



## plot all together
plotList <- list(oroTimeseries_ggp, 
                 slide_grob
                 )

combined <- wrap_plots(plotList, nrow=2, heights=c(1,1))+
  plot_annotation(tag_levels = "a", tag_suffix = ")")&theme(plot.tag.position = c(0, 1))


## Save
ggsave(
  here::here("figures/main/nPublicationsTimeseries_conceptualModel.pdf"),
  plot = combined,
  width = 7, height=9
)

```



Introduce complementary data sets:

```{r introduce complementary data - plot proportions by oro type, eval = FALSE}

## Publications
p1_db <- RSQLite::dbConnect(RSQLite::SQLite(),
                      here::here("data","sqlite-databases","product1.sqlite"),
                                 create=FALSE)

uniquerefs <- tbl(p1_db, "uniquerefs_update2025") %>%
  select(analysis_id, year) 

predOroTypeYear <- tbl(p1_db, "pred_oro_type_mit_long") %>%
  left_join(uniquerefs, by = "analysis_id") %>%
  filter(!is.na(year)) %>%
  group_by(level, year) %>%
  summarise(
    N = n_distinct(analysis_id[0.5 <= (mean_prediction)])
  ) %>%
  collect()%>%
  mutate(
    year = as.numeric(year),
    oro_type = gsub("[.]","-",level),
    component = "publications",
    variable_name = "Publications (N)"
  ) %>%
  select(oro_type, component, variable_name, year, N)

nPubs_oro <- tbl(p1_db, "pred_oro_type_mit_long") %>%
  group_by(level)%>%
  summarise(
    N = n_distinct(analysis_id[0.5 <= (mean_prediction)])
  )%>%
  collect()%>%
  mutate(
    oro_type = gsub("[.]","-",level),
    component = "publications",
    variable_name = "Publications (N)"
  ) %>%
  select(oro_type, component, variable_name, N)

dbDisconnect(p1_db)



## Sumarise by component overall proportional share of each ORO
prop_oro_summary_df <- allComponentDat_model %>%
  filter(!(component %in% c("deployment","publications"))) %>%
  group_by(component, variable_name, oro_type) %>%
  summarise(
    N = sum(y, na.rm=T)
  ) %>%
  bind_rows(
    nPubs_oro
  ) %>%
  group_by(component) %>%
  mutate(
    total_component = sum(N, na.rm=T)
  ) %>%
  ungroup() %>%
  mutate(
    prop_component = N/total_component,
    component = factor(component, 
                       levels = componentAES$level, 
                       labels = componentAES$label_varname)
  )%>%
  mutate(
    component = droplevels(component),
    oro_type = factor(
      oro_type, 
      levels = typeAES$level,
      labels = typeAES$label
    )
  )
  



# Plot proportion of each dimension per ORO
dist_component_ggp <- ggplot(data = prop_oro_summary_df, aes(x=component))+
  geom_col(aes(y=prop_component, fill=oro_type), position="stack")+
  geom_text(aes(label = scales::number(total_component, accuracy = 1, big.mark = ",")), check_overlap = TRUE, vjust=0, y=1)+
  scale_fill_manual(
    breaks = typeAES$label,
    values = typeAES$colour
  )+
  scale_y_continuous(expand = expansion(mult=c(0,0.1)))+
  labs(
    x="Dimension (i.e. Node)",
    y="Proportion",
    fill="ORO type"
  )+
  guides(fill=guide_legend(ncol=2))+
  theme_minimal(base_size = 10)+
  theme(
    axis.text.x = element_text(angle=45, hjust=1)
  )

dist_component_ggp


## Plot the time series as well
year_oro_summary_df <- allComponentDat_model %>%
  filter(!(component %in% c("deployment","publications"))) %>%
  group_by(component, variable_name, oro_type, year) %>%
  summarise(
    N = sum(y, na.rm=T)
  ) %>%
  bind_rows(
    predOroTypeYear
  ) %>%
  filter(
    year_lim[1] <= year, year <= year_lim[2]
  ) %>%
  group_by(component, year) %>%
  mutate(
    total_component = sum(N, na.rm=T)
  ) %>%
  ungroup() %>%
  mutate(
    prop_component = N/total_component,
    component = factor(component, 
                       levels = componentAES$level, 
                       labels = componentAES$label_varname)
  )%>%
  mutate(
    component = droplevels(component),
    oro_type = factor(
      oro_type, 
      levels = typeAES$level,
      labels = typeAES$label
    )
    # component = fct_recode(component, "Publications (log(N))"="Publications (N)")
  )%>%
  select(oro_type, component, year, prop_component, N) 
  


# Annual plot of each dimension per ORO
year_component_ggp <- ggplot(data = year_oro_summary_df, aes(x=year))+
  facet_wrap(vars(component), ncol=1, scales = "free_y")+
  geom_area(aes(y=N, fill=oro_type), position="stack")+
  scale_fill_manual(
    breaks = typeAES$label,
    values = typeAES$colour
  )+
  scale_y_continuous(expand = expansion(mult=c(0,0.1)))+
  labs(
    x="Year",
    y="N",
    fill="ORO type"
  )+
  theme_minimal(base_size = 10)+
  theme(
    legend.position = "bottom"
  )

year_component_ggp

year_component_prop_ggp <- ggplot(data = year_oro_summary_df, aes(x=year))+
  facet_wrap(vars(component), ncol=1, scales = "free_y")+
  geom_col(aes(y=prop_component, fill=oro_type), position="stack", width=1)+
  scale_fill_manual(
    breaks = typeAES$label,
    values = typeAES$colour
  )+
  labs(
    x="Year",
    y="Proportion",
    fill="ORO type"
  )+
  theme_minimal(base_size = 10)+
  theme(
    legend.position = "bottom"
  )

year_component_prop_ggp

oro_leg <- ggpubr::get_legend(dist_component_ggp)



## Plot all together

plotList <- list(
  dist_component_ggp+theme(plot.margin = unit(rep(0.1, 4),"cm")),
  year_component_ggp+
    theme(legend.position = "none",
          plot.margin = unit(rep(0.1, 4),"cm")
          ),
  year_component_prop_ggp+
    theme(legend.position = "none",
          plot.margin = unit(rep(0.1, 4),"cm")
          )
)

plotLay <- 
"#AA#
BBCC"

combined <- wrap_plots(plotList,
                       design = plotLay,
                       widths = c(1,5,0.2,0.2),
                       heights = c(1,2)
                       )+
  plot_annotation(tag_levels = "a", tag_suffix = ")")


## Save
ggsave(
  here::here("figures/main/distribution_by_component_plots.pdf"),
  width=6.5, height=7,
  plot = combined
)


```






A closer look at the ORO-specific 'action' metrics:

MRE - installed capacity (MW) - IRENA renewable electricity statistics

Efficiency - domestic freight transport energy efficiency (gCO2/tkm) - IEA

CCS - Storage Capacity associated with a project- oil and gas climate initiative

CDR-BC - # restoration projects - Duarte et al 2020

CDR-OAE/BioPump - # Field trials/startup companies - Ocean Visions Field trials, GESAMP climate intervention projects, OceanNETs ocean-based CDR companies


```{r plot all deployment time series, eval = FALSE}

allDep_ggp <- ggplot(
  data = allComponentDat_model %>%
    filter(component == "deployment") %>%
    mutate(
    oro_type = factor(oro_type, levels = typeAES$level, labels = typeAES$label)
  )
)+
  geom_line(aes(year, y, col = oro_type))+
  geom_text(aes(label = stringr::str_wrap(variable_name, width = 25),
                fontface=3), x=-Inf, y=Inf,
            check_overlap = TRUE, vjust=1, hjust=0, size=3)+
  facet_wrap(vars(oro_type), scales="free")+
  scale_colour_manual(
    breaks = typeAES$label,
    values = typeAES$colour
  )+
  labs(
    x="Year",
    y="Action metric",
  )+
  theme_minimal()+
  theme(
    legend.position = "none"
  )

allDep_ggp


## Save
# ggsave(here::here("figures/supplemental/mitigationDeploymentIndicators.pdf"), plot = allDep_ggp, width = 7, height=6)

```

```{r plot all dimension timeseries together in multipanel figure - legend on side}
## Plot only timeseries
plotList <- list(
  year_component_ggp+
    theme(legend.position = "none",
          plot.margin = unit(rep(0.1, 4),"cm")
          ),
  year_component_prop_ggp+
    theme(legend.position = "right",
          plot.margin = unit(rep(0.1, 4),"cm")
          ),
  allDep_ggp +theme(plot.margin = unit(rep(0.1, 4),"cm"))
)

plotLay <-
"ABC"

combined <- wrap_plots(plotList,
                       design = plotLay,
                       widths = c(1,1,2.5)
                       )+
  plot_annotation(tag_levels = "a", tag_suffix = ")")


## Save
ggsave(
  here::here("figures/main/distribution_by_component_alltimeseries_plots.pdf"),
  width=14, height=7,
  plot = combined
)
```


```{r plot all dimension timeseries together in multipanel figure - legend on bottom}
## Plot only timeseries
oro_leg <- ggpubr::get_legend(year_component_prop_ggp)

plotList <- list(
  year_component_ggp+
    ggtitle("a)")+
    theme(legend.position = "none",
          plot.margin = unit(rep(0.1, 4),"cm")
          ),
  year_component_prop_ggp+
    ggtitle("b)")+
    theme(legend.position = "none",
          plot.margin = unit(rep(0.1, 4),"cm")
          ),
  oro_leg,
  allDep_ggp+
    ggtitle("c)")+
    theme(plot.margin = unit(rep(0.1, 4),"cm"))
)

plotLay <-
"ABD
CCD"

combined <- wrap_plots(plotList,
                       design = plotLay,
                       widths = c(1,1,2.3),
                       heights = c(6,1)
                       )
  # plot_annotation(tag_levels = "a", tag_suffix = ")")


## Save
ggsave(
  here::here("figures/main/distribution_by_component_alltimeseries_plots.pdf"),
  width=12, height=7,
  plot = combined
)
```





# Network analysis


Method steps:

Step 1. For each ORO, determine if an edge exists between two nodes using variable lag transfer entropy
Step 2. Generalize these findings across groups of similar OROs by pooling edges into a meta-network. If an edge exists for an ORO, it is included in the meta network. Then, use the network to build a qualitative network model, and simulate press perturbations to the different nodes and record the impact on action.



## Step 1: Variable-lag transfer entropy

For each ORO, determine whether there are links between the time series of the different nodes in our conceptual model (publications, policy, legislation, public interest metrics, action). 




```{r VL-TE analysis, eval = FALSE}
## Load in results in next chunk

# Define levels to loop through

# Loop 1 -- different oros
oros <- unique(allComponentDat_model$oro_type)

# Loop 2 -- process all possible edges
# Define nodes and edges to process
nodes <- unique(allComponentDat_model$component)
edges <- expand.grid(nodes, nodes)
edges <- edges[edges[,1] != edges[,2],]

# remove edge pairs whose calculations are inter-related (all the public interest metrics)
publicEdges <- grepl("public", edges[,1]) & grepl("public", edges[,2])
edges <- edges[!publicEdges,]


# Function to make BoxCoxTransformation -- make data normal
BCTransform <- function(series){
  BCMod <- caret::BoxCoxTrans(series)
  series_trans <- predict(BCMod, series)
}

# Wrapper to try the box-cox transformation with error handling
try_BCTransform <- function(series){
  tryCatch(
    {BCTransform(series)},
    error = function(e){
      return(series)
    }
  )
}



## Loop 1 -- loop through OROs
TE_results <- data.frame()
TE_results_summary <- data.frame()
for(oro in oros){
  # oro <- "MRE-Ocean" # for testing
  
  # Data processing
  # Fillin missing values with a 0 -- so that same years are represented across time series
  # correct for heteroskedasiticy
  oroDat <- allComponentDat_model %>%
    filter(oro_type == oro) %>%
    complete(component, year=year_lim[1]:year_lim[2], 
             fill = list(y=0), explicit= FALSE) %>%
    arrange(component, year)%>%
    group_by(component) %>%
    mutate(y_trans = try_BCTransform(y)) %>%
    ungroup()
  # ggplot(oroDat, aes(x=year, y=y_trans))+geom_line()+facet_wrap(vars(component), scales="free_y")
  
  
  edgesResults <- data.frame(
    oro_type = rep(oro, nrow(edges)),
    NodeX = edges[,1],
    NodeY = edges[,2],
    TE_XCauseY = rep(NA, nrow(edges)),
    TE_pval = rep(NA, nrow(edges)),
    TE_ratio = rep(NA, nrow(edges)),
    optDelay = rep(NA, nrow(edges)),
    optCor = rep(NA, nrow(edges)),
    dtw=rep(NA, nrow(edges))
  )
  
  # Loop through the edges and test for causality
  for(e in 1:nrow(edges)){
    
    X = oroDat$y_trans[oroDat$component == edges[e,1]]
    Y = oroDat$y_trans[oroDat$component == edges[e,2]]
    if(length(X) != length(Y)){next}
    
    # remove leading or trailing zeros
    edge_dat <- data.frame(
      X = X,
      Y = Y
    )
    # trim leading and trailing zeros
    first_nonzero <- min(which(rowSums(edge_dat != 0) ==ncol(edge_dat)))
    last_nonzero  <- max(which(rowSums(edge_dat != 0) ==ncol(edge_dat)))
    edge_dat <- edge_dat[first_nonzero:last_nonzero, ]
    
    if(nrow(edge_dat) == 0){next}
    

    TE_out <- tryCatch(
      {VLTransferEntropy(
      X= edge_dat$X,
      Y= edge_dat$Y,
      maxLag = 5,
      VLflag=TRUE,nboot=100, alpha = 0.05)},
      error = function(e){
        return(NULL)
      }
    )
    
    if(is.null(TE_out)){
      edgesResults$TE_pval[e] <- NA
      edgesResults$TE_XCauseY[e] <- NA
      edgesResults$TE_ratio[e] <- NA
      edgesResults$optDelay[e] <- NA
      edgesResults$optCor[e] <- NA
      edgesResults$dtw[e] <- NA
    }else{
      
      ## Save outputs into data frame
      edgesResults$TE_pval[e] <- TE_out$pval
      # TS$X causes TS$Y TRUE/FALSE
      edgesResults$TE_XCauseY[e] <- TE_out$XgCsY_trns 
      # Transfer entropy ratio -- If it is greater than one , then X causes Y.
      edgesResults$TE_ratio[e] <- TE_out$TEratio
      # optimal time delay inferred by cross-correlation of X,Y. It is positive if Y is simply just a time-shift of X (e.g. Y[t]=X[t-optDelay]).
      edgesResults$optDelay[e] <- TE_out$follOut$optDelay 
      # time series of optimal warping-path from DTW that is corrected by cross correlation. It is approximately that Y[t]=X[t-optIndexVec[t]])
      edgesResults$dtw[e] <- mean(TE_out$follOut$optIndexVec[,1], na.rm=T)
      # optimal correlation of Y[t]=X[t-optDelay] for all t
      # Maybe if optCor is negative, that can give me the sign 
      edgesResults$optCor[e] <- TE_out$follOut$optCor
    }
    
  }
  
  # summarize results to most causal direction if a two-way effect found
  # ie. When TRUE for both directions, resolve by picking the direction with the highest TE ratio
  edgesResultsSummary <- data.frame()
  uniqueEdges <- edges[!apply(edges, 1, is.unsorted), ]
  for(ue in 1:nrow(uniqueEdges)){
    
    incl = vector(length = nrow(edgesResults))
    for(i in 1:nrow(edgesResults)){
      tmp <- c(edgesResults[i,c("NodeX","NodeY")]) %in% uniqueEdges[ue,]
      incl[i] <- sum(tmp)==2
    }
    tmpDat <- edgesResults[incl,] %>%
      filter(TE_XCauseY) %>%
      arrange(desc(TE_ratio)) %>%
      slice_head(n=1)
    
    edgesResultsSummary <- edgesResultsSummary %>%
      bind_rows(tmpDat)
  }
  
    
  # Bind results
  TE_results <- TE_results %>%
    bind_rows(edgesResults)
  TE_results_summary <- TE_results_summary %>%
    bind_rows(edgesResultsSummary)
  
}

# clean
# rm(oroDat, nodes, edges, TE_out, edgesResults)




save(TE_results, TE_results_summary, file = here::here("data/derived-data/TE_results.RData"))
```



```{r load TE results}

load(here::here("data/derived-data/TE_results.RData")) # TE_results, TE_results_summary

```


```{r plot of VLTE time series transform for method figure}
oro = "Efficiency" #"MRE-Located"
varx= "policy" # publications
vary = "deployment"

oroDat <- allComponentDat_model %>%
    filter(oro_type == oro) %>%
    complete(component, year=year_lim[1]:year_lim[2], 
             fill = list(y=0), explicit= FALSE) %>%
    group_by(component) %>%
    arrange(component, year)%>%
    mutate(y_trans = try_BCTransform(y)) %>%
    ungroup()

year = oroDat$year[oroDat$component == varx]
X = oroDat$y_trans[oroDat$component == varx] # cause
Y = oroDat$y_trans[oroDat$component == vary] # effect



# remove leading or trailing zeros
edge_dat <- data.frame(
  X = X,
  Y = Y,
  year=year
)
# trim leading and trailing zeros


first_nonzero <- min(which(rowSums(edge_dat != 0) ==ncol(edge_dat)))
last_nonzero  <- max(which(rowSums(edge_dat != 0) ==ncol(edge_dat)))
edge_dat <- edge_dat[first_nonzero:last_nonzero, ]


# year = oroDat$year[oroDat$component == "publications"][1:24]
# X = oroDat$y_trans[oroDat$component == "publications"][1:24] # cause
# Y = oroDat$y_trans[oroDat$component == "deployment"][1:24] # effect

plot(edge_dat$Y)
plot(edge_dat$X)
  
TE_out <- tryCatch(
  {VLTransferEntropy(
  Y = edge_dat$Y, 
  X = edge_dat$X, 
  maxLag = 5,
  VLflag=TRUE,nboot=100, alpha = 0.05)},
  error = function(e){
    return(NULL)
  }
)

TE_out

vlte_df <- rbind(
  data.frame(
    series = paste("X"),
    x = edge_dat$year,
    y=edge_dat$X,
    dtw_lag = NA
  ),
  data.frame(
    series = paste("Y"),
    x = edge_dat$year,
    y = edge_dat$Y,
    dtw_lag = NA
  ),
  data.frame(
    series = paste("X (modified lag to match Y)"),
    x = edge_dat$year,
    y = edge_dat$X[1:length(edge_dat$X)-TE_out$follOut$optIndexVec[,1]],
    dtw_lag = -TE_out$follOut$optIndexVec[,1]
  )
) %>%
  mutate(
    series = factor(series,
                    levels=c(
                      "X","Y","X (modified lag to match Y)"
                    )
                    # labels = c(
                    #   "X (Policy)",
                    #   "Y (Action)",
                    #   "X (modified lag to match Y)"
                    # )
                    )
  )%>%
  arrange(series, y)

vlte_ggp <- ggplot(data=vlte_df, aes(x=x, y=y, color = series))+
  geom_line()+
  geom_text(aes(label = dtw_lag), vjust=0)+
  # scale_color_brewer(type="qual", palette = "Dark2")+
  scale_color_manual(
    values = componentAES$colour[
      c(
        which(componentAES$level == varx),
        which(componentAES$level == vary),
        which(componentAES$level == varx)
      )
    ]
  )+
  facet_wrap(vars(series),ncol=1, scales = "free_y")+
  labs(
    x="Year", y="Value"
  )+
  theme_minimal(base_size=10)+
  theme(
    legend.position = "none"
  )

vlte_ggp

  
ggsave(
  here::here("figures/supplemental/VLTE_demonstration_plot.pdf"),
  width = 4, height=3,
  plot = vlte_ggp
)

ggsave(
  here::here("figures/supplemental/VLTE_demonstration_plot.png"),
  width = 4, height=3,
  dpi=600,
  plot = vlte_ggp
)
```


## Step 2: Create Network by ORO group from TE edges

Aggregate Causal edges (identified from VL transfer entropy) Across oro types

Aggregate the TE results across similar oro_types to build a meta-causal graph:

For each unique edge, compute:

* N significant: how many oro_types had significant causality. This indicates agreement/consistency
* Mean TE_ratio: average causal strength across significant cases.
* sign (direction of edge effect): from optimal correlation sign
* Optimal lag (not the same as all the lags for each time point from dynamic time warping, but the optimal lag over the whole time series)


Visualize the aggregated meta-network where:

* Edge transparency = mean TE ratio.
* Edge color = sign of correlation

```{r group oro networks conceptually and plot combined networks - simplified for clearer arrows}
require(ggraph)
require(tidygraph)



## Get diagraph objects for each defined cluster
# Create a data.frame for lookup
concept_cluster_df <- data.frame(
  oro_type = c(
    "CCS",
    "CDR-BC","CDR-BioPump","CDR-OAE",
    "Efficiency",
    "MRE-Bio","MRE-Located", "MRE-Ocean"
  ),
  cluster = c(1,rep(2,3),3,rep(4,3))
)

n_clusters <- length(unique(concept_cluster_df$cluster))

# List to hold one meta-graph per cluster
concept_meta_graphs <- vector("list", n_clusters)
concept_cluster_names <- vector("list", n_clusters)

for (k in 1:n_clusters) {
  # Get oro_types in this cluster
  types_in_cluster <- concept_cluster_df %>%
    filter(cluster == k) %>%
    pull(oro_type)
  
  # Filter TE results for these oro_types and significant edges
  cluster_te <- TE_results %>%
    filter(oro_type %in% types_in_cluster, 
           NodeX %in% selectedComponents,
           NodeY %in% selectedComponents,
           TE_pval <= 0.05,
           !is.na(TE_ratio),
           0 < TE_ratio
           )
  cluster_te$TE_ratio[cluster_te$TE_ratio==Inf] <- max(cluster_te$TE_ratio[is.finite(cluster_te$TE_ratio)])
  
  # Aggregate TE_ratio per NodeX → NodeY pair
  edge_summary <- cluster_te %>%
    ungroup()%>%
    group_by(NodeX, NodeY) %>%
    summarise(
      n_sig = n(),
      avg_ratio = mean(TE_ratio, na.rm = TRUE),
      avg_correlation = mean(optCor, na.rm=T),
      med_correlation = quantile(optCor, na.rm=T, probs = 0.5),
      avg_delay = mean(optDelay, na.rm=T),
      sign = ifelse(avg_correlation >= 0, 1, -1),
      oro_types = paste(unique(oro_type), collapse = ", "), 
      .groups="drop"
    )%>%
    mutate(
      relative_nsig = n_sig / max(n_sig, na.rm=T)
    )
  
  # Build igraph object
  g <- graph_from_data_frame(edge_summary, directed = TRUE)
  
  # Set edge weights and labels
  E(g)$weight <- scales::rescale(
      edge_summary$avg_ratio,
      to=c(0.25,1),
      from = range(TE_results$TE_ratio[
        TE_results$oro_type %in% concept_cluster_df$oro_type &
          !is.na(TE_results$TE_ratio) &
          is.finite(TE_results$TE_ratio)
      ])
      )
  E(g)$width <- 2
  E(g)$color <- scales::alpha(
    ifelse(E(g)$avg_correlation >= 0, "#4dac26", "#d01c8b"),
    E(g)$weight
    )
  E(g)$sign <- edge_summary$sign

  
  concept_meta_graphs[[k]] <- g
  concept_cluster_names[[k]] <- paste(typeAES$label[match(types_in_cluster, typeAES$level)], collapse = ", ")
}






## Plot the networks of the oro types
nodeTextSize <- 4 #3.5
titleSize <- 14 #12
labelSize <- 2.75


# Convert network to ggplot
source(here::here("R/compute_negative_edge_dots.R"))

plot_ggraph_network <- function(g, cluster_id, cluster_label, end_cap_mm = 10, end_cap_mult_factor=4) {
  

  g_tidy <- as_tbl_graph(g) 
  
  # Code for plotting dots at end of negative effects
  # --- Create layout (needed to access node positions) ---
  graph_layout <- create_layout(g_tidy, layout = "circle")
  
  # --- Extract negative edge targets ---
  neg_edges <- g_tidy %>%
    activate(edges) %>%
    filter(sign == -1) %>%  # Filter for negative edges
    as_tibble() %>%
    mutate(
      from_name = V(g_tidy)$name[from],
      to_name = V(g_tidy)$name[to]
    )
  
  # Get positions of source and target nodes for negative edges
  edge_positions <- graph_layout %>%
    filter(name %in% c(neg_edges$from_name, neg_edges$to_name))
  
  dot_positions <- compute_negative_edge_dots(g, layout = "circle", end_cap_mm, end_cap_mult_factor)

  
  ## plot
  ggraph(g_tidy, layout = "circle") +  # You can try "kk" or "circle" or "fr" for spacing
    
    # # --- Positive edges with arrows ---
    geom_edge_fan(
      aes(
        edge_width = width,
        edge_alpha = weight,
        label = paste0("lag = ", scales::number(avg_delay, accuracy = 1), ", ", oro_types),
        edge_color = color,
        filter = sign==1
      ),
      arrow = arrow(length = unit(7, 'mm')),
      end_cap = circle(end_cap_mm, 'mm'),
      label_dodge = unit(2, 'mm'),
      label_push = unit(2, 'mm'),
      angle_calc = 'along',
      label_size=labelSize,
      show.legend = FALSE
    ) +
  
    # --- Negative edges with dot ends (no arrow) ---
    geom_edge_fan(
      aes(
        edge_width = width,  # abs for width scaling
        edge_alpha = weight,
        label = paste0("lag = ", scales::number(avg_delay, accuracy = 1), ", ", oro_types),
        edge_color = color,
        filter = sign==-1
      ),
      end_cap = circle(end_cap_mm, 'mm'),  # no arrow
      lineend = "round",   
      label_dodge = unit(2, 'mm'),
      label_push = unit(2, 'mm'),
      angle_calc = 'along',
      label_size=labelSize,
      show.legend = FALSE
    ) +
    
    # --- Draw a point (dot) at target node of negative edges ---
    geom_point(
      data = dot_positions,
      aes(x = dot_x, y = dot_y, size = 4*width, alpha = weight),
      shape = 21,             
      fill = "#d01c8b",
      color="transparent",
      show.legend = FALSE
    ) +
    scale_size_identity()+
    
    # Plot nodes
    # geom_node_point(size = 6, aes(color = name)) +
    
    scale_color_manual(values = componentAES$colour,
                       breaks = componentAES$level, 
                       guide="none")+
    scale_fill_manual(values = componentAES$colour,
                       breaks = componentAES$level, 
                       guide="none")+
    scale_edge_color_identity()+
    scale_alpha_identity()+
    scale_edge_alpha_identity()+
    scale_edge_width_identity()+
    scale_x_continuous(expand = expansion(mult=0.1))+
    scale_y_continuous(expand = expansion(mult=0.1))+
    coord_cartesian(clip="off")+

    geom_node_label(aes(label = componentAES$label[match(name, componentAES$level)], fill = name), #repel = TRUE,
                   label.size = nodeTextSize, alpha=0.8,
                   color = "white"
                   ) +

    labs(title = paste(cluster_label), tag = letters[cluster_id]) +
    theme_void() +
    theme(
      plot.title = element_text(size = titleSize, hjust = 0.5),
      plot.margin = unit(c(5,5,5,5), units = "mm"),
      legend.position = "none"
      )
}

cluster_order <- 1:n_clusters
concept_cluster_names2 <- list(
  "CCS","mCDR","Efficiency","MRE"
)
concept_ggraph_plots <- lapply(cluster_order, function(k) {
  plot_ggraph_network(concept_meta_graphs[[k]], k, concept_cluster_names2[[k]], end_cap_mm = 10, end_cap_mult_factor=1.8)
})

concept_ggraph_plots[[3]]

# Arrange the ggraph plots horizontally according to cluster number
final_plot <- wrap_plots(concept_ggraph_plots, nrow = 2, widths = c(1,1.2))


# Show or save
ggsave(filename=here::here("figures/main/conceptual_cluster_networks.pdf"), final_plot, width = 10, height = 8)

```



```{r plot just efficiency network for method figure}
ggsave(
  here::here("figures/supplemental/networks_efficiency.png"),
  width = 4, height=4, dpi=600,
  plot = concept_ggraph_plots[[which(concept_cluster_names2 == "Efficiency")]]
)

```



```{r calculate node importance and influence}
networkBaseSize <- 16

plot_network_metrics <- function(network_graph){
  # causal influencers - legislation, policy
  influencers <- degree(network_graph, mode = "out") %>% 
    sort(decreasing = T) %>% 
    as.data.frame() %>%
    tibble::rownames_to_column("component") %>%
    mutate(metric = "Out-degree (N effects given)")
    
  # causal responders - deployment - 8
  responders <- degree(network_graph, mode = "in") %>% 
    sort(decreasing = T) %>% 
    as.data.frame() %>%
    tibble::rownames_to_column("component") %>%
    mutate(metric = "In-degree (N effects received)")
  
  
  # general influence - legislation                    policy           public interest
  eigenvalues <- sort(eigen_centrality(network_graph)$vector, decreasing =TRUE)  %>%
    as.data.frame() %>%
    tibble::rownames_to_column("component") %>%
    mutate(metric = "Eigenvector centrality")
  
  # mediators - deployment - 14
  mediators <- sort(betweenness(network_graph), decreasing=TRUE)%>%
    as.data.frame() %>%
    tibble::rownames_to_column("component") %>%
    mutate(metric = "Betweenness (N shortest paths)")
  
  
  ## format results 
  metaNetCentralityMetrics_tmp <- rbind(
    influencers,
    responders,
    eigenvalues,
    mediators
  )%>%
    mutate(
      component = factor(
        component, 
        levels = componentAES$level[componentAES$level %in% selectedComponents],
        labels = componentAES$label[componentAES$level %in% selectedComponents]
      )
    )%>%
    complete(component, metric)
  
  summary(metaNetCentralityMetrics_tmp)
  
  colnames(metaNetCentralityMetrics_tmp)[!(colnames(metaNetCentralityMetrics_tmp) %in% c("component","metric"))] <- "Value"
  
  ## make plot
  ## Plot of a heatmap of each metric value
  networkMetricstmp_ggp <- ggplot(
    data = metaNetCentralityMetrics_tmp %>%
      group_by(metric) %>%
      mutate(
        Value_scaled = scales::rescale(Value, to = c(0,1))
      ),
    aes(x=component, y=metric)
  )+
    geom_tile(aes(fill = Value_scaled))+
    # geom_text(aes(label = scales::number(Value, accuracy=0.1)))+
    scale_y_discrete(
      labels = function(x) stringr::str_wrap(x, width=15)
    )+
    scale_x_discrete(
      breaks = levels(metaNetCentralityMetrics_tmp$component),
      drop=FALSE
    )+
    scale_fill_distiller(palette = "Blues", direction=1, na.value = "grey")+
    labs(
      x = "Node",
      y="Network metric",
      fill = "Metric\n(scaled)"
    )+
    theme_minimal(base_size = networkBaseSize)+
    theme(
      axis.text.x = element_text(angle = 45, hjust=1)
      # legend.position = "bottom"
    )
  # networkMetricstmp_ggp
  return(networkMetricstmp_ggp)
}


concept_metrics_heatmaps <- lapply(c(2,3,4), function(k) {
  plot_network_metrics(concept_meta_graphs[[k]])
})


```

```{r SKIP and load in next chunk - compute QNM , eval=FALSE}
source(here::here("R/impact.barplot.myMod"))
source(here::here("R/extend.vector"))
source(here::here("R/my.QPress.R"))
addTaskCallback(function(...) {set.seed(123);TRUE})

qnmBaseSize <- 12


# Function to run press perturbation with timeout if it freezes
safe_impact_barplot <- function(ModSim, press, timeout = 30) {
  tryCatch(
    withTimeout(
      {
        impact.barplot.myMod(ModSim,
            perturb = press,
            plot=FALSE, percentage = TRUE
        )
      },
      timeout = timeout,
      onTimeout = "silent"   # do not throw error on timeout
    ),
    error = function(e) NULL,
    TimeoutException = function(e) NULL
  )
}

plot_QNM <- function(k, nSims = 5000, weightSimulation = FALSE, uncertainThreshold=0.5){
  
  # Get oro_types in this cluster
  oroTypes <- concept_cluster_df %>%
    filter(cluster == k) %>%
    pull(oro_type)

  ## 1. Pool the transfer entropy results
  sig_te_edges <- TE_results %>%
    filter(oro_type %in% oroTypes,
           NodeX %in% selectedComponents,
             NodeY %in% selectedComponents,
             TE_pval <= 0.05,
             !is.na(TE_ratio),
             0 < TE_ratio
             ) %>%
    group_by(NodeX, NodeY) %>%
    summarise(
      n_sig = n(),
      avg_ratio = mean(TE_ratio, na.rm = TRUE),
      avg_correlation = mean(optCor, na.rm=T),
      med_correlation = quantile(optCor, na.rm=T, probs = 0.5),
      avg_delay = mean(optDelay, na.rm=T),
      sign = ifelse(avg_correlation >= 0, 1, -1),
      oro_types = paste(unique(oro_type), collapse = ", "), 
      .groups = 'drop'
    )%>%
    mutate(
      Probability = n_sig / max(n_sig),
      Group = ifelse(uncertainThreshold < Probability,0,1),
      Type = ifelse(sign==1,"P","N")
    ) %>%
    arrange(desc(Probability)) %>%
    select(From=NodeX, To=NodeY, Group, Probability, Type)
  

  
  
  
  # Format the data frame for Q Press
  sig_nodes <- as.character(unique(c(sig_te_edges$From, sig_te_edges$To)))
  
  sig_te_edges <- sig_te_edges %>%
    mutate(
      From = factor(From, levels = sig_nodes),
      To = factor(To, levels=sig_nodes),
      Type = factor(Type, levels = c("N","P","U","Z")),
      # Group = factor(Group)
    ) %>%
    ungroup() %>%
    ## add Pair
    mutate(
      # Convert From and To to character to avoid issues with factors
      From_char = as.character(From),
      To_char = as.character(To),
      # Sort From and To alphabetically
      PairKey = pmin(From_char, To_char),
      PairKey = paste(PairKey, pmax(From_char, To_char), sep = "-")
    ) %>%
    mutate(Pair = as.integer(factor(PairKey)))
  
  
  
  
  allEdgesLabels <- edge.labels(sig_te_edges) 
  
  Mod <- enforce.limitation(sig_te_edges%>% select(From, To, Group, Type, Pair)) 
  
  
  ## 2. Build Qualitative network model
  # Optional: Simulate with weights?
  if(weightSimulation){
    s <- community.sampler(Mod)
    s$select(mean(uncertainSampleProbsDf$Probability))
    ModSim <- tryCatch(
      {system.simulate(nSims, Mod, s)},
      error = function(e){NULL}
    )
   
  }else{
    ModSim <- tryCatch(
      {system.simulate(nSims, Mod)},
      error = function(e){NULL}
    )
  }
  
  ## If Mod sim Null, return nothing, else calculate presses
  if(is.null(ModSim)){
    return(NULL)
  }else{
    
    ## 3. For each node (except action), simulate a press perturbation and record the response of action.
  
    # Loop over interventions on each node → observe effect on "deployment"
    all_nodes <- sig_nodes[sig_nodes != "deployment"]
    qp_results_df <- data.frame()
    for (node in all_nodes) {
      press <- c(1)
      names(press) <- node
    
      # Set evidence and query deployment
      temp <- safe_impact_barplot(ModSim, press, timeout = 30)
      if(is.null(temp)){
        return(tibble(Negative=NA, `No Change`=NA, Positive=NA, Press_perturb=node))
      }else{
        temp <- as.data.frame(t(temp["deployment",]))
        temp$Press_perturb <- node
      }
      
      qp_results_df <- qp_results_df %>% bind_rows(temp)
      
    } # end of calculating presses
    
    

    # Plot results
    uniquePP <- unique(qp_results_df$Press_perturb)
    
    qnm_ggp <- ggplot(qp_results_df, 
           aes(x=Press_perturb))+
      geom_col(aes(y=Positive), fill="#4dac26")+ 
      geom_col(aes(y=-Negative), fill = "#d01c8b")+
      
      geom_text(
        data = qp_results_df[0 < qp_results_df$Positive,],
        aes(y=Positive, label = Positive), vjust=0)+
      geom_text(
        data = qp_results_df[0 < qp_results_df$Negative,],
        aes(y=-Negative, label = paste0("-",Negative)), vjust=1)+
      
      geom_hline(yintercept=0, col="red")+
      # scale_fill_stepsn(name = stringr::str_wrap("Positive effect on action (% sims)", width=25), n.breaks=5, colours = viridis(5))+
      scale_x_discrete(
        breaks = componentAES$level[
          componentAES$level %in% uniquePP
        ],
        labels = componentAES$label[
          componentAES$level %in% uniquePP
        ]
      )+
      scale_y_continuous(expand = expansion(mult=0.15))+
      labs(y="Effect on action (% sims)",x="Positive press perturbation node")+
      theme_minimal(base_size = qnmBaseSize)+
      theme(
        legend.position = "bottom",
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.x=element_text(angle=45, hjust=1)
      )
    # qnm_ggp
    return(qnm_ggp)
    
  } # end of !is.null(ModSim)
  
    
}


concept_qnm_barplots <- lapply(c(2,3,4), function(k){
  plot_QNM(k=k)
})

save(concept_qnm_barplots, file=here::here("data/derived-data/concept_qnm_barplots.RData"))

```

```{r LOAD QNM for different ORO networks}
load(here::here("data/derived-data/concept_qnm_barplots.RData"))

```

```{r plot networks and network metrics heatmaps and qnm results for conceptual clusters}

concept_ggraph_plots2 <- lapply(cluster_order, function(k) {
  plot_ggraph_network(concept_meta_graphs[[k]], k, concept_cluster_names2[[k]], end_cap_mm = 10, end_cap_mult_factor=2.5)
})

plotList <- c(
  # MRE
  concept_ggraph_plots2[[4]],
  concept_metrics_heatmaps[[3]],
  concept_qnm_barplots[[3]],
  
  # Efficiency
  concept_ggraph_plots2[[3]],
  concept_metrics_heatmaps[[2]],
  concept_qnm_barplots[[2]],
  
  # mCDR
  concept_ggraph_plots2[[2]],
  concept_metrics_heatmaps[[1]],
  concept_qnm_barplots[[1]]
  
)

combined <- wrap_plots(plotList, ncol=3,
                       guides="collect", widths = c(1.6, 1,1))+
  plot_annotation(tag_levels = "a", tag_suffix = ")") &
  theme(legend.position = "bottom",
        legend.key.width = unit(0.6, "in"),
        legend.title = element_text(margin = margin(r = 15)))

ggsave(
  here::here("figures/main/conceptual_networks_heatmaps_qnm.pdf"),
  width=11, height=13, 
  plot=combined
)

```




# Alginment vs incongruence (GLM regression)

 
Which OROs receive more/less attention in policy, legislation and social media, given their number of publications?


NB: Because compares OROs between each other, cannot include deployment because all the metrics are on different scales



```{r fit glm of proportional share of component ~ proportional share of publication + year, eval = FALSE}


## Loop through the different components and fit glm
glmBaseSize <- 12
axisTitleSize <- 13

# different components to loop through
components <- unique(allComponentDat_model$component) 
components <- components[!(components %in% c("publications","deployment"))]


## Start loop
ggps_prop <- vector("list", length(components))
names(ggps_prop) <- components
modelResultsProp <- vector("list", length(components))
for(c in 1:length(components)){
  
  ## Fit the model
  tmpDat_annualSum <- allComponentDat_model %>%
    filter(component %in% c("publications", components[c])) %>%
    group_by(year, component) %>%
    summarise(
      total = round(sum(y, na.rm = T))
    ) %>%
    pivot_wider(names_from = component, values_from = total)

  tmpDat <- allComponentDat_model %>%
    mutate(
      y= round(y)
    )%>%
    filter(component %in% c("publications", components[c])) %>%
    pivot_wider(names_from = component, values_from = y, id_cols = c("oro_type","year")) %>%
    left_join(tmpDat_annualSum, by = c("year"), suffix = c("","_total")) %>%
    mutate(across(where(is.numeric), ~ ifelse(is.na(.x), 0, .x))) %>%
    mutate(
      prop_pub = publications/publications_total,
      oro_type = factor(
        oro_type,
        levels = typeAES$level,
        labels = typeAES$label
      )
    )
  
  colnames(tmpDat) <- gsub(components[c],"y", colnames(tmpDat))
  tmpDat$prop_y <- tmpDat$y/tmpDat$y_total
  tmpDat$prop_y[is.na(tmpDat$prop_y)] <- 0
  tmpDat$fail <- as.integer(tmpDat$y_total-tmpDat$y)
  tmpDat$y <- as.integer(tmpDat$y)
  tmpDat<- tmpDat[0<tmpDat$y_total,]

  mod <- glm(cbind(y, fail) ~ prop_pub + year, 
                 data = tmpDat ,
                 family = binomial)
  
  od <- performance::check_overdispersion(mod)
  if(od$dispersion_ratio > 1 & od$p_value < 0.05){
    
    mod <- glm(cbind(y, fail) ~ prop_pub + year, 
                 data = tmpDat,
                 family = quasibinomial)
    
  }
  
  
  plot_model(mod, type = "pred", terms = c("prop_pub"))
  
  
  # The coefficients table for oro_type (exclude intercept and year)
  mod_sum <- summary(mod)
  
  if(mod_sum$family$family == "quasibinomial"){
    distributionLabel <- paste0(
      "Quasibinomial (dispersion = ",
      scales::number(mod_sum$dispersion, accuracy = 1, big.mark = ","),")"
    )
    
  }else{
    distributionLabel <- "Binomial"
  }
  
  ## store model results
  componentLabel <- componentAES$label[match(components[c], componentAES$level)]
  
  modSummary <- as.data.frame(mod_sum$coefficients) %>%
    rownames_to_column("Term") %>%
    mutate(Component = 
             paste0(
               componentLabel
             ),
    Distribution = distributionLabel) %>%
    relocate(Component) %>%
    mutate(
      Term = gsub("prop_pub","Proportion of publications", gsub("year", "Year", Term))
    )
  

  modelResultsProp[[c]] <- modSummary

  pval = data.frame(
    estimate = paste(signif(modSummary[grepl("publications",modSummary$Term),grep("Estimate", colnames(modSummary))],
                                    digits = 2)),
    pval = ufs::formatPvalue(modSummary[grepl("publications",modSummary$Term),grep("Pr", colnames(modSummary))])
  ) %>%
    mutate(
      label = paste0(
        "\u03b2 = ", estimate,"\n",pval
      )
    )
  
  
  # add a shape variable
  shapelookup <- tmpDat %>%
    distinct(oro_type)%>%
     mutate(
       oro_branch = case_when(
         grepl("MRE", oro_type)~"MRE",
         grepl("CDR",oro_type)~"CDR",
         TRUE~"Other"
       )
     )%>%
    group_by(oro_branch)%>%
    mutate(
      shape_type = factor(row_number(), levels = 1:3, labels = c(15, 16, 17))
    )
  tmpDatPlot <- tmpDat%>%
    left_join(shapelookup, by = "oro_type")
  
  
  tmpDatSums <- tmpDat %>%
    group_by(oro_type) %>%
    summarise(
      prop_y = mean(prop_y, na.rm = T),
      prop_pub = mean(prop_pub, na.rm=T)
    )
    
 
  
  ## Plot 
  pred <- ggpredict(mod, terms = "prop_pub [all]")
  
  ggps_prop[[c]] <- ggplot(data = pred)+
    geom_line(aes(x, predicted), col="black",linetype = "solid", size=0.7)+
    geom_ribbon(aes(x=x, ymin = conf.low, ymax = conf.high), alpha = 0.1)+
    geom_point(data = tmpDatPlot,
               aes(x=prop_pub, y = prop_y, shape = shape_type, col=oro_type),
                             size=3, alpha = 0.25)+
    ggrepel::geom_text_repel(data = tmpDatSums, aes(x=prop_pub, y = prop_y, label = oro_type), force = 0.3,
                             size=3.5)+
    ggrepel::geom_text_repel(data = pval, aes(label = label, fontface=3), 
                             x=Inf, y=-Inf, hjust=1, vjust=0, col="black",
                             size=4)+
    scale_color_manual(
      breaks = typeAES$label,
      values = typeAES$colour
    )+
    scale_shape_manual(
      breaks = levels(shapelookup$shape_type),
      values = c(15, 16, 17)
    ) +
    guides(
      color = guide_legend(override.aes = list(shape = as.numeric(as.character(shapelookup$shape_type[order(shapelookup$oro_type)])))),
      shape = "none" 
    )+
    scale_x_continuous(expand = expansion(mult = c(0.15,0)))+
    scale_y_continuous(expand = expansion(mult = c(0.15,0)))+
    labs(
      x = "Proportion of publications",
      y = paste("Proportion of",tolower(componentAES$label[componentAES$level == components[c]])),
      color = "ORO type"
    )+
    theme_minimal(glmBaseSize)+
    theme(
      axis.title = element_text(size=axisTitleSize),
      plot.margin = unit(c(0.7,0.3,0.3,0.3),"cm")
    )
  
  ggps_prop[[c]]
  
  leg <- ggpubr::get_legend(ggps_prop[[c]])
  ggps_prop[[c]] <- ggps_prop[[c]]+theme(legend.position = "none")
  
  
}

modelResultsPropTable <- do.call("bind_rows", modelResultsProp)

modelResultsPropTable[,3:6] <- apply(modelResultsPropTable[,3:6],1:2, function(x) format(as.numeric(x),digits = 2, scientific=TRUE))





# ## save
# 
# write.csv(
#   modelResultsPropTable,row.names = FALSE,
#   here::here("outputs/publications_otherDim_glmResults_proportion.csv")
# )









ggps_prop_tagged <- mapply(
  function(p, tag) p + 
    labs(tag = tag) + 
    theme(legend.position = "none",
          plot.tag.position = c(0, 1.1),  # top-left like ggpubr
          plot.tag = element_text(size = 10, hjust = 0, vjust = 1.2),
          plot.margin = unit(c(1,0.3,0,0.3),"cm")
          ),
  ggps_prop,
  paste0(letters[1:length(ggps_prop)], ") ", 
         componentAES$label[match(names(ggps_prop), componentAES$level)]),
  SIMPLIFY = FALSE
)

combined <- wrap_plots(ggps_prop_tagged, ncol = 2)


# Combine plots with legend on the right
final_plot <- combined + leg +theme(plot.margin = unit(c(0.1,0,0,0.1),"cm")) + 
  plot_layout(ncol = 3, widths = c(4,4, 2), byrow=FALSE)  # adjust widths as needed


ggsave(
  filename = here::here("figures/main/GlmFitPlots_proportion.pdf"),
  plot = final_plot,
  width = 10,
  height = 6.5
)

```

```{r plot just significant regression plots}

ggps_prop_tagged_signif <- mapply(
  function(p, varname, tag) p + 
    labs(y=varname, tag = tag) + 
    theme(#legend.position = "none",
          plot.tag.position = c(0, 1.1),  # top-left like ggpubr
          plot.tag = element_text(size = 10, hjust = 0, vjust = 1.2),
          plot.margin = unit(c(1,0.3,0,0.3),"cm")
          ),
  ggps_prop[c("legislation","public support")],
  c("Relative N legislation documents","Relative N positive posts"),
  paste0(letters[1:2], ") ", 
         # componentAES$label[match(c("legislation","public support"), componentAES$level)]),
         c("Legislation","Positive social media posts")),
  SIMPLIFY = FALSE
)

combined <- wrap_plots(ggps_prop_tagged_signif, ncol = 2)


# Combine plots with legend on the right
final_plot <- combined + leg +theme(plot.margin = unit(c(0.1,0,0,0.1),"cm")) + 
  plot_layout(ncol = 3, widths = c(4,4, 2), byrow=FALSE)  # adjust widths as needed


ggsave(
  filename = here::here("figures/poster/GlmFitPlots_signif_proportion.png"),
  plot = final_plot,
  width = 9,
  height = 4,
  dpi=2000
)


```

```{r combine regression plots with plots of distribution by ORO type}

## Get data and plot distribution by oro for the different dimensions 

p1_db <- RSQLite::dbConnect(RSQLite::SQLite(),
                      here::here("data","sqlite-databases","product1.sqlite"),
                                 create=FALSE)

nPubs_oro <- tbl(p1_db, "pred_oro_type_mit_long") %>%
  group_by(level)%>%
  summarise(
    N = n_distinct(analysis_id[0.5 <= (mean_prediction)])
  )%>%
  collect()%>%
  mutate(
    oro_type = gsub("[.]","-",level),
    component = "publications",
    variable_name = "Publications (N)"
  ) %>%
  select(oro_type, component, variable_name, N)

dbDisconnect(p1_db)



## Sumarise by component overall proportional share of each ORO
prop_oro_summary_df <- allComponentDat_model %>%
  filter(!(component %in% c("deployment","publications"))) %>%
  group_by(component, variable_name, oro_type) %>%
  summarise(
    N = sum(y, na.rm=T)
  ) %>%
  bind_rows(
    nPubs_oro
  ) %>%
  group_by(component) %>%
  mutate(
    total_component = sum(N, na.rm=T)
  ) %>%
  ungroup() %>%
  mutate(
    prop_component = N/total_component,
    component = factor(component, 
                       levels = componentAES$level, 
                       labels = componentAES$label_varname)
  )%>%
  mutate(
    component = droplevels(component),
    oro_type = factor(
      oro_type,
      levels = typeAES$level,
      labels = typeAES$label
    )
  )
  


# Plot proportion of each dimension per ORO
propBaseSize <- 11
dist_component_ggp_bottom <- ggplot(data = prop_oro_summary_df, aes(x=component))+
  geom_col(aes(y=prop_component, fill=oro_type), position="stack")+
  geom_text(aes(label = scales::number(total_component, accuracy = 1, big.mark = ",")), check_overlap = TRUE, vjust=0, hjust=0, y=1, angle=45)+
  scale_fill_manual(
    breaks = typeAES$label,
    values = typeAES$colour
  )+
  scale_y_continuous(expand = expansion(mult=c(0,0.2)))+
  labs(
    x="Dimension (i.e. Node)",
    y="Proportion",
    fill="ORO type",
    tag = "a"
  )+
  guides(fill=guide_legend(ncol=2))+
  coord_cartesian(clip="off")+
  theme_minimal(base_size = propBaseSize)+
  theme(
    axis.text.x = element_text(angle=45, hjust=1),
    legend.position = "bottom",
    legend.title.position = "top"
  )

dist_component_ggp_bottom


## Combine plots
ggps_prop_leg <- lapply(ggps_prop, function(x) x+theme(legend.position="bottom"))

combined <- wrap_plots(ggps_prop_leg, guides = "collect") & theme(legend.position = 'bottom')


plotLayout<-"AB
#B"

combined2 <- dist_component_ggp_bottom + combined + plot_annotation(tag_levels = "a", tag_suffix = ")")+plot_layout(widths = c(1.3,3), heights=c(1,0.2), design = plotLayout)


ggsave(
  filename = here::here("figures/main/GlmFitPlots_proportion_andBarplots.pdf"),
  plot = combined2,
  width = 12,
  height = 8
)

```








# Narrative analysis

*Key message*
Key changes in policy/legislation, catalysed by the right socio-political environment, can have enormous impact on publication & action.


Steps:
1. Run a baysean change point analysis to determine likely inflection points in each time series
2. Choose three case study OROs (MRE-Located, mCDR-OAE, Efficiency) and construct a qualitative narrative (discussing as well alongside international policy moments)

```{r changepoint analysis all oros, eval=FALSE}

# The oro types to analyse 
oroTypes = unique(allComponentDat_model$oro_type)


# Jags initialization parameters
jagsInits <- list(
  # Maximum number of changepoints to look for
  "K_max"=3,
  # Time buffer for looking for change points. 
  # i.e. don't find a change point at the first or last year
  "cpmin"="jagsData$MINX+1", 
  "cpmax"="jagsData$MAXX-1")

# Compile all data into a list to iterate through
dataList <- list()
for(c in selectedComponents){
  dataList[[c]] <- allComponentDat_model %>% filter(oro_type %in% oroTypes, component == c)
 
}

# Compile all model inputs into a list to iterate through
modelInputs <- list(
  "data" = dataList,
  "bugsModFiles" = list(
    "deployment" = "R/bugs-models/GammaModMultCP", # continuous non-negative
    "publications" = "R/bugs-models/PoissonModMultCP", 
    "policy" ="R/bugs-models/PoissonModMultCP", 
    "legislation" = "R/bugs-models/PoissonModMultCP", 
    "public interest" = "R/bugs-models/PoissonModMultCP",
    "public support" = "R/bugs-models/PoissonModMultCP",
    "public opposition"= "R/bugs-models/PoissonModMultCP"
  ),
  "jagsData" = list(jagsInits)[rep(1,length(components))]
)

# The different time series components to analyse (e.g. deployment, legislation, policy, posts, etc)
components <- names(modelInputs$bugsModFiles)

## Loop through all oro types and time series (components) to run change point analysis
cutpoint_results <- data.frame()
cutpoint_densities <- data.frame()

for(oro in oroTypes){
  # oro = "CCS"
  
  for(i in 1:length(components)){
    # i = 4
      
    print(paste(oro,":", components[i]))
    
    # get model to fit
    modFile <- modelInputs$bugsModFiles[[components[i]]]
    
    # If OAE deployment, y is poisson not Gamma so change
    if(components[i] == "deployment" & oro %in% c("CDR-OAE")){
      modFile <- "R/bugs-models/PoissonModMultCP"
    }
    
    # Format data
    jagsData <- modelInputs$jagsData[[i]]
    myinits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 123)
    dat <- modelInputs$data[[components[i]]][modelInputs$data[[components[i]]]$oro_type == oro,]
    if(nrow(dat) < 5){
      print("Insufficient data")
      next
    }
    yearLims <- range(modelInputs$data[[components[i]]][
      modelInputs$data[[components[i]]]$oro_type == oro,"year"
    ], na.rm=T)
    
    # Format specific inputs depending on the model
    if(grepl("pubBinom", modFile)){ 
      trialVar <- ifelse(grepl("ORO", modelInputs$y_variable[[i]]),
                         "n_OC","total_posts")
      successVar <- ifelse(grepl("ORO", modelInputs$y_variable[[i]]),
                         "n_ORO","n_posts")
      # if a proportion, only keep proportions of total values above 100 # modelInputs$y_variable[[i]] == "prop_ORO"
      keepYears <- dat$year[dat$variable_name == trialVar & 100 < dat$y]
      dat <- dat[dat$year %in% keepYears,]
      dat <- dat %>% arrange(variable_name, year)
      nTrial <- dat$y[dat$variable_name == trialVar]
      jagsData$nTrial <- nTrial
      jagsData$nSuccess <- dat$y[dat$variable_name == successVar]
      
    }
    
    if(grepl("ZIP", modFile)){ 
      # if the model is zero inflated poisson, 
      # Fill missing years with 0 values
      dat <- dat %>% tidyr::complete(year = yearLims[1]:yearLims[2])
      dat$y[is.na(dat$y)] <- 0
      dat <- dat %>% tidyr::fill(component, oro_type, variable_name)
      # add the latent binomial indicator, as to whether there are no documents at all
      myinits$w <- dat$y
      myinits$w[myinits$w > 0] <- 1
      
    }
    
    if(grepl("Poisson", modFile)){ 
      # Fill missing years with 0 values
      dat <- dat %>% tidyr::complete(year = yearLims[1]:yearLims[2])
      dat$y[is.na(dat$y)] <- 0
      dat$y <- round(dat$y) # force discrete y values
      dat <- dat %>% tidyr::fill(component, oro_type, variable_name)
      
    }
    
    if(grepl("Gamma", modFile)){
      # Fill missing years with 0 values
      dat <- dat %>% tidyr::complete(year = yearLims[1]:yearLims[2])
      dat$y[is.na(dat$y)] <- 0
      dat$y[dat$y==0] <- 0.1
      dat <- dat %>% tidyr::fill(component, oro_type, variable_name)
    }
    
    # make sure no NAs
    dat <- na.omit(dat)
    
    # Skip if after subsetting data there is insufficient data points
    if(nrow(dat) < 5){
      print("Insufficient data after subsetting")
      next
    }
    #with(dat, plot(year, y))
    
    jagsData$y <- dat$y
    jagsData$x <- dat$year 
    jagsData$MINX <- min(jagsData$x)
    jagsData$MAXX <- max(jagsData$x)
    jagsData$cpmin <- eval(parse(text = jagsData$cpmin))
    jagsData$cpmax <- eval(parse(text = jagsData$cpmax))
    
    if((jagsData$cpmax-jagsData$cpmin)<5){
      jagsData$cpmin <- jagsData$MINX
      jagsData$cpmax <- jagsData$MAXX
    }
    
    
    ## Fit mdel
    bugs.model <- readChar(modFile, file.info(modFile)$size)
    
    jagsModel <- tryCatch(
      {
        rjags::jags.model(
          file = textConnection(bugs.model),
          data = jagsData,
          inits = myinits,
          n.chains = 5,
          n.adapt = 1500,
          quiet = FALSE
        )
        },
     error = function(e){
       return(NULL)
     }
    )
    if(is.null(jagsModel)){
      next
    }
    
    updatedOK <- tryCatch(
      {
        update(jagsModel, 1000)
        TRUE
      },
      error = function(e) {
        message("update() failed: ", e$message)
        FALSE
      }
    )
    
    if (!updatedOK) {
      message("Retrying with K_max-1")
      jagsData$K_max <- jagsData$K_max - 1
      
      jagsModel <- tryCatch(
        {
          rjags::jags.model(
            file = textConnection(bugs.model),
            data = jagsData,
            inits = myinits,
            n.chains = 5,
            n.adapt = 1500,
            quiet = FALSE
          )
        },
        error = function(e) {
          message("Re-fit failed at model creation: ", e$message)
          return(NULL)
        }
      )
      
      if (is.null(jagsModel)) {
        next
      }
      
      updatedOK <- tryCatch(
        {
          update(jagsModel, 1000)
          TRUE
        },
        error = function(e) {
          message("Re-fit update() failed again: ", e$message)
          FALSE
        }
      )
      
      if (!updatedOK) {
        next
      }
      
    }
    
    
    
    s <- tryCatch(
      {
        rjags::coda.samples(
          model = jagsModel,
          variable.names = c("alpha","beta","K","x_cp","z","pk"),
          n.iter = 1000,
          quiet = FALSE
        )
      },
      error = function(e){
        return(NULL)
      }
    )
    if(is.null(s)){
      next
    }
    
    qs <- summary(s)$quantile
    # qs
    # plot(s[,"pk[2]"]) # check mixing
    
    # Subset to only the change points that are signif 
    # And if there are signif change points, only those that indicate a positive change, because narrative analyses changes in progress
    K <- round(quantile(qs["K",], 0.5))
    if(grepl("ZIP", modFile)){
      K <- sum(0.5 <= qs[grepl("pk", rownames(qs)),"50%"])
    }
    
    if(K == 0){
      print("No significant change points")
      next
    }else{
      pk <- qs[grep("pk", rownames(qs)),"50%"] # probability for each change point
      x_cp <- qs[grep("x_cp", rownames(qs)),"50%"] # locations of each change point
      # arrange in order of decreasing probability to find the most K probable
      pkIndKeep <- order(pk, decreasing=T) 
      pkIndKeep <- pkIndKeep[1:K] 
      # then re-arrange in chronological order for calculating trends of segments
      pkIndKeep <- sort(pkIndKeep) 
      
      # Find which change points mark a positive change in trend
      trends <- vector("numeric", K+1) # store the trend for each segment
      
      starts <- c(min(jagsData$x), sort(round(x_cp[pkIndKeep])))
      match(starts, round(x_cp[pkIndKeep]))
      ends <- c(round(x_cp[pkIndKeep]), max(jagsData$x))
      for(k in 1:(K+1)){
        if(grepl("Binom", modFile)){
          trends[k] <- jagsData$y[which.min(abs(jagsData$x-ends[k]))] - jagsData$y[which.min(abs(jagsData$x-starts[k]))]
        }else if(grepl("ZIP|Pois|Gamma", modFile)){
          trends[k] <- sum(jagsData$y[which.min(abs(jagsData$x-starts[k])):which.min(abs(jagsData$x-ends[k]))] )
        }
        
      }
      # only keep the positive changes in trends
      matchInd <- match(starts[-1], round(x_cp[pkIndKeep]))
      pkIndKeep <- pkIndKeep[matchInd[which(0 < diff(trends))]] 
      
      if(length(pkIndKeep) == 0){
        print("no changepoints marking increasing trend")
        next
      }else{
        pkIndNames <- names(x_cp[pkIndKeep])
      
        
        # save probability density of the cut points for plotting
        calc_density <- function(vals){
          dens <- density(vals, n=100)
          return(data.frame("year"=dens$x, "cp_density"=dens$y))
        }
        densTemp <- do.call(rbind, s)
        densTemp <- as.matrix(densTemp[,pkIndNames], ncol=length(pkIndNames))
        densTemp <- do.call(rbind.data.frame, apply(densTemp, 2, function(x) calc_density(x)))
        densTemp$cp_id <- rep(pkIndNames, 100)[sort(rep(1:length(pkIndNames),100))]
        
        
        # Save summary of quantiles
        dfTemp <- do.call(rbind, apply(qs[pkIndNames,,drop=FALSE], 1, function(x) as.data.frame(x)))
        dfTemp$quantile = rep(colnames(qs), length(pkIndNames))
        dfTemp$cp_id <- rep(pkIndNames, 5)[sort(rep(1:length(pkIndNames),5))] 
        
        rownames(dfTemp) <- NULL
        # Cap the distribution of cutpoints at the hard limits of the data
        colnames(dfTemp)[which(colnames(dfTemp) == "x")] <- "year"
        dfTemp$year <- ifelse(dfTemp$year < yearLims[1], yearLims[1], dfTemp$year)
        dfTemp$year <- ifelse(yearLims[2] < dfTemp$year, yearLims[2], dfTemp$year)
        # Add id variables
        addIdVariables <- function(df){
          df %>% mutate(
                        oro_type = oro,
                        component = dat$component[1],
                        variable_name = dat$variable_name[1])
        }
        dfTemp <- addIdVariables(dfTemp)
        densTemp <- addIdVariables(densTemp)
        
        ## Bind data to results
        cutpoint_results <- rbind(cutpoint_results, dfTemp)
        cutpoint_densities <- rbind(cutpoint_densities, densTemp)
        # remove jags model
        rm(jagsModel)
      }
    }

      

  } # end looping through ORO types
} # end looping through bugs models



## Save results
save(cutpoint_results,cutpoint_densities, file=here::here("outputs/cutpointResults_mitigation_alloros.RData"))
```

`


```{r plot all time series and international policy moments grouped by country, eval=FALSE}
load(here::here("outputs/cutpointResults_mitigation_alloros.RData"))

# Grouped by country
# Load data for specific OROs
load(here::here("data", "derived-data", "mitigationORO_pubs_country.RData")) #pubs
load(here::here("data", "derived-data", "n_policy_leg_country.RData")) # leg_pol_country
load(here::here("data/derived-data/mitigationDeployDat_country.RData")) # allDeployDat_country

# bind together
allComponentDat_model_country <- pubs_country %>%
  bind_rows(leg_pol_country) %>%
  bind_rows(allDeployDat_country) %>%
  bind_rows(postsDat) %>%
  mutate(
    component = replace(component, component == "non-binding policy", "policy")
  ) %>%
  filter(year_lim[1] <= year & year <= year_lim[2],
         oro_type %in% typeAES$level) %>%
  filter(
    component %in% selectedComponents
  )


# Calculate overall proportions for country groups
maxy <- allComponentDat_model_country %>%
  group_by(component, variable_name)%>%
  summarise(
    total = sum(y, na.rm=T)
  )
  
allCountrySums <- allComponentDat_model_country %>%
  filter(component %in% selectedComponents, !is.na(country_simple)) %>%
  group_by(component, variable_name, country_simple)%>%
  summarise(
    y_total=sum(y)
  )%>%
  ungroup()%>%
  left_join(maxy, by=c("component","variable_name")) %>%
  mutate(
    y_percent = y_total/total*100
  ) 
  

allCountrySums %>% filter(component %in% c("publications"), country_simple == "China")
#   component    variable_name    country_simple y_total total y_percent
#   <chr>        <chr>            <fct>            <dbl> <dbl>     <dbl>
# 1 publications Publications (N) China            6454. 36410      17.7


# Calculate overall proportions for country groups by ORO type
maxy <- allComponentDat_model_country %>%
  group_by(oro_type, component, variable_name)%>%
  summarise(
    total = sum(y, na.rm=T)
  )
allCountrySums <- allComponentDat_model_country %>%
  filter(component %in% selectedComponents, !is.na(country_simple)) %>%
  group_by(oro_type, component, variable_name, country_simple)%>%
  summarise(
    y_total=sum(y)
  )%>%
  ungroup()%>%
  left_join(maxy, by=c("component","variable_name","oro_type")) %>%
  mutate(
    y_percent = y_total/total*100
  ) 
  

allCountrySums %>% filter(component %in% c("publications"), country_simple == "China")
#   oro_type    component    variable_name    country_simple y_total  total y_percent
#   <chr>       <chr>        <chr>            <fct>            <dbl>  <dbl>     <dbl>
# 1 CCS         publications Publications (N) China             87.5  1017.      8.60
# 2 CDR-BC      publications Publications (N) China            217.   1263.     17.2 
# 3 CDR-BioPump publications Publications (N) China             14.8   202       7.33
# 4 CDR-OAE     publications Publications (N) China             13.2    93      14.2 
# 5 Efficiency  publications Publications (N) China            485.   2308      21.0 
# 6 MRE-Bio     publications Publications (N) China             24.9   274.      9.11
# 7 MRE-Located publications Publications (N) China           2974.  13366.     22.3 
# 8 MRE-Ocean   publications Publications (N) China           2638.  17888.     14.7



## make panel plots ##
savePlot = FALSE

oro_groups <- list(
  "mCDR" = c("CDR-BioPump","CDR-BC","CDR-OAE"),
  "MRE" = c("MRE-Ocean","MRE-Located","MRE-Bio"),
  "Other"= c("CCS","Efficiency"),
  "narrative" = c("MRE-Located", "Efficiency","CDR-OAE")
)

# ## View summary table of cutpoints
# View(cutpoint_results %>% filter(oro_type %in% oro_groups$narrative) %>% pivot_wider(id_cols = c(oro_type, cp_id, component, variable_name), names_from = quantile, values_from = year))


## Plot time series by group
ggps_country <- vector("list", length(oro_groups))

for(og in which(names(oro_groups)=="narrative")){
  
  ggps_country[[og]] <- vector("list", length(oro_groups[[og]]))
  
  for(o in 1:length(oro_groups[[og]])){
    # get data
    tmpDat <- allComponentDat_model %>%
      filter(oro_type == oro_groups[[og]][o], !is.na(year), component %in% selectedComponents) %>%
      mutate(
        y = ifelse(component %in% 
                     c("deployment"), y, log(y))
      )
    
    # Maybe scale by proportion?
    maxy <- tmpDat %>%
      group_by(component, year)%>%
      summarise(
        maxy = max(y, na.rm=T)
      )
    
    tmpDat_country <- allComponentDat_model_country %>%
      filter(oro_type == oro_groups[[og]][o], !is.na(year), component %in% selectedComponents, !is.na(country_simple)) %>%
      group_by(component, variable_name, country_simple, year)%>%
      summarise(
        y_c=sum(y)
      )%>%
      group_by(component, variable_name, year) %>%
      mutate(
        y_total = sum(y_c)
      ) %>%
      ungroup()%>%
      left_join(maxy, by=c("component","year")) %>%
      mutate(
        y_prop = y_c/y_total,
        y = y_prop*maxy
      )
    
    # format for manual geom_area
    if(!is.null(tmpDat_country)){
      df_min <- tmpDat_country %>%
        group_by(component) %>%
        summarise(ymin = min(y[is.finite(y)]))
      tmpDat_country <- tmpDat_country %>%
        left_join(df_min, by = "component") %>%
        group_by(component, year) %>%
        arrange(component, year, desc(country_simple)) %>%
        mutate(
          ymin_stack = ymin + cumsum(dplyr::lag(abs(y), default = 0)),
          ymax_stack = ymin + cumsum(abs(y))
        ) %>%
        ungroup()
    }
    
      
    
    actionMetric <- tmpDat$variable_name[tmpDat$component == "deployment"][1]
    facetLabels <- c("Action" = paste0("Action\n(", actionMetric, ")"),
                     "Publications" = "Publications\nlog(N articles)",
                     "Policy" = "Policy\nlog(N documents)",
                     "Legislation" = "Legislation\nlog(N documents)",
                     "Interest" = "Interest\nlog(N posts)",
                     "Support" = "Support\nlog(N positive posts + likes)"
                     # "Opposition" = "Opposition\nlog(N negative posts + likes)"
                     )
    
    tmpDat <- tmpDat %>%
      mutate(
        component_color = factor(component, levels = componentAES$level, labels = componentAES$colour),
        component = factor(component, levels = componentAES$level, labels = componentAES$label)
      ) %>%
      mutate(
        component = droplevels(component),
        component_color = droplevels(component_color)
      )%>%
      mutate(
        component = plyr::revalue(component, facetLabels)
      )
    
    
    if(!is.null(tmpDat_country)){
      tmpDat_country <- tmpDat_country %>%
        mutate(
          component_color = factor(component, levels = componentAES$level, labels = componentAES$colour),
          component = factor(component, levels = componentAES$level, labels = componentAES$label)
        ) %>%
        mutate(
          component = droplevels(component),
          component_color = droplevels(component_color)
        )%>%
        mutate(
          component = plyr::revalue(component, facetLabels)
        )
    }
    
    # Get cutpoints
    changePointsQuantiles <- cutpoint_results %>%
      filter(oro_type == oro_groups[[og]][o], component %in% selectedComponents) %>%
      pivot_wider(names_from = quantile, values_from = year)%>%
      mutate(iqr = `97.5%`-`2.5%`)%>%
      filter(iqr < 5) %>%
      select(-iqr)%>%
      pivot_longer(cols = `2.5%`:`97.5%`, names_to = "quantile", values_to = "year") %>%
      mutate(
        component = factor(component, levels = componentAES$level, labels = componentAES$label)
      ) %>%
      mutate(
        component = plyr::revalue(component, facetLabels)
      )
    
    
    ggp_tmp <- ggplot()+
      
      # geom_vline(data = changePointsQuantiles %>%filter(quantile %in% c("2.5%","97.5%")),
      #          aes(xintercept=.data$year),
      #          linewidth = 0.5, linetype = "dashed")+
      geom_rect(data = changePointsQuantiles %>%filter(quantile %in% c("2.5%","97.5%")) %>% pivot_wider(names_from = "quantile", values_from = year),
               aes(xmin=.data$`2.5%`, xmax=.data$`97.5%`),
               ymin=-Inf, ymax=Inf,
               fill="darkgrey", alpha=0.7)+
      geom_line(data = tmpDat, aes(x=year, y=y, color=component_color))+
      geom_rect(data = tmpDat_country, aes(xmin=year-0.5,xmax=year+0.5, ymin = ymin_stack, ymax = ymax_stack, fill=country_simple), alpha = 0.6) +
      scale_x_continuous(limits = year_lim)+
      scale_color_identity(guide="none")+
      scale_fill_manual(
        breaks = levels(tmpDat_country$country_simple),
        values = c(RColorBrewer::brewer.pal(4, "Paired"),"bisque2")
      )+
      facet_wrap(vars(component),ncol=1, scales = "free_y")+
      labs(
        y="",
        x="Year",
        title = typeAES$label[match(oro_groups[[og]][o], typeAES$level)],
        fill = "Country\ngroup"
      )+
      theme_minimal(base_size = 10)+
      theme(
        plot.margin = unit(c(0.1, 0.1,0.1,0.1), units = "cm"),
        legend.position = "bottom"
      )
    ggp_tmp
    
    ggps_country[[og]][[o]] <- ggp_tmp
    
      
  }
  
  
}





## Plot with international policy moments as well

intPolMoments <- readxl::read_excel(
  here::here("data/raw-data/international_policy_moments.xlsx")
)%>%
  mutate(
    year_min = as.numeric(year_min),
    year_max = as.numeric(year_max),
    component = "International policy"
  )
tmpYlims <- c(min(intPolMoments$year_min), max(intPolMoments$year_max)+2)

## Old version of just vertical lines and geom_text

# ggp_intPolMom <- ggplot()+
#   geom_rect(data = intPolMoments,
#            aes(xmin=.data$year_min, xmax=.data$year_max, fill = .data$label),
#            ymin = -Inf, ymax=Inf, alpha=0.5
#            )+ #fill="lightpink"
#   geom_vline(data = intPolMoments,
#            aes(xintercept=.data$year_min, col=.data$label),
#            linewidth = 0.5, linetype = "solid")+ #col="red"
#   geom_text(data = intPolMoments %>%
#               mutate(
#                 y= rep(seq(0.1,0.7, length.out=3),ceiling(nrow(intPolMoments)/3))[1:nrow(intPolMoments)]
#               ),
#            aes(x=.data$year_min, y=.data$y, label= stringr::str_wrap(.data$label, width=10)),
#            angle=30, hjust=0, col="black", size=2, nudge_x = 0.2)+
#   scale_x_continuous(limits = tmpYlims)+
#   scale_y_continuous(limits = c(0,1))+
#   labs(
#     y="",
#     x="Year"
#   )+
#   scale_color_discrete()+
#   theme_minimal(base_size = 10)+
#   coord_cartesian(clip="off")+
#   theme(
#     axis.text.y = element_blank(),
#     axis.ticks.y = element_blank(),
#     plot.margin = unit(c(0.1, 0.1,0.1,0.1), units = "cm"),
#     legend.position = "none"
#   )
# 
# ggp_intPolMom


## Infographic version
# Import slide
slide_img <- image_read_pdf("figures/main/policy_moments_timeline/pol_mom_timeline.pdf", density = 600)
slide_img <- image_trim(slide_img)
slide_ras <- as.raster(slide_img)
# plot(slide_ras)
ggp_intPolMom <- rasterGrob(slide_ras, interpolate = FALSE)


require(ggnewscale)

plotList <- lapply(ggps_country[[which(names(oro_groups)=="narrative")]],
                   function(x){
                     x+
                       new_scale_color()+
                       geom_vline(data = intPolMoments%>% select(year_min, label),
                         aes(xintercept=.data$year_min, col=.data$label),
                         linewidth = 0.5, linetype = "solid")+
                       scale_color_discrete(guide="none")
                   })

leg <- ggpubr::get_legend( plotList[[2]])

plotList <- c(
  list(ggp_intPolMom),
  plotList
)



plotLayout <- "AAA
              BCD"

combined_patchwork0 <- wrap_plots(plotList, 
                                 design = plotLayout, heights=c(0.35,1),
                                 guides = "collect") +
    plot_annotation(tag_levels = 'a', tag_suffix = ")") &
  theme(legend.position = "none")



leg_patch <- as_ggplot(leg) & theme(plot.tag = element_text(color="transparent"))



combined_patchwork <- wrap_plots(combined_patchwork0, leg_patch, 
                                 ncol=1, heights = c(2,0.15))+
  plot_annotation(tag_levels = "a", tag_suffix=")")


ggsave(
  here::here("figures/main/allComponents_narrative_intPolicy_country.pdf"),
  width=8, height=9,
  plot=combined_patchwork
)

```


